[{"id":0,"href":"/basic/feature_selection/boruta/","title":"Boruta","section":"特徴選択","content":" Boruta # Borutaを使って特徴量を選択してみます。このブロックのコードはBorutaの実行サンプルをそのまま持ってきたものです。\nKursa, Miron B., and Witold R. Rudnicki. \u0026quot;Feature selection with the Boruta package.\u0026quot; Journal of statistical software 36 (2010): 1-13.\nimport numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from boruta import BorutaPy # FIXME np.random.seed(777) np.int = int np.float = float np.bool = bool # load X and y X = pd.read_csv(\u0026#34;examples/test_X.csv\u0026#34;, index_col=0).values y = pd.read_csv(\u0026#34;examples/test_y.csv\u0026#34;, header=None, index_col=0).values y = y.ravel() # define random forest classifier, with utilising all cores and # sampling in proportion to y labels rf = RandomForestClassifier(n_jobs=-1, class_weight=\u0026#34;balanced\u0026#34;, max_depth=5) # define Boruta feature selection method feat_selector = BorutaPy(rf, n_estimators=\u0026#34;auto\u0026#34;, verbose=2, random_state=1) # find all relevant features - 5 features should be selected feat_selector.fit(X, y) # check selected features - first 5 features are selected feat_selector.support_ # check ranking of features feat_selector.ranking_ # call transform() on X to filter it down to selected features X_filtered = feat_selector.transform(X) Iteration: 1 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 2 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 3 / 100\rConfirmed: 0\rTentative: 10\rBorutaPy finished running.\rIteration: 9 / 100\rConfirmed: 5\rTentative: 0\rRejected: 5\r人工データでの実験 # from sklearn.datasets import make_classification from xgboost import XGBClassifier def fs_by_boruta(model, X, y): feat_selector = BorutaPy(model, n_estimators=\u0026#34;auto\u0026#34;, verbose=2, random_state=1) feat_selector.fit(X, y) X_filtered = feat_selector.transform(X) if X.shape[1] == X_filtered.shape[1]: print(\u0026#34;不用な特徴は見つかりませんでした\u0026#34;) else: print(\u0026#34;不用な特徴を削除しました\u0026#34;) print(f\u0026#34;{X.shape[1]} --\u0026gt; {X_filtered.shape[1]}\u0026#34;) return X_filtered すべて必要な特徴ならば削除しない # X, y = make_classification( n_samples=1000, n_features=10, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, random_state=0, shuffle=False, ) model = XGBClassifier(max_depth=4) fs_by_boruta(model, X, y) Iteration: 1 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 2 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 3 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 4 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 5 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 6 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 7 / 100\rConfirmed: 0\rTentative: 10\rRejected: 0\rIteration: 8 / 100\rConfirmed: 10\rTentative: 0\rRejected: 0\rBorutaPy finished running.\rIteration: 9 / 100\rConfirmed: 10\rTentative: 0\rRejected: 0\r不用な特徴は見つかりませんでした\rarray([[ 0.38760058, -0.4398061 , 1.0103586 , ..., -2.11674403,\r-3.59368321, -0.43265007],\r[-2.18745511, -2.45701675, 1.99758878, ..., 1.16128752,\r-1.92766999, 3.18705784],\r[ 3.98304273, 0.06250274, -1.31136045, ..., 1.45498409,\r-4.17178063, -2.21695578],\r...,\r[-0.44293666, 3.25707522, -0.50633794, ..., -0.72410483,\r-1.5420989 , 0.75991518],\r[-1.12641706, -0.48636924, 0.92918889, ..., -1.01001779,\r-2.69280573, -3.47050681],\r[-2.3936814 , 1.44048113, 1.95832126, ..., -5.15104933,\r-1.02766442, 1.4853396 ]])\r不用な特徴は削除する # 100個のうち10個だけ有用な特徴を混ぜて、何個削除できるかを試します。\nsklearn.datasets.make_classificationの仕様は\nWithout shuffling, X horizontally stacks features in the following order: the primary n_informative features, followed by n_redundant linear combinations of the informative features, followed by n_repeated duplicates, drawn randomly with replacement from the informative and redundant features. The remaining features are filled with random noise. Thus, without shuffling, all useful features are contained in the columns X[:, :n_informative + n_redundant + n_repeated].\nとなっているので、有用な特徴である先頭の10個の列が削除されていないかどうか確認してみます。\nX, y = make_classification( n_samples=2000, n_features=100, n_informative=10, n_redundant=0, n_repeated=0, n_classes=2, random_state=0, shuffle=False, ) model = XGBClassifier(max_depth=5) X_b = fs_by_boruta(model, X, y) Iteration: 1 / 100\rConfirmed: 0\rTentative: 100\rRejected: 0\rIteration: 2 / 100\r\u0026hellip;\nBorutaPy finished running.\rIteration: 100 / 100\rConfirmed: 10\rTentative: 1\rRejected: 88\r不用な特徴を削除しました\r100 --\u0026gt; 10\rフィルタリング後のデータに有用な特徴が残っていることを確認する # 期待どおりならば先頭10列は有用な特徴なのですべて残っているはずです。\nX[:, :10] == X_b[:, :10] array([[ True, True, True, ..., True, True, True],\r[ True, True, True, ..., True, True, True],\r[ True, True, True, ..., True, True, True],\r...,\r[ True, True, True, ..., True, True, True],\r[ True, True, True, ..., True, True, True],\r[ True, True, True, ..., True, True, True]])\r"},{"id":1,"href":"/prep/special/deepl/","title":"deeplで論文を翻訳","section":"特殊なデータ","content":"\rarXivから論文を取得する # 以下の論文をarXivからダウンロードして、概要を翻訳します。 断りがない限り、コード中に出現する英文は以下の論文の「Abstract」の英文の一部です。\nVaswani, Ashish, et al. \u0026ldquo;Attention is all you need.\u0026rdquo; Advances in neural information processing systems 30 (2017).\nimport arxiv search = arxiv.Search(id_list=[\u0026#34;1706.03762\u0026#34;]) paper = next(search.results()) print(f\u0026#34;論文タイトル：{paper.title}\u0026#34;) 論文タイトル：Attention Is All You Need\rpdf_path = paper.download_pdf() print(f\u0026#34;pdf保存先：{pdf_path}\u0026#34;) pdf保存先：./1706.03762v5.Attention_Is_All_You_Need.pdf\rpdfからテキストを抽出する # import fitz abstract_text = \u0026#34;\u0026#34; with fitz.open(pdf_path) as pages: first_page = pages[0] text = first_page.get_text().replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) print(f\u0026#39;アブスト開始位置：{text.find(\u0026#34;Abstract\u0026#34;)}, イントロ開始位置：{text.find(\u0026#34;Introduction\u0026#34;)}\u0026#39;) abstract_text = text[text.find(\u0026#34;Abstract\u0026#34;) + 8 : text.find(\u0026#34;Introduction\u0026#34;) - 1] print(f\u0026#34;{abstract_text[:400]}...\u0026#34;) アブスト開始位置：394, イントロ開始位置：1528\rThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperforming models also connect the encoder and decoder through an attentionmechanism. We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely. Experimen...\rdeepl-pythonを使って英語を翻訳する # import deepl import os translator = deepl.Translator(os.getenv(\u0026#34;DEEPL_AUTH_KEY\u0026#34;)) result = translator.translate_text(\u0026#34;Good morning!\u0026#34;, source_lang=\u0026#34;EN\u0026#34;, target_lang=\u0026#34;JA\u0026#34;) print(result) おはようございます。\rresult = translator.translate_text(abstract_text, source_lang=\u0026#34;EN\u0026#34;, target_lang=\u0026#34;JA\u0026#34;) print(result) 優性配列変換モデルは、エンコーダとデコーダを含む複雑なリカレントニューラルネットワークまたは畳み込みニューラルネットワークをベースにしています。また、最も優れたモデルでは、エンコーダとデコーダをアテンションメカニズムで接続しています。本研究では、再帰や畳み込みを必要とせず、注目メカニズムのみに基づいた新しいシンプルなネットワークアーキテクチャ「Transformer」を提案する。2つの機械翻訳タスクを用いた実験では、これらのモデルが優れた品質を持ち、並列化が可能で学習時間が大幅に短縮されることが示された。WMT 2014の英独翻訳タスクにおいて、我々のモデルは28.4 BLEUを達成し、アセンブルを含む既存の最良の結果よりも2 BLEU以上向上した。WMT 2014の英仏翻訳タスクでは、8つのGPUを用いて3.5日間の学習を行った結果、41.8という最新のBLEUスコアを達成しましたが、これは文献に掲載されている最高のモデルの学習コストのごく一部です。また、大規模および限定的な学習データを用いた英語の構文解析に適用することで、Transformerが他のタスクにもよく適応することを示しました。\r"},{"id":2,"href":"/finance/main/001-fred.jp/","title":"FREDデータベース","section":"時系列データ","content":"\rimport os import pandas as pd import seaborn as sns from full_fred.fred import Fred # FRED_API_KEY = os.getenv(\u0026#39;FRED_API_KEY\u0026#39;) fred = Fred() print(f\u0026#34;FRED APIキーが環境変数に設定されている：{fred.env_api_key_found()}\u0026#34;) def get_fred_data(name, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;): df = fred.get_series_df(name)[[\u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;]].copy() df[\u0026#34;date\u0026#34;] = pd.to_datetime(df[\u0026#34;date\u0026#34;]) df[\u0026#34;value\u0026#34;] = pd.to_numeric(df[\u0026#34;value\u0026#34;], errors=\u0026#34;coerce\u0026#34;) df = df.set_index(\u0026#34;date\u0026#34;) if end == \u0026#34;\u0026#34;: df = df.loc[f\u0026#34;{start}\u0026#34;:] else: df = df.loc[f\u0026#34;{start}\u0026#34;:f\u0026#34;{end}\u0026#34;] return df FRED APIキーが環境変数に設定されている：True\rUnemployment Rate # ソース：https://fred.stlouisfed.org/series/UNRATE\ndf_UNRATE = get_fred_data(\u0026#34;UNRATE\u0026#34;, start=\u0026#34;2010-01-01\u0026#34;) df_UNRATE.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rsns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) sns.lineplot(data=df_UNRATE, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;) \u0026lt;AxesSubplot:xlabel='date', ylabel='value'\u0026gt;\rICE BofA US High Yield Index Option-Adjusted Spread # ソース：https://fred.stlouisfed.org/series/BAMLH0A0HYM2\ndf_BAMLH0A0HYM2 = get_fred_data(\u0026#34;BAMLH0A0HYM2\u0026#34;, start=\u0026#34;2010-01-01\u0026#34;) df_BAMLH0A0HYM2.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rWilshire US Large-Cap Growth Total Market Index # ソース：https://fred.stlouisfed.org/series/WILLLRGCAPGR\ndf_WILLLRGCAPGR = get_fred_data(\u0026#34;WILLLRGCAPGR\u0026#34;, start=\u0026#34;2010-01-01\u0026#34;) df_WILLLRGCAPGR.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rsns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) sns.lineplot(data=df_WILLLRGCAPGR, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;) sns.lineplot(data=df_BAMLH0A0HYM2 * 10000, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;) \u0026lt;AxesSubplot:xlabel='date', ylabel='value'\u0026gt;\r"},{"id":3,"href":"/basic/clustering/k-means1/","title":"k-means","section":"クラスタリング","content":"\rimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.cluster import KMeans from sklearn.datasets import make_blobs k-means クラスタリングの例 # サンプルデータを作成 # make_blobsでサンプルデータを作成し、それに対してクラスタリングを適用します。\nn_samples = 1000 random_state = 117117 n_clusters = 4 X, y = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=1.5, centers=8 ) # k-meansの初期値用のポイント init = X[np.where(X[:, 1] \u0026lt; -8)][:n_clusters] # k-meansの初期値をプロット plt.figure(figsize=(8, 8)) plt.scatter(X[:, 0], X[:, 1], c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;学習用データ\u0026#34;) plt.legend() plt.show() セントロイドの計算 # 4回セントロイドを求め直して、4回目できれいにデータが別れていることが確認できます。\nplt.figure(figsize=(12, 12)) for i in range(4): plt.subplot(2, 2, i + 1) max_iter = 1 + i km = KMeans( n_clusters=n_clusters, init=init, max_iter=max_iter, n_init=1, random_state=1 ).fit(X) cluster_centers = km.cluster_centers_ y_pred = km.predict(X) plt.title(f\u0026#34;max_iter={max_iter}\u0026#34;) plt.scatter(X[:, 0], X[:, 1], c=y_pred, marker=\u0026#34;x\u0026#34;) plt.scatter( cluster_centers[:, 0], cluster_centers[:, 1], c=\u0026#34;r\u0026#34;, marker=\u0026#34;o\u0026#34;, label=\u0026#34;クラスタのセントロイド\u0026#34;, ) plt.tight_layout() plt.show() クラスタが重なり合っている場合 # クラスタのデータが重なり合っている時の、クラスタリング結果を確認します。\nn_samples = 1000 random_state = 117117 plt.figure(figsize=(8, 8)) for i in range(4): cluster_std = (i + 1) * 1.5 X, y = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=cluster_std ) y_pred = KMeans(n_clusters=2, random_state=random_state, init=\u0026#34;random\u0026#34;).fit_predict( X ) plt.subplot(2, 2, i + 1) plt.title(f\u0026#34;cluster_std={cluster_std}\u0026#34;) plt.scatter(X[:, 0], X[:, 1], c=y_pred, marker=\u0026#34;x\u0026#34;) plt.tight_layout() plt.show() kの数の影響 # n_samples = 1000 random_state = 117117 plt.figure(figsize=(8, 8)) for i in range(4): n_clusters = (i + 1) * 2 X, y = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=1, centers=5 ) y_pred = KMeans( n_clusters=n_clusters, random_state=random_state, init=\u0026#34;random\u0026#34; ).fit_predict(X) plt.subplot(2, 2, i + 1) plt.title(f\u0026#34;#cluster={n_clusters}\u0026#34;) plt.scatter(X[:, 0], X[:, 1], c=y_pred, marker=\u0026#34;x\u0026#34;) plt.tight_layout() plt.show() "},{"id":4,"href":"/visualize/tips/stylesheet/","title":"matplotlibスタイルシート","section":"Appendix","content":" Customizing Matplotlib with style sheets and rcParams\nデフォルトで読み込まれるスタイルシートのファイルを確認する # matplotlibがデフォルトで読み込むスタイルシートを確認します。コマンドライン上で python -c \u0026quot;import matplotlib;print(matplotlib.matplotlib_fname())\u0026quot; と実行することで確認ができます。\npython -c \u0026#34;import matplotlib;print(matplotlib.matplotlib_fname())\u0026#34; C:\\Users\\xxx\\env-py3.10\\.venv\\lib\\site-packages\\matplotlib\\mpl-data\\matplotlibrc このファイルを直接書き換えるか、以下に示すように plt.style.use(\u0026quot;k_dm.mplstyle\u0026quot;) として指定したファイルを読み込むことによって、スタイルシートを変更できます。\n自作したスタイルシートを読み込む # 以下では、自作したスタイルシート(k_dm.mplstyle)の表示を確認してみます。使用したスタイルシートの詳細な指定を知りたい方はgithub上で確認してください。\nimport numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris import japanize_matplotlib plt.style.use(\u0026#34;k_dm.mplstyle\u0026#34;) # ここで自作したスタイルシートを読み込んでいます 散布図 # Xy = load_iris(as_frame=True) X = Xy[\u0026#34;data\u0026#34;].values y = Xy[\u0026#34;target\u0026#34;].values markers = [\u0026#34;o\u0026#34;, \u0026#34;^\u0026#34;, \u0026#34;v\u0026#34;] featname_1 = \u0026#34;sepal length (cm)\u0026#34; featname_2 = \u0026#34;sepal width (cm)\u0026#34; fig, ax = plt.subplots(figsize=(8, 8)) for i in range(3): plt.scatter(X[y == i, 0], X[y == i, 1], marker=markers[i]) plt.xlabel(featname_1) plt.ylabel(featname_2) plt.legend(Xy.target_names) plt.title(\u0026#34;sample title\u0026#34;) plt.show() fig, ax = plt.subplots(figsize=(8, 8)) X, Y = np.meshgrid(np.linspace(0.0, 5.0, 10), np.linspace(0.0, 5.0, 10)) x, y = X.ravel(), Y.ravel() ax.scatter(x, y, label=\u0026#34;A\u0026#34;) ax.scatter(x + 2.5, y + 2.5, label=\u0026#34;B\u0026#34;) ax.scatter(x - 2.5, y - 2.5, label=\u0026#34;C\u0026#34;) plt.grid() plt.show() プロット # plt.figure(figsize=(13, 7)) x = np.linspace(0, 10, 100) + 1.0 plt.plot(x, np.cos(x), label=\u0026#34;cos\u0026#34;) plt.plot(x, np.sin(x), label=\u0026#34;sin\u0026#34;) plt.plot(x, np.log(x), label=\u0026#34;log\u0026#34;) plt.plot(x, np.cos(x) + np.sin(x), label=\u0026#34;cos+sin\u0026#34;) plt.legend() plt.title(\u0026#34;sample title\u0026#34;) plt.grid() plt.show() plt.figure(figsize=(13, 7)) x = np.linspace(0, 10, 40) + 1.0 plt.plot(x, np.cos(x), \u0026#34;x\u0026#34;, label=\u0026#34;cos\u0026#34;) plt.plot(x, np.sin(x), \u0026#34;^\u0026#34;, label=\u0026#34;sin\u0026#34;) plt.plot(x, np.log(x), \u0026#34;.-\u0026#34;, label=\u0026#34;log\u0026#34;) plt.plot(x, np.cos(x) + np.sin(x), \u0026#34;v\u0026#34;, label=\u0026#34;cos+sin\u0026#34;) plt.legend() plt.title(\u0026#34;sample title\u0026#34;) plt.show() 箱髭図 # np.random.seed(777) x1 = np.random.normal(1, 1, 200) x2 = np.random.normal(5, 2, 200) x3 = np.random.normal(10, 3, 200) fig = plt.figure(figsize=(13, 7)) ax = fig.add_subplot(1, 1, 1) ax.boxplot([x1, x2, x3], labels=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;, \u0026#34;C\u0026#34;]) plt.title(\u0026#34;sample title\u0026#34;) plt.show() 棒グラフ # np.random.seed(777) fig = plt.figure(figsize=(13, 7)) indexes = np.array([i for i in range(10)]) * 6 for i, label in enumerate([\u0026#34;あいうえお\u0026#34;, \u0026#34;かきくけこ\u0026#34;, \u0026#34;さしすせそ\u0026#34;, \u0026#34;たちつてと\u0026#34;, \u0026#34;なにぬねの\u0026#34;]): plt.bar(indexes + i, [np.random.randint(5, 10) + i for _ in range(10)], label=label) plt.legend() plt.title(\u0026#34;sample title\u0026#34;) plt.grid(axis=\u0026#34;y\u0026#34;) plt.show() stackplot # matplotlibのギャラリーよりStackplots コードを引用しています。\nyear = [1950, 1960, 1970, 1980, 1990, 2000, 2010, 2018] population_by_continent = { \u0026#34;africa\u0026#34;: [228, 284, 365, 477, 631, 814, 1044, 1275], \u0026#34;americas\u0026#34;: [340, 425, 519, 619, 727, 840, 943, 1006], \u0026#34;asia\u0026#34;: [1394, 1686, 2120, 2625, 3202, 3714, 4169, 4560], \u0026#34;europe\u0026#34;: [220, 253, 276, 295, 310, 303, 294, 293], \u0026#34;oceania\u0026#34;: [12, 15, 19, 22, 26, 31, 36, 39], } fig, ax = plt.subplots(figsize=(13, 7)) ax.stackplot( year, population_by_continent.values(), labels=population_by_continent.keys(), alpha=0.8, ) ax.legend(loc=\u0026#34;upper left\u0026#34;) ax.set_title(\u0026#34;World population\u0026#34;) ax.set_xlabel(\u0026#34;Year\u0026#34;) ax.set_ylabel(\u0026#34;Number of people (millions)\u0026#34;) plt.show() "},{"id":5,"href":"/prep/categorical/onehot/","title":"One-hot","section":"カテゴリカルデータ","content":" サンプルデータ # import pandas as pd X = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;) X.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rOneHotEncoder # sklearn.preprocessing.OneHotEncoder sklearn.compose.make_column_transformer from sklearn.preprocessing import OneHotEncoder from sklearn.compose import make_column_transformer c_oh = OneHotEncoder() ct = make_column_transformer( (c_oh, [\u0026#34;元号\u0026#34;]), # カテゴリ変数にonehotを適用するように指定 sparse_threshold=0, # 結果の行列をdenseな行列にする ) 結果を確認する # 元号（大正・昭和・平成）が三次元のベクトルになっていることが確認できます。\n# 変換方法を求めて、変換した結果を返す X_transform = ct.fit_transform(X) # 変換後のテーブルを表示 pd.DataFrame(X_transform).head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":6,"href":"/basic/dimensionality_reduction/pca/","title":"PCA","section":"次元削減","content":"\rimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_blobs 実験用のデータ # sklearn.datasets.make_blobs\nX, y = make_blobs(n_samples=600, n_features=3, random_state=117117) fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\u0026#34;o\u0026#34;, c=y) ax.set_xlabel(\u0026#34;$x_1$\u0026#34;) ax.set_ylabel(\u0026#34;$x_2$\u0026#34;) ax.set_zlabel(\u0026#34;$x_3$\u0026#34;) PCAで二次元に次元削減する # sklearn.decomposition.PCA\nfrom sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler pca = PCA(n_components=2, whiten=True) X_pca = pca.fit_transform(StandardScaler().fit_transform(X)) fig = plt.figure(figsize=(8, 8)) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y) 正規化の効果を見る # クラスタ数3, クラスタに重複がある場合 # # 実験用データ X, y = make_blobs( n_samples=200, n_features=3, random_state=11711, centers=3, cluster_std=2.0 ) X[:, 1] = X[:, 1] * 1000 X[:, 2] = X[:, 2] * 0.01 X_ss = StandardScaler().fit_transform(X) # 元データをプロット fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\u0026#34;o\u0026#34;, c=y) ax.set_xlabel(\u0026#34;$x_1$\u0026#34;) ax.set_ylabel(\u0026#34;$x_2$\u0026#34;) ax.set_zlabel(\u0026#34;$x_3$\u0026#34;) plt.title(\u0026#34;実験用データ\u0026#34;) plt.show() # 正規化をせずにPCA pca = PCA(n_components=2).fit(X) X_pca = pca.transform(X) # 正規化をしてPCA pca_ss = PCA(n_components=2).fit(X_ss) X_pca_ss = pca_ss.transform(X_ss) fig = plt.figure(figsize=(10, 5)) plt.subplot(121) plt.title(\u0026#34;正規化なし\u0026#34;) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, marker=\u0026#34;x\u0026#34;, alpha=0.6) plt.subplot(122) plt.title(\u0026#34;正規化あり\u0026#34;) plt.scatter(X_pca_ss[:, 0], X_pca_ss[:, 1], c=y, marker=\u0026#34;x\u0026#34;, alpha=0.6) クラスタ数6, クラスタに重複が無い場合 # # 実験用データ X, y = make_blobs( n_samples=500, n_features=3, random_state=11711, centers=6, cluster_std=0.4 ) X[:, 1] = X[:, 1] * 1000 X[:, 2] = X[:, 2] * 0.01 X_ss = StandardScaler().fit_transform(X) # 元データをプロット fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\u0026#34;o\u0026#34;, c=y) ax.set_xlabel(\u0026#34;$x_1$\u0026#34;) ax.set_ylabel(\u0026#34;$x_2$\u0026#34;) ax.set_zlabel(\u0026#34;$x_3$\u0026#34;) plt.title(\u0026#34;実験用データ\u0026#34;) plt.show() # 正規化をせずにPCA pca = PCA(n_components=2).fit(X) X_pca = pca.transform(X) # 正規化をしてPCA pca_ss = PCA(n_components=2).fit(X_ss) X_pca_ss = pca_ss.transform(X_ss) fig = plt.figure(figsize=(10, 5)) plt.subplot(121) plt.title(\u0026#34;正規化なし\u0026#34;) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, marker=\u0026#34;x\u0026#34;, alpha=0.6) plt.subplot(122) plt.title(\u0026#34;正規化あり\u0026#34;) plt.scatter(X_pca_ss[:, 0], X_pca_ss[:, 1], c=y, marker=\u0026#34;x\u0026#34;, alpha=0.6) "},{"id":7,"href":"/prep/table/extract_table_from_pdf/","title":"pdfから表を抽出","section":"テーブルデータ全般","content":"\rcamelotを使う場合 # 必要なライブラリをインストール # 今回はCamelotというライブラリを使ってpdfからテーブルを抽出します。 opencv-contrib-python、camelot、tabula-pyが必要なので適宜インストールします。\npoetry add opencv-contrib-python camelot tabula-py ghostscript Ghostscriptというソフトウェアも必要なのでインストールします。 OSによってインストール方法が異なるので注意してください。 インストール方法はこちらを参照してください。\nghostscriptがインストールされているか確認する # from ctypes.util import find_library find_library(\u0026#34;gs\u0026#34;) # gsが実行可能ならば /usr/local/lib/libgs.dylibなどの表示がされます '/usr/local/lib/libgs.dylib'\rpdfからテーブルを抽出する # 例として総務省のページで公開されている「政策ごとの予算との対応について」のpdfからテーブルを抽出します。テーブルがパースできたことがわかります。\n※jupyterbookでエラーが出る場合があるためコメントアウトしています\nimport camelot # pdfを読み込んでテーブルを抽出 # pdf_name = \u0026#34;000788423.pdf\u0026#34; # tables = camelot.read_pdf(pdf_name) # print(\u0026#34;パースできたテーブル数\u0026#34;, tables.n) # 先頭５行のみ表示 # tables[0].df.head() 今度はFLEX LNGという会社の決算情報をパースしてみます。 データはFLEXLNG｜Investor Homeで取得したファイルで実行しています。 今後はテーブルのパースに失敗してしましました。\n# pdfを読み込んでテーブルを抽出 pdf_name = \u0026#34;flex-lng-earnings-release-q3-2021.pdf\u0026#34; tables = camelot.read_pdf(pdf_name) print(\u0026#34;パースできたテーブル数\u0026#34;, tables.n) # 先頭５行のみ表示 # tables[0].df.head() パースできたテーブル数 0\rtabula-py を使う場合 # poetry add tabula-pyなどとしてtabulaをインストールしてください。 tabulaはバックグラウンドでtabula-java を使用していますが、Javaのバージョンが古い場合こちらがエラーになる場合があるようです。\n参考文献：subprocess.CalledProcessError While extracting table from PDF using tabula-py\nfrom tabula import read_pdf tables = read_pdf(\u0026#34;flex-lng-earnings-release-q3-2021.pdf\u0026#34;, pages=\u0026#34;all\u0026#34;) data = tables[1] data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rパースしたテーブルを可視化する # tabulateでパースしたテーブルは文字列になっているので、そこから数値を読み取ります。\nimport matplotlib.pyplot as plt d = data.query(\u0026#34;ASSETS==\u0026#39;Cash and cash equivalents\u0026#39;\u0026#34;).iloc[0][2:] # パースしたテーブルは文字列になっているので数値に変換する d = [int(v.replace(\u0026#34;,\u0026#34;, \u0026#34;\u0026#34;)) for v in d] # プロット plt.bar([0, 1, 2], d) plt.xticks([0, 1, 2], [\u0026#34;2021 Sep 30\u0026#34;, \u0026#34;2021 June 30\u0026#34;, \u0026#34;2020 Dec 31\u0026#34;]) plt.show() "},{"id":8,"href":"/timeseries/forecast/001-prophet/","title":"Prophet","section":"Prophet","content":""},{"id":9,"href":"/basic/timeseries/prophet/","title":"Prophetを使ってみる","section":"時系列","content":"\rインストール方法は「prophet Installation」を参照してください。また、 公式ドキュメントのQuick Start（prophet | Quick Start）もご参照ください。\nK_DM - 時系列 \u0026gt; 予測 \u0026gt; ProphetでもProphetを扱っています。 こちらもご参照ください。また、時系列の続きはK_DM - 時系列に書いていきますのでそちらも参照してみてください。\n時系列データの作成 # ダミーの時系列データを作成します。\nimport numpy as np import pandas as pd import seaborn as sns from prophet import Prophet sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) 実験用のデータを作成 # date = pd.date_range(\u0026#34;2020-01-01\u0026#34;, periods=365, freq=\u0026#34;D\u0026#34;) y = [np.cos(di.weekday()) + di.month % 2 + np.log(i + 1) for i, di in enumerate(date)] df = pd.DataFrame({\u0026#34;ds\u0026#34;: date, \u0026#34;y\u0026#34;: y}) df.index = date sns.lineplot(data=df) Prophetの訓練 # m = Prophet(yearly_seasonality=False, daily_seasonality=True) m.fit(df) Initial log joint probability = -24.5101\r\u0026lt;prophet.forecaster.Prophet at 0x7fe3b5d93d60\u0026gt;\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 99 798.528 0.00821602 204.832 1 1 136 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 136 799.486 0.00040141 83.1101 5.02e-06 0.001 225 LS failed, Hessian reset 158 799.529 0.00027729 48.4168 3.528e-06 0.001 291 LS failed, Hessian reset 199 799.55 3.15651e-05 54.5691 1 1 345 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 204 799.553 3.54297e-05 56.7445 5.36e-07 0.001 397 LS failed, Hessian reset 267 799.556 6.19351e-08 44.7029 0.2081 1 490 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\r予測用のデータを作成し予測を実行する # future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig1 = m.plot(forecast) "},{"id":10,"href":"/eval/classification/rocauc/roc-auc/","title":"ROC-AUC","section":"分類","content":"\r*ROC曲線の下の面積を AUC (Area Under the Curve) と呼び、分類モデルの評価指標として用いられる。AUC が 1 のときが最良であり、ランダムで全く無効なモデルでは 0.5 となる。*出典：ROC(受信者操作特性)\n二項分類の評価指標 1が最良であり、0.5は完全にランダムな予測をした状態に近い 0.5を下回るのは、正解とは反対の予測をしているときにあり得る ROC曲線をプロットすることで、分類の閾値をどうすべきか判断することができる import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_curve ROC Curveをプロット # sklearn.metrics.roc_curve\nROC Curveをプロットするための関数 # def plot_roc_curve(test_y, pred_y): \u0026#34;\u0026#34;\u0026#34;正解と予測からROC Curveをプロット Args: test_y (ndarray of shape (n_samples,)): テストデータの正解 pred_y (ndarray of shape (n_samples,)): テストデータに対する予測値 \u0026#34;\u0026#34;\u0026#34; # False Positive Rate, True Positive Rateを計算 fprs, tprs, thresholds = roc_curve(test_y, pred_y) # ROCをプロット plt.figure(figsize=(8, 8)) plt.plot([0, 1], [0, 1], linestyle=\u0026#34;-\u0026#34;, c=\u0026#34;k\u0026#34;, alpha=0.2, label=\u0026#34;ROC-AUC=0.5\u0026#34;) plt.plot(fprs, tprs, color=\u0026#34;orange\u0026#34;, label=\u0026#34;ROC Curve\u0026#34;) plt.xlabel(\u0026#34;False Positive Rate\u0026#34;) plt.ylabel(\u0026#34;True Positive Rate\u0026#34;) # ROC-AUCスコアに相当する部分を塗りつぶす y_zeros = [0 for _ in tprs] plt.fill_between(fprs, y_zeros, tprs, color=\u0026#34;orange\u0026#34;, alpha=0.3, label=\u0026#34;ROC-AUC\u0026#34;) plt.legend() plt.show() サンプルデータに対してモデルを作成し、ROC Curveをプロット # X, y = make_classification( n_samples=1000, n_classes=2, n_informative=4, n_clusters_per_class=3, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) model = RandomForestClassifier(max_depth=5) model.fit(train_X, train_y) pred_y = model.predict_proba(test_X)[:, 1] plot_roc_curve(test_y, pred_y) ROC-AUCを計算 # sklearn.metrics.roc_auc_score\nfrom sklearn.metrics import roc_auc_score roc_auc_score(test_y, pred_y) 0.89069793083171\r"},{"id":11,"href":"/timeseries/unsupervised/ruptures/","title":"ruptures","section":"教師なしの手法","content":"\rRupture # Truong, Charles, Laurent Oudre, and Nicolas Vayatis. \u0026ldquo;ruptures: change point detection in Python.\u0026rdquo; arXiv preprint arXiv:1801.00826 (2018). (arXiv)\n公式ドキュメント：ruptures documentation\n# seaborn内部のpandas使用箇所でFutureWarningが出ていたので表示しないようにする import warnings from datetime import datetime, timedelta import matplotlib.pyplot as plt import numpy as np import pandas as pd import ruptures as rpt import seaborn as sns warnings.simplefilter(action=\u0026#34;ignore\u0026#34;, category=FutureWarning) # generate signal n_samples, dim, sigma = 1000, 3, 4 n_bkps = 4 # number of breakpoints signal, bkps = rpt.pw_constant(n_samples, dim, n_bkps, noise_std=sigma) # detection algo = rpt.Pelt(model=\u0026#34;rbf\u0026#34;).fit(signal) result = algo.predict(pen=10) # display rpt.display(signal, bkps, result) plt.show() さまざまなデータでの比較 # 変化点検出＋可視化のコード # def vizualize_change_point( signals, true_change_points=[], method=\u0026#34;rbf\u0026#34;, pen=0.5, min_size=3 ): algo = rpt.Pelt(model=method, min_size=min_size).fit(signals) result = algo.predict(pen=pen) rpt.display(signals, true_change_points, result) plt.suptitle(f\u0026#34;method={method}, penalty={pen}, \u0026#34;, fontsize=20) plt.tight_layout() plt.show() サンプルデータの作成 # X = [] dates = [datetime(2021, 1, 1) + timedelta(days=i) for i in range(1000)] for i, di in enumerate(dates): value = i // 100 X.append( [ np.sin(di.weekday() * 0.001) * 2 + np.random.rand() * 0.1 + value * np.cos(i), np.sin(di.day * 0.01) * 3 + np.random.rand() * 0.1 + value * np.cos(i), ] ) data = pd.DataFrame(X) data.columns = [f\u0026#34;f{c}\u0026#34; for c in data.columns] data.index = dates plt.figure(figsize=(12, 3)) sns.lineplot(data=data) \u0026lt;Axes: \u0026gt;\rペナルティの違いによる結果の違い # for pen in np.linspace(0.1, 1, 5): vizualize_change_point(data.values, method=\u0026#34;rbf\u0026#34;, pen=pen) 動的計画法を使った変化点の検出 # 動的計画法を使う場合、変化点の数（n_bkps）をあらかじめ指定する必要がある。\ndef vizualize_change_point_dynp( signals, true_change_points=[], method=\u0026#34;rbf\u0026#34;, min_size=3, n_bkps=3 ): algo = rpt.Dynp(model=method, min_size=min_size).fit(signals) result = algo.predict(n_bkps=n_bkps) rpt.display(signals, true_change_points, result) plt.suptitle(f\u0026#34;method={method}, penalty={pen}, \u0026#34;, fontsize=20) plt.tight_layout() plt.show() vizualize_change_point_dynp( data.values, true_change_points=[], method=\u0026#34;rbf\u0026#34;, min_size=3, n_bkps=8 ) 人工データでの実験2 # 移動平均は大体いつもおなじだけれど、分散がある時点で変化するデータです。\nX2 = [] dates2 = [datetime(2021, 1, 1) + timedelta(days=i) for i in range(1000)] for i, di in enumerate(dates2): if i == 550: X2.append( [ np.sin(di.weekday() * 0.001) * 2 + np.random.rand() * 0.1, np.sin(di.day * 0.01) * 3 + np.random.rand() * 0.1, ] ) elif 500 \u0026lt; i \u0026lt; 600: X2.append( [ np.sin(di.weekday() * 0.001) * 2 + np.random.rand() * 0.1 + np.random.randint(-2, 2) * 0.1, np.sin(di.day * 0.01) * 3 + np.random.rand() * 0.1, ] ) else: X2.append( [ np.sin(di.weekday() * 0.001) * 2 + np.random.rand() * 0.1, np.sin(di.day * 0.01) * 3 + np.random.rand() * 0.1, ] ) data2 = pd.DataFrame(X2) data2.columns = [f\u0026#34;f{c}\u0026#34; for c in data2.columns] data2.index = dates2 plt.figure(figsize=(12, 3)) sns.lineplot(data=data2) plt.ylim(0, 1) (0.0, 1.0)\r# 注意点：値の絶対値や損失関数によって指定すべきpenaltyの値は異なる vizualize_change_point(data2.values, method=\u0026#34;rbf\u0026#34;, pen=1.5, min_size=50) vizualize_change_point(data2.values, method=\u0026#34;l2\u0026#34;, pen=1.5, min_size=50) data2[\u0026#34;var\u0026#34;] = data2[\u0026#34;f0\u0026#34;].rolling(window=10).var().fillna(0) vizualize_change_point(data2.values, method=\u0026#34;rbf\u0026#34;, pen=1.5, min_size=50) vizualize_change_point(data2.values, method=\u0026#34;l2\u0026#34;, pen=0.1, min_size=100) vizualize_change_point_dynp(data2.values, min_size=90, n_bkps=2) 実データでの実験 # Yahoo financeのDDOGのデータを使って変化点を見てみようと思います。\nddog = pd.read_csv(\u0026#34;./DDOG.csv\u0026#34;) ddog.index = ddog[\u0026#34;Date\u0026#34;] ddog.drop([\u0026#34;Date\u0026#34;], axis=1, inplace=True) ddog[\u0026#34;Volume\u0026#34;] = 100 * ddog[\u0026#34;Volume\u0026#34;] / ddog[\u0026#34;Volume\u0026#34;].median() ddog = ddog[[\u0026#34;Open\u0026#34;, \u0026#34;Close\u0026#34;, \u0026#34;Volume\u0026#34;]] plt.figure(figsize=(12, 4)) sns.lineplot(data=ddog) plt.xticks(rotation=90) plt.show() vizualize_change_point(ddog.values, method=\u0026#34;rbf\u0026#34;, pen=0.5, min_size=40) "},{"id":12,"href":"/timeseries/exponential_smoothing/001-simple-es/","title":"Simple Exponential Smoothing","section":"指数平滑化","content":"import japanize_matplotlib as jm import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from statsmodels.tsa.holtwinters import SimpleExpSmoothing jm.japanize() Sample Data # We are using the data from Natural Gas Consumption (NATURALGAS). The unit of measurement is BCF (billion cubic feet).\ndata = { \u0026#34;value\u0026#34;: { \u0026#34;2013-01-01\u0026#34;: 2878.8, \u0026#34;2013-02-01\u0026#34;: 2567.2, \u0026#34;2013-03-01\u0026#34;: 2521.1, \u0026#34;2013-04-01\u0026#34;: 1967.5, \u0026#34;2013-05-01\u0026#34;: 1752.5, \u0026#34;2013-06-01\u0026#34;: 1742.9, \u0026#34;2013-07-01\u0026#34;: 1926.3, \u0026#34;2013-08-01\u0026#34;: 1927.4, \u0026#34;2013-09-01\u0026#34;: 1767.0, \u0026#34;2013-10-01\u0026#34;: 1866.8, \u0026#34;2013-11-01\u0026#34;: 2316.9, \u0026#34;2013-12-01\u0026#34;: 2920.8, \u0026#34;2014-01-01\u0026#34;: 3204.1, \u0026#34;2014-02-01\u0026#34;: 2741.2, \u0026#34;2014-03-01\u0026#34;: 2557.9, \u0026#34;2014-04-01\u0026#34;: 1961.7, \u0026#34;2014-05-01\u0026#34;: 1810.2, \u0026#34;2014-06-01\u0026#34;: 1745.4, \u0026#34;2014-07-01\u0026#34;: 1881.0, \u0026#34;2014-08-01\u0026#34;: 1933.1, \u0026#34;2014-09-01\u0026#34;: 1809.3, \u0026#34;2014-10-01\u0026#34;: 1912.8, \u0026#34;2014-11-01\u0026#34;: 2357.5, \u0026#34;2014-12-01\u0026#34;: 2679.2, \u0026#34;2015-01-01\u0026#34;: 3115.0, \u0026#34;2015-02-01\u0026#34;: 2925.2, \u0026#34;2015-03-01\u0026#34;: 2591.3, \u0026#34;2015-04-01\u0026#34;: 2007.9, \u0026#34;2015-05-01\u0026#34;: 1858.2, \u0026#34;2015-06-01\u0026#34;: 1899.9, \u0026#34;2015-07-01\u0026#34;: 2067.7, \u0026#34;2015-08-01\u0026#34;: 2052.7, \u0026#34;2015-09-01\u0026#34;: 1901.3, \u0026#34;2015-10-01\u0026#34;: 1987.3, \u0026#34;2015-11-01\u0026#34;: 2249.1, \u0026#34;2015-12-01\u0026#34;: 2588.2, \u0026#34;2016-01-01\u0026#34;: 3091.7, \u0026#34;2016-02-01\u0026#34;: 2652.3, \u0026#34;2016-03-01\u0026#34;: 2356.3, \u0026#34;2016-04-01\u0026#34;: 2083.9, \u0026#34;2016-05-01\u0026#34;: 1965.8, \u0026#34;2016-06-01\u0026#34;: 2000.7, \u0026#34;2016-07-01\u0026#34;: 2186.6, \u0026#34;2016-08-01\u0026#34;: 2208.4, \u0026#34;2016-09-01\u0026#34;: 1947.8, \u0026#34;2016-10-01\u0026#34;: 1925.2, \u0026#34;2016-11-01\u0026#34;: 2159.5, \u0026#34;2016-12-01\u0026#34;: 2866.3, \u0026#34;2017-01-01\u0026#34;: 2913.8, \u0026#34;2017-02-01\u0026#34;: 2340.2, \u0026#34;2017-03-01\u0026#34;: 2523.3, \u0026#34;2017-04-01\u0026#34;: 1932.0, \u0026#34;2017-05-01\u0026#34;: 1892.0, \u0026#34;2017-06-01\u0026#34;: 1910.4, \u0026#34;2017-07-01\u0026#34;: 2141.6, \u0026#34;2017-08-01\u0026#34;: 2093.8, \u0026#34;2017-09-01\u0026#34;: 1920.5, \u0026#34;2017-10-01\u0026#34;: 2031.5, \u0026#34;2017-11-01\u0026#34;: 2357.3, \u0026#34;2017-12-01\u0026#34;: 3086.0, \u0026#34;2018-01-01\u0026#34;: 3340.9, \u0026#34;2018-02-01\u0026#34;: 2710.7, \u0026#34;2018-03-01\u0026#34;: 2796.7, \u0026#34;2018-04-01\u0026#34;: 2350.5, \u0026#34;2018-05-01\u0026#34;: 2055.0, \u0026#34;2018-06-01\u0026#34;: 2063.1, \u0026#34;2018-07-01\u0026#34;: 2350.7, \u0026#34;2018-08-01\u0026#34;: 2313.8, \u0026#34;2018-09-01\u0026#34;: 2156.1, \u0026#34;2018-10-01\u0026#34;: 2285.9, \u0026#34;2018-11-01\u0026#34;: 2715.9, \u0026#34;2018-12-01\u0026#34;: 2999.5, \u0026#34;2019-01-01\u0026#34;: 3424.3, \u0026#34;2019-02-01\u0026#34;: 3019.1, \u0026#34;2019-03-01\u0026#34;: 2927.8, \u0026#34;2019-04-01\u0026#34;: 2212.4, \u0026#34;2019-05-01\u0026#34;: 2134.0, \u0026#34;2019-06-01\u0026#34;: 2119.3, \u0026#34;2019-07-01\u0026#34;: 2393.9, \u0026#34;2019-08-01\u0026#34;: 2433.9, \u0026#34;2019-09-01\u0026#34;: 2206.3, \u0026#34;2019-10-01\u0026#34;: 2306.5, \u0026#34;2019-11-01\u0026#34;: 2783.8, \u0026#34;2019-12-01\u0026#34;: 3170.7, \u0026#34;2020-01-01\u0026#34;: 3320.6, \u0026#34;2020-02-01\u0026#34;: 3058.5, \u0026#34;2020-03-01\u0026#34;: 2722.0, \u0026#34;2020-04-01\u0026#34;: 2256.9, \u0026#34;2020-05-01\u0026#34;: 2072.2, \u0026#34;2020-06-01\u0026#34;: 2127.9, \u0026#34;2020-07-01\u0026#34;: 2464.1, \u0026#34;2020-08-01\u0026#34;: 2399.5, \u0026#34;2020-09-01\u0026#34;: 2151.2, \u0026#34;2020-10-01\u0026#34;: 2315.9, \u0026#34;2020-11-01\u0026#34;: 2442.0, \u0026#34;2020-12-01\u0026#34;: 3182.8, \u0026#34;2021-01-01\u0026#34;: 3343.9, \u0026#34;2021-02-01\u0026#34;: 3099.2, \u0026#34;2021-03-01\u0026#34;: 2649.4, \u0026#34;2021-04-01\u0026#34;: 2265.1, \u0026#34;2021-05-01\u0026#34;: 2117.4, \u0026#34;2021-06-01\u0026#34;: 2238.4, \u0026#34;2021-07-01\u0026#34;: 2412.2, \u0026#34;2021-08-01\u0026#34;: 2433.8, \u0026#34;2021-09-01\u0026#34;: 2142.3, \u0026#34;2021-10-01\u0026#34;: 2262.6, \u0026#34;2021-11-01\u0026#34;: 2693.3, \u0026#34;2021-12-01\u0026#34;: 3007.3, \u0026#34;2022-01-01\u0026#34;: 3612.1, \u0026#34;2022-02-01\u0026#34;: 3064.2, \u0026#34;2022-03-01\u0026#34;: 2785.4, \u0026#34;2022-04-01\u0026#34;: 2379.3, \u0026#34;2022-05-01\u0026#34;: 2247.8, \u0026#34;2022-06-01\u0026#34;: 2326.9, \u0026#34;2022-07-01\u0026#34;: 2597.9, \u0026#34;2022-08-01\u0026#34;: 2566.1, \u0026#34;2022-09-01\u0026#34;: 2263.3, } } data = pd.DataFrame(data) data.rename(columns={\u0026#34;value\u0026#34;: \u0026#34;Natural Gas Consumption(BCF)\u0026#34;}, inplace=True) data.index = pd.to_datetime(data.index) data = data.asfreq(\u0026#34;MS\u0026#34;) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data) plt.grid() plt.show() Data Splitting # To evaluate the predictive performance, the data is split such that the data from the end (post-2020) is used for validation.\ndata_train = data[data.index \u0026lt; \u0026#34;2020-1-1\u0026#34;] data_test = data[data.index \u0026gt;= \u0026#34;2020-1-1\u0026#34;] plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.legend() plt.grid() Model Training and Prediction # ses = SimpleExpSmoothing(data_train) ses = ses.fit(smoothing_level=0.1) pred = ses.forecast(33) plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.plot(pred.index, pred.values, \u0026#34;-x\u0026#34;, label=\u0026#34;pred\u0026#34;) plt.legend() plt.grid() "},{"id":13,"href":"/webapp/streamlit/streamlit/","title":"streamlit","section":"アプリケーション作成","content":" streamlit # アプリの作り方 # 以下のyoutube動画とgithubのコードを参考にしてください。 実行に必要な環境はpython3.9かそれ以降のバージョンが動作する環境のみです。\nアプリの起動方法 # githubからコードをクローンしてくる pip3 install -r requirements.txt streamlit run app.py --server.address=localhost をコマンドラインで実行 http://localhost:8501にアクセス 元の動画 # "},{"id":14,"href":"/timeseries/preprocess/001-check-data/","title":"データの確認","section":"プロットと前処理","content":"\rデータの中身を見る # import datetime import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns from scipy import stats from statsmodels.tsa import stattools データセットの読み込み # data = pd.read_csv(\u0026#34;sample.csv\u0026#34;) data.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rタイムスタンプをdatetimeにする # Date列は現在Object型、つまり文字列として読み込まれています。これをタイムスタンプとして扱うため、 datetime \u0026mdash; 基本的な日付型および時間型を確認しつつ、datetime型に変換します。\ndata[\u0026#34;Date\u0026#34;] = data[\u0026#34;Date\u0026#34;].apply( lambda x: datetime.datetime.strptime(str(x), \u0026#34;%Y-%m-%d\u0026#34;) ) print(f\u0026#34;Date列のdtype: {data[\u0026#39;Date\u0026#39;].dtype}\u0026#34;) Date列のdtype: datetime64[ns]\r時系列の概要を確認する # pandas.DataFrame.describe # はじめに、データがどのようなものであるか簡単に確認します。 pandas.DataFrame.describeを使って、Temp列の簡単な統計量を確認します。\ndata.describe() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r折れ線グラフ # seaborn.lineplotを使って、どのような周期になっているか確認します。\nplt.figure(figsize=(12, 6)) sns.lineplot(x=data[\u0026#34;Date\u0026#34;], y=data[\u0026#34;Temp\u0026#34;]) plt.ylabel(\u0026#34;Temp\u0026#34;) plt.grid(axis=\u0026#34;x\u0026#34;) plt.grid(axis=\u0026#34;y\u0026#34;, color=\u0026#34;r\u0026#34;, alpha=0.3) plt.show() ヒストグラム # plt.figure(figsize=(12, 6)) plt.hist(x=data[\u0026#34;Temp\u0026#34;], rwidth=0.8) plt.xlabel(\u0026#34;Temp\u0026#34;) plt.ylabel(\u0026#34;日数\u0026#34;) plt.grid(axis=\u0026#34;y\u0026#34;) plt.show() 自己相関とコレログラム # pandas.plotting.autocorrelation_plotを用いて自己相関を確認し、時系列データの周期性をチェックします。\n大雑把に言うと、自己相関とは、信号がそれ自身を時間シフトした信号とどれくらい一致するかを測る尺度であり、時間シフトの大きさの関数として表される。　引用元：wikipedia - 自己相関\nplt.figure(figsize=(12, 6)) pd.plotting.autocorrelation_plot(data[\u0026#34;Temp\u0026#34;]) plt.grid() plt.axvline(x=365) plt.xlabel(\u0026#34;ラグ\u0026#34;) plt.ylabel(\u0026#34;自己相関\u0026#34;) plt.show() 単位根検定 # データが単位根過程であるかどうかを確認します。 単位根過程であることを帰無仮説とする検定（Augmented Dickey-Fuller test）を行います。\nstatsmodels.tsa.stattools.adfuller\nstattools.adfuller(data[\u0026#34;Temp\u0026#34;], autolag=\u0026#34;AIC\u0026#34;) (-4.444804924611697,\r0.00024708263003610177,\r20,\r3629,\r{'1%': -3.4321532327220154,\r'5%': -2.862336767636517,\r'10%': -2.56719413172842},\r16642.822304301197)\rトレンドの確認 # 1次元多項式を時系列にフィットさせてトレンドの線を引きます。今回のデータはほとんどトレンド定常に近いので、トレンドはほとんどありません。\nnumpy.poly1d — NumPy v1.22 Manual\ndef get_trend(timeseries, deg=3): \u0026#34;\u0026#34;\u0026#34;時系列データのトレンドの線を作成する Args: timeseries(pd.Series) : 時系列データ。 Returns: pd.Series: トレンドに相当する時系列データ。 \u0026#34;\u0026#34;\u0026#34; x = list(range(len(timeseries))) y = timeseries.values coef = np.polyfit(x, y, deg) trend = np.poly1d(coef)(x) return pd.Series(data=trend, index=timeseries.index) data[\u0026#34;Trend\u0026#34;] = get_trend(data[\u0026#34;Temp\u0026#34;]) # グラフをプロット plt.figure(figsize=(12, 6)) sns.lineplot(x=data[\u0026#34;Date\u0026#34;], y=data[\u0026#34;Temp\u0026#34;], alpha=0.5, label=\u0026#34;Temp\u0026#34;) sns.lineplot(x=data[\u0026#34;Date\u0026#34;], y=data[\u0026#34;Trend\u0026#34;], label=\u0026#34;トレンド\u0026#34;) plt.grid(axis=\u0026#34;x\u0026#34;) plt.legend() plt.show() 補足：トレンドがはっきりとある場合 # 緑色がトレンドとなる線です。\ndata_sub = data.copy() data_sub[\u0026#34;Temp\u0026#34;] = ( data_sub[\u0026#34;Temp\u0026#34;] + np.log(data_sub[\u0026#34;Date\u0026#34;].dt.year - 1980) * 10 ) # ダミーのトレンド data_sub[\u0026#34;Trend\u0026#34;] = get_trend(data_sub[\u0026#34;Temp\u0026#34;]) # グラフをプロット plt.figure(figsize=(12, 6)) sns.lineplot(x=data_sub[\u0026#34;Date\u0026#34;], y=data_sub[\u0026#34;Temp\u0026#34;], alpha=0.5, label=\u0026#34;Temp\u0026#34;) sns.lineplot(x=data_sub[\u0026#34;Date\u0026#34;], y=data_sub[\u0026#34;Trend\u0026#34;], label=\u0026#34;トレンド\u0026#34;) plt.grid(axis=\u0026#34;x\u0026#34;) plt.legend() plt.show() "},{"id":15,"href":"/prep/howtouse/","title":"データ前処理する前に","section":"データ前処理","content":"データ前処理をしたあとで機械学習を使って意思決定をする際、モデルの出力はさまざまな要素によって変化します。たとえば、\nデータそのもののバイアス 標本の選択の仕方によるバイアス 帰納バイアス (\u0026quot;機械学習手法が汎化のために採用している仮定が，実世界の状況とはずれている場合[引用元])\u0026quot; などが挙げられます。そのため、データ前処理をする時は必ずそれが後続の処理（前処理・機械学習モデル・後処理・意思決定）にどのような影響を与えるかを確認しましょう。たとえば、特定の県の人のデータだけフィルタリングしてモデルを作成すると、偏りのある予測をするモデルができるかもしれません。たとえ明示的に特定の県をフィルタリングするようなことがなくても、欠損の多いデータをフィルタリングした結果、実は特定の県のデータのみ欠損が多く、それが結果的に偏りにつながるかもしれません。前処理をする際は意図した通りの前処理ができているか、正しく運用できているかどうかを常にチェックする必要があります。\n参考サイト # [1] 人工知能学会倫理委員会 機械学習と公平性に関する声明 [2] 機械学習と公平性に関する声明とシンポジウム [3] 私のブックマーク「機械学習のプライバシーとセキュリティ（Privacy and security issues in machine learning）」 [4] ＡＩと著作権　学習用データセットの生成 [5] EU 一般データ保護規則（GDPR）の概要（前編） [6] 著作権 | 文化庁 [7] GDPR（General Data Protection Regulation：一般データ保護規則） "},{"id":16,"href":"/finance/visualize/country_risk/","title":"カントリーリスクプレミアム","section":"可視化","content":"\rもとのデータは Country Default Spreads and Risk Premiums (Last updated: January 5, 2022) よりインポートしています。\n"},{"id":17,"href":"/prep/data/sample-data/","title":"サンプルデータ","section":"データの読み込み・集約","content":" 総務省のデータ # 総務省のデータをつかってpandasを動かしてみましょう。\n当ホームページ(総務省)で公開している情報（以下「コンテンツ」といいます。）は、どなたでも以下の1）～7）に従って、複製、公衆送信、翻訳・変形等の翻案等、自由に利用できます。商用利用も可能です。また、数値データ、簡単な表・グラフ等は著作権の対象ではありませんので、これらについては本利用ルールの適用はなく、自由に利用できます。コンテンツ利用に当たっては、本利用ルールに同意したものとみなします。 引用元：当省ホームページについて\nimport pandas as pd import os from urllib.parse import urlparse from IPython.display import display, HTML def getfn(url: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;urlからファイル名を取得する\u0026#34;\u0026#34;\u0026#34; return os.path.basename(urlparse(url).path) def disp(df: pd.DataFrame, text: str): \u0026#34;\u0026#34;\u0026#34;notebook上にデータとテキストを表示する\u0026#34;\u0026#34;\u0026#34; display(HTML(f\u0026#34;\u0026lt;h3\u0026gt;{text}\u0026lt;/h3\u0026gt;\u0026#34;)) display(df.head(6)) display(HTML(f\u0026#34;\u0026lt;hr /\u0026gt;\u0026#34;)) 国内総生産の増加率に対する寄与度 # 出典：令和3年版地方財政白書（https://www.soumu.go.jp/menu_seisaku/hakusyo/chihou/r03data/2021data/r03czb01-01.html#p010102）\ndf = pd.read_csv(\u0026#34;z-006-pre.csv\u0026#34;) disp(df, \u0026#34;加工前\u0026#34;) df.columns = df.iloc[1] df = df.drop([0, 1]).replace(\u0026#34;-\u0026#34;, 0) df.to_csv(\u0026#34;z-006.csv\u0026#34;, encoding=\u0026#34;utf-8-sig\u0026#34;) disp(df, \u0026#34;加工後\u0026#34;) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r国・地方を通じた純計歳出規模 # 出典：令和3年版地方財政白書 （https://www.soumu.go.jp/menu_seisaku/hakusyo/chihou/r03data/2021data/r03czb01-01.html#p010102）\ndf = pd.read_csv(\u0026#34;z-002-pre.csv\u0026#34;) df.to_csv(\u0026#34;z-002-pre.csv\u0026#34;, index=None) disp(df, \u0026#34;加工前\u0026#34;) df = df.drop([0, 1, 2]) df.to_csv(\u0026#34;z-002.csv\u0026#34;, encoding=\u0026#34;utf-8-sig\u0026#34;) disp(df, \u0026#34;加工後\u0026#34;) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r国・地方を通じた目的別歳出構成比の推移 # 出典：令和3年版地方財政白書（https://www.soumu.go.jp/menu_seisaku/hakusyo/chihou/r03data/2021data/r03czb01-01.html#p010102）\ndf = pd.read_csv(\u0026#34;z-001-pre.csv\u0026#34;) disp(df, \u0026#34;加工前\u0026#34;) df.columns = df.iloc[1] df = df.drop([0, 1, 8]) df.to_csv(\u0026#34;z-001.csv\u0026#34;, encoding=\u0026#34;utf-8-sig\u0026#34;) disp(df, \u0026#34;加工後\u0026#34;) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":18,"href":"/finance/nlp/001-sentiment-analysis/","title":"テキストの感情分析","section":"テキスト処理","content":"\r『SiEBERT - English-Language Sentiment Classification』のモデルを使って英文の感情分類をします。英文の各文章の感情をポジティブ・ネガティブで分類してみたいと思います。\nHartmann, Jochen and Heitmann, Mark and Siebert, Christian and Schamp, Christina, \u0026ldquo;More than a feeling: Accuracy and Application of Sentiment Analysis\u0026rdquo;, International Journal of Research in Marketing(2022)\nここではhuggingface上のsiebert/sentiment-roberta-large-englishのモデルを使用しています。Google Colab上でtransformersを使用する場合は事前にtransformersをインストールする必要があります。\nimport numpy as np from transformers import pipeline from IPython.display import HTML sentiment_pipeline = pipeline( \u0026#34;sentiment-analysis\u0026#34;, model=\u0026#34;siebert/sentiment-roberta-large-english\u0026#34; ) テキストの各文の感情を分析してみる # テキスト全体を「.」で区切ることで、一文ごとに分けています。\nここではPetrobras Webcast – 3rd Quarter Results 2022 November 5, 2022の文字起こしデータを使用しています。\ntranscript = \u0026#34;\u0026#34;\u0026#34;Hello!Hello!Hello!Hello!Hello!\u0026#34;\u0026#34;\u0026#34; ts_list = [ts for ts in transcript.split(\u0026#34;.\u0026#34;) if len(ts) \u0026gt; 20] scores = sentiment_pipeline(ts_list) 結果を可視化 # ポジティブ・ネガティブのラベルと、そのスコアを用いて結果を可視化してみます。\nfor t, s in zip(ts_list, scores): score = np.round(float(s[\u0026#34;score\u0026#34;]), 4) # 感情スコア font_weight = \u0026#34;bold\u0026#34; if score \u0026gt; 0.995 else \u0026#34;normal\u0026#34; # 表示する文字の太さ # 感情ごとに色を分けて表示 if s[\u0026#34;label\u0026#34;] == \u0026#34;NEGATIVE\u0026#34;: r = 255 - 10 * int(1000 - score * 1000) display( HTML( f\u0026#34;[score={score}] \u0026lt;span style=\u0026#39;color:rgb({r},100,100);font-weight:{font_weight};\u0026#39;\u0026gt;{t}\u0026lt;/span\u0026gt;\u0026#34; ) ) elif s[\u0026#34;label\u0026#34;] == \u0026#34;POSITIVE\u0026#34;: g = 255 - 10 * int(1000 - score * 1000) display( HTML( f\u0026#34;[score={score}] \u0026lt;span style=\u0026#39;color:rgb(100,{g},100);font-weight:{font_weight};\u0026#39;\u0026gt;{t}\u0026lt;/span\u0026gt;\u0026#34; ) ) [score=0.9976] Hello!Hello!Hello!Hello!Hello!\n"},{"id":19,"href":"/install/","title":"はじめに","section":"トップページ","content":" はじめに # このサイトは私が勉強した内容をVlog・コードの形で書き残すページです。 2021年10月初めに公開して以降、データ分析に必要な処理や機械学習に関するトピックを中心にまとめています。\n機械学習：アルゴリズムの仕組みや実際に動かしてみた系 データ前処理：色々なデータのデータ加工/前処理について 評価指標：モデル選択やモデルの性能評価について 時系列：タイムスタンプと数値が合わさったデータについて コードの動かし方 # Google Colaboratory(Colab)\rAnaconda\rPoetry\rGoogle Colaboratory(Colab)はGoogleが提供するブラウザで動作するPython実行環境の名称であり、 環境構築なしで簡単にコードを実行して共有できるサービスです。利用にはGoogleアカウントが必要です。 各ページの右上にあるColabボタンをクリックすると、そのページのコードをGoogle Colaboratory上で実行できます。\nColabについては「Colaboratory よくある質問」などのページもご参照ください。\nAnacondaで仮想環境を作成したのち、各コードで必要なライブラリ・パッケージをインストールしてください。 ただし、このサイトではAnacondaは一切使用していないので動作確認はしていない点ご了承ください。\nconda create -n py38_env python=3.8\rconda install ライブラリ名 Poetry | Installationに記載されている方法でpoetryをインストール後、pyproject.toml のファイルがある階層にて poetry install を実行してください。\n"},{"id":20,"href":"/visualize/distribution/histogram/","title":"ヒストグラム","section":"数値の分布","content":"密度プロットと同様に、数値データがどのように分布しているかを可視化します。\nsns.histplot\nimport matplotlib.pyplot as plt import seaborn as sns df = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (12, 8)}) sns.histplot(data=df[\u0026#34;sepal_length\u0026#34;], binwidth=0.5) "},{"id":21,"href":"/timeseries/preprocess/","title":"プロットと前処理","section":"時系列","content":" Chapter 1 # プロットと前処理 # "},{"id":22,"href":"/eval/model-selection/","title":"モデル選択","section":"評価指標","content":" Chapter 1 # モデル選択 # "},{"id":23,"href":"/basic/ensemble/randomforest/","title":"ランダムフォレスト","section":"アンサンブル","content":"\rsklearn.ensemble.RandomForestClassifier\nimport numpy as np import matplotlib.pyplot as plt ランダムフォレストを学習 # ROC-AUCについては、ROC-AUCにプロットの仕方について説明を載せています。\nsklearn.metrics.roc_auc_score sklearn.model_selection.train_test_split from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score n_features = 20 X, y = make_classification( n_samples=2500, n_features=n_features, n_informative=10, n_classes=2, n_redundant=0, n_clusters_per_class=4, random_state=777, ) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=777 ) model = RandomForestClassifier( n_estimators=50, max_depth=3, random_state=777, bootstrap=True, oob_score=True ) model.fit(X_train, y_train) y_pred = model.predict(X_test) rf_score = roc_auc_score(y_test, y_pred) print(f\u0026#34;テストデータでのROC-AUC = {rf_score}\u0026#34;) テストデータでのROC-AUC = 0.814573097628059\rランダムフォレストに含まれる各木の性能を確認する # import japanize_matplotlib estimator_scores = [] for i in range(10): estimator = model.estimators_[i] estimator_pred = estimator.predict(X_test) estimator_scores.append(roc_auc_score(y_test, estimator_pred)) plt.figure(figsize=(10, 4)) bar_index = [i for i in range(len(estimator_scores))] plt.bar(bar_index, estimator_scores) plt.bar([10], rf_score) plt.xticks(bar_index + [10], bar_index + [\u0026#34;RF\u0026#34;]) plt.xlabel(\u0026#34;木のインデックス\u0026#34;) plt.ylabel(\u0026#34;ROC-AUC\u0026#34;) plt.show() 特徴の重要度 # 不純度(impurity)に基づいた重要度 # plt.figure(figsize=(10, 4)) feature_index = [i for i in range(n_features)] plt.bar(feature_index, model.feature_importances_) plt.xlabel(\u0026#34;特徴のインデックス\u0026#34;) plt.ylabel(\u0026#34;特徴の重要度\u0026#34;) plt.show() permutation importance # permutation_importance\nfrom sklearn.inspection import permutation_importance p_imp = permutation_importance( model, X_train, y_train, n_repeats=10, random_state=77 ).importances_mean plt.figure(figsize=(10, 4)) plt.bar(feature_index, p_imp) plt.xlabel(\u0026#34;特徴のインデックス\u0026#34;) plt.ylabel(\u0026#34;特徴の重要度\u0026#34;) plt.show() ランダムフォレストに含まれる各木を出力する # from sklearn.tree import export_graphviz from subprocess import call from IPython.display import Image from IPython.display import display for i in range(10): try: estimator = model.estimators_[i] export_graphviz( estimator, out_file=f\u0026#34;tree{i}.dot\u0026#34;, feature_names=[f\u0026#34;x{i}\u0026#34; for i in range(n_features)], class_names=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;], proportion=True, filled=True, ) call([\u0026#34;dot\u0026#34;, \u0026#34;-Tpng\u0026#34;, f\u0026#34;tree{i}.dot\u0026#34;, \u0026#34;-o\u0026#34;, f\u0026#34;tree{i}.png\u0026#34;, \u0026#34;-Gdpi=500\u0026#34;]) display(Image(filename=f\u0026#34;tree{i}.png\u0026#34;)) except KeyboardInterrupt: # TODO: jupyter bookビルド時に出力に失敗するため一時的に例外処理を挟む pass OOB(out-of-bag) Score # OOBによる検証とテストデータでの検証結果が近い値を取ることが確認できます。 乱数と木の深さを変えつつ、OOBでのAccuracyとテストデータでのAccuracyを比較します。\nfrom sklearn.metrics import accuracy_score for i in range(10): model_i = RandomForestClassifier( n_estimators=50, max_depth=3 + i % 2, random_state=i, bootstrap=True, oob_score=True, ) model_i.fit(X_train, y_train) y_pred = model_i.predict(X_test) oob_score = model_i.oob_score_ test_score = accuracy_score(y_test, y_pred) print(f\u0026#34;OOBでの検証結果＝{oob_score} テストデータでの検証結果＝{test_score}\u0026#34;) OOBでの検証結果＝0.786865671641791 テストデータでの検証結果＝0.8121212121212121\rOOBでの検証結果＝0.8101492537313433 テストデータでの検証結果＝0.8363636363636363\rOOBでの検証結果＝0.7886567164179105 テストデータでの検証結果＝0.8024242424242424\rOOBでの検証結果＝0.8161194029850747 テストデータでの検証結果＝0.8315151515151515\rOOBでの検証結果＝0.7910447761194029 テストデータでの検証結果＝0.8072727272727273\rOOBでの検証結果＝0.8101492537313433 テストデータでの検証結果＝0.833939393939394\rOOBでの検証結果＝0.7814925373134328 テストデータでの検証結果＝0.8133333333333334\rOOBでの検証結果＝0.8059701492537313 テストデータでの検証結果＝0.833939393939394\rOOBでの検証結果＝0.7832835820895523 テストデータでの検証結果＝0.7951515151515152\rOOBでの検証結果＝0.8083582089552239 テストデータでの検証結果＝0.8387878787878787\r"},{"id":24,"href":"/basic/classification/logistic_regression/","title":"ロジスティック回帰","section":"線形分類","content":" import matplotlib.pyplot as plt import numpy as np ロジット関数の可視化 # fig = plt.figure(figsize=(4, 8)) p = np.linspace(0.01, 0.999, num=100) y = np.log(p / (1 - p)) plt.plot(p, y) plt.xlabel(\u0026#34;p\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.axhline(y=0, color=\u0026#34;k\u0026#34;) plt.ylim(-3, 3) plt.grid() plt.show() ロジスティック回帰 # 人工的に生成したデータに対してロジスティック回帰を実行します。\nsklearn.linear_model.LogisticRegression sklearn.datasets.make_classification from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification ## データセット X, Y = make_classification( n_samples=300, n_features=2, n_redundant=0, n_informative=1, random_state=2, n_clusters_per_class=1, ) # ロジスティック回帰を学習 clf = LogisticRegression() clf.fit(X, Y) # 決定境界の直線を求める b = clf.intercept_[0] w1, w2 = clf.coef_.T b_1 = -w1 / w2 b_0 = -b / w2 xmin, xmax = np.min(X[:, 0]), np.max(X[:, 0]) ymin, ymax = np.min(X[:, 1]), np.max(X[:, 1]) xd = np.array([xmin, xmax]) yd = b_1 * xd + b_0 # グラフを描画 plt.figure(figsize=(10, 10)) plt.plot(xd, yd, \u0026#34;k\u0026#34;, lw=1, ls=\u0026#34;-\u0026#34;) plt.scatter(*X[Y == 0].T, marker=\u0026#34;o\u0026#34;) plt.scatter(*X[Y == 1].T, marker=\u0026#34;x\u0026#34;) plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.show() "},{"id":25,"href":"/basic/anomaly/adtk1/","title":"異常検知①","section":"異常検知","content":"Anomaly Detection Toolkit (ADTK)を使った異常検知をしてみます。 オリジナルデータはNumenta Anomaly Benchmarkのものを使用しています。\nimport pandas as pd s_train = pd.read_csv( \u0026#34;./training.csv\u0026#34;, index_col=\u0026#34;timestamp\u0026#34;, parse_dates=True, squeeze=True ) from adtk.data import validate_series s_train = validate_series(s_train) print(s_train) timestamp\r2014-04-01 00:00:00 18.090486\r2014-04-01 00:05:00 20.359843\r2014-04-01 00:10:00 21.105470\r2014-04-01 00:15:00 21.151585\r2014-04-01 00:20:00 18.137141\r... 2014-04-14 23:35:00 18.269290\r2014-04-14 23:40:00 19.087351\r2014-04-14 23:45:00 19.594689\r2014-04-14 23:50:00 19.767817\r2014-04-14 23:55:00 20.479156\rFreq: 5T, Name: value, Length: 4032, dtype: float64\rs_train = pd.read_csv(\u0026quot;./training.csv\u0026quot;, index_col=\u0026quot;timestamp\u0026quot;, parse_dates=True, squeeze=True)\rfrom adtk.visualization import plot plot(s_train) 異常検知手法の比較 # SeasonalADを用いた異常検知を行います。他の手法はDetectorを参照してください。\nimport matplotlib.pyplot as plt from adtk.detector import ( AutoregressionAD, InterQuartileRangeAD, LevelShiftAD, PersistAD, SeasonalAD, ) model_dict = { \u0026#34;LevelShiftAD\u0026#34;: LevelShiftAD(window=5), \u0026#34;SeasonalAD\u0026#34;: SeasonalAD(), \u0026#34;PersistAD\u0026#34;: PersistAD(c=3.0, side=\u0026#34;positive\u0026#34;), \u0026#34;InterQuartileRangeAD\u0026#34;: InterQuartileRangeAD(c=1.5), \u0026#34;AutoregressionAD\u0026#34;: AutoregressionAD(n_steps=7 * 2, step_size=24, c=3.0), } for model_name, model in model_dict.items(): anomalies = model.fit_detect(s_train) plot(s_train, anomaly=anomalies, anomaly_color=\u0026#34;red\u0026#34;, anomaly_tag=\u0026#34;marker\u0026#34;) plt.title(model_name) plt.show() "},{"id":26,"href":"/timeseries/hts/001-hts/","title":"階層的な構造","section":"階層化・グループ化","content":"時系列データが複数あるとき、そのデータが階層的な構造を持つことがあります。\n例：地域・カテゴリごとの販売データ # 経済産業省のサイトでは『商業動態統計商業動態統計』が定期的に公開されています。 これは、日本の商業を営む企業の販売活動の動向をチェックすることを目的として集められているデータです。たとえば、2022年1月の付表3　百貨店・スーパー販売　都道府県別、商品別販売額等には以下のようなデータが含まれています。\nこのデータには以下のような階層構造が含まれています。\n"},{"id":27,"href":"/basic/tree/decision_tree_classifier/","title":"決定木(分類)","section":"決定木","content":"\rimport numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier, plot_tree サンプルデータを作成 # ２クラス分類をするためのサンプルデータを作成します。\nn_classes = 2 X, y = make_classification( n_samples=100, n_features=2, n_redundant=0, n_informative=2, random_state=2, n_classes=n_classes, n_clusters_per_class=1, ) 決定木を作成 # DecisionTreeClassifier(criterion=\u0026quot;gini\u0026quot;).fit(X, y)でモデルを訓練し、作成した木の決定境界を可視化します。 criterion=\u0026quot;gini\u0026quot;は分岐を決めるための指標を指定するオプションです。\nsklearn.tree.DecisionTreeClassifier\n# モデルの訓練 clf = DecisionTreeClassifier(criterion=\u0026#34;gini\u0026#34;).fit(X, y) # 決定境界のカラーマップ用データ x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # 決定境界を可視化 plt.figure(figsize=(8, 8)) plt.tight_layout() cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1) plt.xlabel(\u0026#34;x1\u0026#34;) plt.ylabel(\u0026#34;x2\u0026#34;) # ラベルごとに色を分けてプロット for i, color, label_name in zip(range(n_classes), [\u0026#34;r\u0026#34;, \u0026#34;b\u0026#34;], [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;]): idx = np.where(y == i) plt.scatter(X[idx, 0], X[idx, 1], c=color, label=label_name, cmap=plt.cm.Pastel1) plt.legend() plt.show() 決定木の構造を画像で出力 # sklearn.tree.plot_tree — scikit-learn 1.0.2 documentation\nplt.figure() clf = DecisionTreeClassifier(criterion=\u0026#34;gini\u0026#34;).fit(X, y) plt.figure(figsize=(12, 12)) plot_tree(clf, filled=True) plt.show() \u0026lt;Figure size 432x288 with 0 Axes\u0026gt;\r"},{"id":28,"href":"/eval/model-selection/crossvalidation/","title":"交差検証","section":"モデル選択","content":" 標本データを分割し、その一部をまず解析して、残る部分でその解析のテストを行い、解析自身の妥当性の検証・確認に当てる手法 交差検証 出典: フリー百科事典『ウィキペディア（Wikipedia）』\nimport numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_validate from sklearn.metrics import roc_auc_score サンプルデータに対してモデルを作成し交差検証 # 実験用データ # X, y = make_classification( n_samples=300, n_classes=2, n_informative=4, n_features=6, weights=[0.2, 0.8], n_clusters_per_class=2, shuffle=True, random_state=RND, ) train_valid_X, test_X, train_valid_y, test_y = train_test_split( X, y, test_size=0.2, stratify=y, random_state=RND ) 交差検証なしの場合のテストデータでのモデル精度 # 一度だけ train_test_split を実行してそのテストデータで評価します。\ntrain_X, valid_X, train_y, valid_y = train_test_split( train_valid_X, train_valid_y, test_size=0.2, random_state=RND ) model = RandomForestClassifier(max_depth=4, random_state=RND) model.fit(train_X, train_y) pred_y = model.predict(valid_X) rocauc = roc_auc_score(valid_y, pred_y) print(f\u0026#34;ROC-AUC = {rocauc}\u0026#34;) ROC-AUC = 0.5277777777777778\r交差検証時のスコア # データを１０分割して交差検証をして、ROC-AUCの平均値を指標として使用します。sklearn.model_selection.cross_val_scoreを用いると複数の評価指標で簡単に交差検証のスコアを算出できます。 以下の例では、ROC-AUCとAccuracyを交差検証して求めます。\nmetrics = (\u0026#34;roc_auc\u0026#34;, \u0026#34;accuracy\u0026#34;) model = RandomForestClassifier(max_depth=4, random_state=RND) cv_scores = cross_validate( model, train_valid_X, train_valid_y, cv=5, scoring=metrics, return_train_score=True ) for m in metrics: cv_m = cv_scores[f\u0026#34;test_{m}\u0026#34;] print(f\u0026#34;{m} {np.mean(cv_m)}\u0026#34;) roc_auc 0.8443019943019943\raccuracy 0.8583333333333334\rテストデータでの性能 # model = RandomForestClassifier(max_depth=4, random_state=RND).fit( train_valid_X, train_valid_y ) pred_y = model.predict(test_X) rocauc = roc_auc_score(test_y, pred_y) print(f\u0026#34;test ROC-AUC = {rocauc}\u0026#34;) test ROC-AUC = 0.8125000000000001\r"},{"id":29,"href":"/basic/regression/linear_regression/","title":"最小二乗法","section":"線形回帰","content":"\rimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler japanize_matplotlib はグラフに日本語を表示するためにインポートしています。\n実験用の回帰データを作成 # np.linspace を使ってデータを作成します。これは、指定した値の間を等間隔に区切った値のリストを作成します。 以下のコードでは線形回帰をするためのサンプルデータを500個作成しています。\n# 訓練データ n_samples = 500 X = np.linspace(-10, 10, n_samples)[:, np.newaxis] epsolon = np.random.normal(size=n_samples) y = np.linspace(-2, 2, n_samples) + epsolon # 直線を可視化 plt.figure(figsize=(10, 5)) plt.scatter(X, y, marker=\u0026#34;x\u0026#34;, label=\u0026#34;正解データ\u0026#34;, c=\u0026#34;orange\u0026#34;) plt.xlabel(\u0026#34;$x_1$\u0026#34;) plt.xlabel(\u0026#34;$y$\u0026#34;) plt.legend() plt.show() yに乗っているノイズの確認 # y = np.linspace(-2, 2, n_samples) + epsolon の epsolon のヒストグラムをプロットします。 正規分布に近い分布のノイズが目的変数に乗っていることを確認します。\nplt.figure(figsize=(10, 5)) plt.hist(epsolon) plt.xlabel(\u0026#34;$\\epsilon$\u0026#34;) plt.ylabel(\u0026#34;データ数\u0026#34;) plt.show() 最小二乗法で直線を当てはめる # sklearn.linear_model.LinearRegression\n# モデルをフィット lin_r = make_pipeline(StandardScaler(with_mean=False), LinearRegression()).fit(X, y) y_pred = lin_r.predict(X) # 直線を可視化 plt.figure(figsize=(10, 5)) plt.scatter(X, y, marker=\u0026#34;x\u0026#34;, label=\u0026#34;正解データ\u0026#34;, c=\u0026#34;orange\u0026#34;) plt.plot(X, y_pred, label=\u0026#34;線形回帰でフィットさせた直線\u0026#34;) plt.xlabel(\u0026#34;$x_1$\u0026#34;) plt.xlabel(\u0026#34;$y$\u0026#34;) plt.legend() plt.show() "},{"id":30,"href":"/finance/main/","title":"時系列データ","section":"経済データ","content":" Chapter 1 # 時系列データ # "},{"id":31,"href":"/prep/numerical/binning/","title":"数値をビン化","section":"数値データ","content":"import numpy as np import pandas as pd df = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rデータを各分位点でビン化 # pandas.qcut\nデータをソートした時に、全体のX％でどれくらいの値になるかを基準としてビニング。\n0.0 46.0\r0.1 18002.9 \u0026lt;- 10%の値\r0.2 20476.8\r0.3 22755.0\r0.4 26204.8\r0.5 30824.0\r0.6 45622.6\r0.7 89873.9\r0.8 245544.0\r0.9 290714.1 \u0026lt;- 90%の値\r1.0 765403.0 df[\u0026#34;人口総数_ビン化\u0026#34;] = pd.qcut(df[\u0026#34;人口総数\u0026#34;], q=11) df[[\u0026#34;人口総数\u0026#34;, \u0026#34;人口総数_ビン化\u0026#34;]].head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":32,"href":"/basic/regression/","title":"線形回帰","section":"機械学習","content":" Chapter 1 # 線形回帰 # "},{"id":33,"href":"/eval/regression/correlation_coefficient/","title":"相関係数","section":"回帰","content":"\r相関係数とは、2つのデータまたは確率変数の間にある線形な関係の強弱を測る指標である。 出典： 相関係数 出典: フリー百科事典『ウィキペディア（Wikipedia）』\n二つの変数の間に直線的な形の傾向変化があるかどうかを確認することができる指標であり、式で表すと以下のようになります。\n$ \\frac{\\Sigma_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma_{i=1}^N(x_i - \\bar{x})^2 \\Sigma_{i=1}^N(y_i - \\bar{y})^2 }} $\n以下のような性質があります。\n−1 以上 1 以下の値をとる 相関係数が1に近いならば、$x$が増加 → $y$も増加 $x, y$を低数倍しても相関係数の値は変化しない 二つの数値列の相関係数を計算 # import numpy as np np.random.seed(777) # 乱数を固定するため import matplotlib.pyplot as plt import numpy as np x = [xi + np.random.rand() for xi in np.linspace(0, 100, 40)] y = [yi + np.random.rand() for yi in np.linspace(1, 50, 40)] plt.figure(figsize=(5, 5)) plt.scatter(x, y) plt.show() coef = np.corrcoef(x, y) print(coef) [[1. 0.99979848]\r[0.99979848 1. ]]\r複数の変数の間の相関係数をまとめて計算 # pandas.io.formats.style.Styler.background_gradient\nimport seaborn as sns df = sns.load_dataset(\u0026#34;iris\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r全ての変数間の相関係数を確認する # irisデータセットを使って、変数間の相関を見てみます。\nThe Iris Dataset pandas.io.formats.style.Styler.background_gradient df.corr().style.background_gradient(cmap=\u0026#34;YlOrRd\u0026#34;) ヒートマップだとどこが最も相関が高いか見づらいです。sepal_length と相関が高い変数はどれか、棒グラフで確認します。\npandas.DataFrame.plot.bar\ndf.corr()[\u0026#34;sepal_length\u0026#34;].plot.bar(grid=True, ylabel=\u0026#34;corr\u0026#34;) \u0026lt;AxesSubplot:ylabel='corr'\u0026gt;\r相関係数が低い場合 # 相関係数が低い場合のデータ分布を確認し、変数間に関係がある場合でも相関係数が低くなる可能性があることを確認します。\nnumpy.random.multivariate_normal — NumPy v1.22 Manual\nn_samples = 1000 plt.figure(figsize=(12, 12)) for i, ci in enumerate(np.linspace(-1, 1, 16)): ci = np.round(ci, 4) mean = np.array([0, 0]) cov = np.array([[1, ci], [ci, 1]]) v1, v2 = np.random.multivariate_normal(mean, cov, size=n_samples).T plt.subplot(4, 4, i + 1) plt.plot(v1, v2, \u0026#34;x\u0026#34;) plt.title(f\u0026#34;r={ci}\u0026#34;) plt.tight_layout() plt.show() 相関係数が低くても、変数の間に関係があるケースもあります。 そのような例を簡単にですが作ってみます。\nimport japanize_matplotlib from sklearn import datasets japanize_matplotlib.japanize() n_samples = 1000 circle, _ = datasets.make_circles(n_samples=n_samples, factor=0.1, noise=0.05) moon, _ = datasets.make_moons(n_samples=n_samples, noise=0.05) corr_circle = np.round(np.corrcoef(circle[:, 0], circle[:, 1])[1, 0], 4) plt.title(f\u0026#34;相関係数={corr_circle}\u0026#34;, fontsize=23) plt.scatter(circle[:, 0], circle[:, 1]) plt.show() corr_moon = np.round(np.corrcoef(moon[:, 0], moon[:, 1])[1, 0], 4) plt.title(f\u0026#34;相関係数={corr_moon}\u0026#34;, fontsize=23) plt.scatter(moon[:, 0], moon[:, 1]) plt.show() "},{"id":34,"href":"/timeseries/shape/001_dtw/","title":"動的時間伸縮法(DTW)","section":"形状・類似度","content":"\rimport numpy as np import matplotlib.pyplot as plt from fastdtw import fastdtw 実験に使用する二つの波形をプロット # data1 = [91.0 * np.sin(i / 2.1) for i in range(30)] data2 = [100.0 * np.sin(i / 2.0) + np.random.rand() for i in range(30)] data3 = [50.0 * np.cos(i / 2.0) + np.random.rand() for i in range(30)] plt.figure(figsize=(12, 4)) # 波形をプロット plt.plot(data1, label=\u0026#34;data1\u0026#34;, color=\u0026#34;k\u0026#34;) plt.plot(data2, label=\u0026#34;data2\u0026#34;, color=\u0026#34;r\u0026#34;) plt.plot(data3, label=\u0026#34;data3\u0026#34;, color=\u0026#34;b\u0026#34;) plt.legend() plt.show() DTWを計算し対応関係をプロットする # 黒色と赤色の波形の間のDTWが小さい値を示しており、青色の波形よりも（DTWという指標の上では）似ているという判断ができます。\n# DTWを計算 distance_12, path_12 = fastdtw(data1, data2) distance_13, path_13 = fastdtw(data1, data3) # 対応するポイントを線で結ぶ plt.figure(figsize=(12, 4)) for x_12, x_13 in zip(path_12, path_13): plt.plot(x_12, [data1[x_12[0]], data2[x_12[1]]], color=\u0026#34;r\u0026#34;, linestyle=\u0026#34;dotted\u0026#34;) plt.plot(x_13, [data1[x_13[0]], data3[x_13[1]]], color=\u0026#34;b\u0026#34;, linestyle=\u0026#34;dotted\u0026#34;) # 波形をプロット plt.plot(data1, label=\u0026#34;data1\u0026#34;, color=\u0026#34;k\u0026#34;) plt.plot(data2, label=\u0026#34;data2\u0026#34;, color=\u0026#34;r\u0026#34;) plt.plot(data3, label=\u0026#34;data2\u0026#34;, color=\u0026#34;b\u0026#34;) plt.legend() plt.title( f\u0026#34;DTW(data1, data2) {np.round(distance_12, 3)} \u0026lt; {np.round(distance_13, 3)} DTW(data1, data3)\u0026#34;, fontsize=14, ) plt.show() "},{"id":35,"href":"/visualize/category-groupby/japanmap/","title":"日本地図","section":"カテゴリと数値","content":" japanmap SaitoTsutomu/japanmap 県別データの可視化(作者本人による使い方の説明ページ) 日本地図でヒートマップなどを作成する場合に便利なパッケージです。開発者に感謝！python3.9以上のバージョンで実行する必要があります。\n地図を表示する # import matplotlib.pyplot as plt import numpy as np from japanmap import picture np.random.seed(77) plt.figure(figsize=(10, 10)) plt.xticks([]) plt.yticks([]) plt.imshow(picture()) 着色する # たとえば、以下のような県名をkey、色名をvalueとした辞書を用意することで日本地図を着色できます。\n{\u0026#39;北海道\u0026#39;: \u0026#39;#a9e5bb\u0026#39;, \u0026#39;青森\u0026#39;: \u0026#39;#fcf6b1\u0026#39;, \u0026#39;沖縄\u0026#39;: \u0026#39;#fcf6b1\u0026#39;} colors = [ \u0026#34;#e3170a\u0026#34;, \u0026#34;#a9e5bb\u0026#34;, \u0026#34;#fcf6b1\u0026#34;, \u0026#34;#f7b32b\u0026#34;, \u0026#34;#2d1e2f\u0026#34;, ] prefectures = [ \u0026#34;北海道\u0026#34;, \u0026#34;青森\u0026#34;, \u0026#34;岩手\u0026#34;, \u0026#34;宮城\u0026#34;, \u0026#34;秋田\u0026#34;, \u0026#34;山形\u0026#34;, \u0026#34;福島\u0026#34;, \u0026#34;茨城\u0026#34;, \u0026#34;栃木\u0026#34;, \u0026#34;群馬\u0026#34;, \u0026#34;埼玉\u0026#34;, \u0026#34;千葉\u0026#34;, \u0026#34;東京\u0026#34;, \u0026#34;神奈川\u0026#34;, \u0026#34;新潟\u0026#34;, \u0026#34;富山\u0026#34;, \u0026#34;石川\u0026#34;, \u0026#34;福井\u0026#34;, \u0026#34;山梨\u0026#34;, \u0026#34;長野\u0026#34;, \u0026#34;岐阜\u0026#34;, \u0026#34;静岡\u0026#34;, \u0026#34;愛知\u0026#34;, \u0026#34;三重\u0026#34;, \u0026#34;滋賀\u0026#34;, \u0026#34;京都\u0026#34;, \u0026#34;大阪\u0026#34;, \u0026#34;兵庫\u0026#34;, \u0026#34;奈良\u0026#34;, \u0026#34;和歌山\u0026#34;, \u0026#34;鳥取\u0026#34;, \u0026#34;島根\u0026#34;, \u0026#34;岡山\u0026#34;, \u0026#34;広島\u0026#34;, \u0026#34;山口\u0026#34;, \u0026#34;徳島\u0026#34;, \u0026#34;香川\u0026#34;, \u0026#34;愛媛\u0026#34;, \u0026#34;高知\u0026#34;, \u0026#34;福岡\u0026#34;, \u0026#34;佐賀\u0026#34;, \u0026#34;長崎\u0026#34;, \u0026#34;熊本\u0026#34;, \u0026#34;大分\u0026#34;, \u0026#34;宮崎\u0026#34;, \u0026#34;鹿児島\u0026#34;, \u0026#34;沖縄\u0026#34;, ] pref_color_dict = {prefecture: np.random.choice(colors) for prefecture in prefectures} plt.figure(figsize=(10, 10)) plt.xticks([]) plt.yticks([]) plt.imshow(picture(pref_color_dict)) print(f\u0026#34;入力：{pref_color_dict}\u0026#34;) 入力：{'北海道': '#2d1e2f', '青森': '#2d1e2f', '岩手': '#f7b32b', '宮城': '#e3170a', '秋田': '#e3170a', '山形': '#a9e5bb', '福島': '#2d1e2f', '茨城': '#f7b32b', '栃木': '#e3170a', '群馬': '#f7b32b', '埼玉': '#2d1e2f', '千葉': '#2d1e2f', '東京': '#fcf6b1', '神奈川': '#f7b32b', '新潟': '#f7b32b', '富山': '#a9e5bb', '石川': '#f7b32b', '福井': '#a9e5bb', '山梨': '#a9e5bb', '長野': '#2d1e2f', '岐阜': '#e3170a', '静岡': '#2d1e2f', '愛知': '#a9e5bb', '三重': '#e3170a', '滋賀': '#a9e5bb', '京都': '#fcf6b1', '大阪': '#f7b32b', '兵庫': '#2d1e2f', '奈良': '#f7b32b', '和歌山': '#a9e5bb', '鳥取': '#2d1e2f', '島根': '#e3170a', '岡山': '#2d1e2f', '広島': '#e3170a', '山口': '#a9e5bb', '徳島': '#f7b32b', '香川': '#2d1e2f', '愛媛': '#f7b32b', '高知': '#fcf6b1', '福岡': '#2d1e2f', '佐賀': '#2d1e2f', '長崎': '#2d1e2f', '熊本': '#fcf6b1', '大分': '#a9e5bb', '宮崎': '#fcf6b1', '鹿児島': '#a9e5bb', '沖縄': '#2d1e2f'}\r特定の地域のみを拡大表示する # 日本の特定の地域（関東・関西など）のみを拡大して表示することもできます。\nfrom japanmap import get_data, groups, pref_map print(f\u0026#34;groups: {groups}\u0026#34;) pref_map( groups[\u0026#34;近畿\u0026#34;], cols=[np.random.choice(colors) for _ in groups[\u0026#34;近畿\u0026#34;]], qpqo=get_data() ) groups: {'北海道': [1], '東北': [2, 3, 4, 5, 6, 7], '関東': [8, 9, 10, 11, 12, 13, 14], '中部': [15, 16, 17, 18, 19, 20, 21, 22, 23], '近畿': [24, 25, 26, 27, 28, 29, 30], '中国': [31, 32, 33, 34, 35], '四国': [36, 37, 38, 39], '九州': [40, 41, 42, 43, 44, 45, 46, 47]}\r"},{"id":36,"href":"/misc/griddata/00-griddata/","title":"不規則データの補完","section":"その他","content":" データの補完 # 不規則な地点で観測されたデータのみから全体を補完する時には scipy.interpolate.griddata を使うことができる。\n参考文献：Scattered data interpolation (griddata)\nimport matplotlib.pyplot as plt import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib import seaborn as sns from scipy.interpolate import griddata def func(x, y): return (x - 0.5) ** 2 + (y - 0.5) ** 2 + np.sin((x - 0.5) ** 2) grid_x, grid_y = np.meshgrid( np.linspace(0, 1, 100), np.linspace(0, 1, 100), indexing=\u0026#34;ij\u0026#34; ) データ数による補完精度の違い # データ数を増やすとより正確に補完できるようすが確認できる。 近傍法は仕組み上どうしてもタイルを並べたような補完になってしまう。\nrng = np.random.default_rng() for points_num in [10, 50, 100, 1000]: points = rng.random((points_num, 2)) values = func(points[:, 0], points[:, 1]) grid_z0 = griddata(points, values, (grid_x, grid_y), method=\u0026#34;nearest\u0026#34;) grid_z1 = griddata(points, values, (grid_x, grid_y), method=\u0026#34;linear\u0026#34;) grid_z2 = griddata(points, values, (grid_x, grid_y), method=\u0026#34;cubic\u0026#34;) plt.suptitle(f\u0026#34;観測できたデータ数={points_num}\u0026#34;, fontsize=22) plt.subplot(221) sns.heatmap(func(grid_x, grid_y).T, cmap=\u0026#34;BrBG\u0026#34;) plt.plot(points[:, 0], points[:, 1], \u0026#34;k.\u0026#34;, ms=1) # data plt.title(\u0026#34;Original\u0026#34;) plt.subplot(222) sns.heatmap(grid_z0.T, cmap=\u0026#34;BrBG\u0026#34;) plt.title(\u0026#34;Nearest\u0026#34;) plt.subplot(223) sns.heatmap(grid_z1.T, cmap=\u0026#34;BrBG\u0026#34;) plt.title(\u0026#34;Linear\u0026#34;) plt.subplot(224) sns.heatmap(grid_z2.T, cmap=\u0026#34;BrBG\u0026#34;) plt.title(\u0026#34;Cubic\u0026#34;) plt.gcf().set_size_inches(6, 6) plt.tight_layout() plt.show() "},{"id":37,"href":"/finance/visualize/002-visuazlize-by-circle/","title":"＋-の変化の表現","section":"可視化","content":"import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib.colors import LinearSegmentedColormap cmap = LinearSegmentedColormap.from_list(\u0026#34;rg\u0026#34;, [\u0026#34;r\u0026#34;, \u0026#34;w\u0026#34;, \u0026#34;g\u0026#34;], N=256) data = { \u0026#34;$AAAA\u0026#34;: { \u0026#34;EPS\u0026#34;: [0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3, 0.4], \u0026#34;Revenue\u0026#34;: [-0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3, 0.4], }, \u0026#34;$BBBB\u0026#34;: { \u0026#34;EPS\u0026#34;: [0.3, 0.1, -0.3, 0.1, 0.1, -0.2, 0.3, 0.4], \u0026#34;Revenue\u0026#34;: [0.1, -0.2, 0.3, 0.4, 0.3, 0.1, -0.3, 0.1], }, \u0026#34;$CCCC\u0026#34;: { \u0026#34;EPS\u0026#34;: [0.1, 0.4, 0.5, 0.2, 0.1, 0.4, 0.5, 0.2], \u0026#34;Revenue\u0026#34;: [0.1, -0.2, 0.3, 0.4, 0.5, 0.2, 0.1, 0.2], }, } n_companies = 3 n_times = 8 プロット # y_index = 0 y_label = [] fig = plt.figure(figsize=(12, 6)) ax = plt.gca() ax.set_facecolor(\u0026#34;#fefefe\u0026#34;) for company_name, eps_rev in data.items(): d = -0.1 for name, v in eps_rev.items(): x = np.arange(n_times) y = [y_index + d for _ in range(n_times)] plt.scatter(x, y, c=v, s=500, cmap=cmap) for xi, vi in zip(x, v): plt.text(xi + 0.15, y_index + d, f\u0026#34;{vi}%\u0026#34;, fontsize=15) d += 0.2 y_label.append(company_name) y_index += 1 plt.xticks( np.arange(n_times), labels=[f\u0026#34;2022年{m+1}月\u0026#34; for m in np.arange(n_times)], fontsize=14 ) plt.yticks(np.arange(n_companies), labels=y_label, fontsize=20) plt.grid(axis=\u0026#34;x\u0026#34;, color=\u0026#34;#ddd\u0026#34;) "},{"id":38,"href":"/prep/numerical/boxcox/","title":"BoxCox変換","section":"数値データ","content":"\rscipy.stats.boxcox\n以下のような変換を数値に対して適用して、分布の形を正規分布に近づけます。 $ y = \\begin{cases} \\displaystyle \\frac{x^\\lambda - 1}{\\lambda} \u0026amp; \\lambda \\neq 0\\ \\log x \u0026amp; \\lambda = 0\\end{cases} $\n式の形から、この変換を数値データに適用するには、$x$は必ず非負の値を取る必要があります。負の値が含まれる場合には、全てを０より大きくするために定数を足すかYeoJonson変換を使用することが考えられます。\nfrom scipy import stats import matplotlib.pyplot as plt x = stats.loggamma.rvs(1, size=1000) + 10 plt.hist(x) plt.show() import numpy as np from scipy.stats import boxcox plt.hist(boxcox(x)) plt.show() "},{"id":39,"href":"/timeseries/shape/003_ddtw/","title":"DDTW(Derivative-DTW)","section":"形状・類似度","content":"\rKeogh, Eamonn J., and Michael J. Pazzani. \u0026ldquo;Derivative dynamic time warping.\u0026rdquo; Proceedings of the 2001 SIAM international conference on data mining. Society for Industrial and Applied Mathematics, 2001. (pdf) の式を参考に実装しています\nimport numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import japanize_matplotlib from IPython.display import display, HTML from utils import get_finance_data import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # TODO: フォントが見つからない場合のwarning抑制 Derivative DTW # pythonで実行できるライブラリがすぐに見つけられなかったので、実装してみました。\ndef DDTW(Q, C): \u0026#34;\u0026#34;\u0026#34; Args: Q (np.array or list): 一つ目の波形 C (np.array or list): 二つ目の波形 Returns: γ_mat (np.array): DDTWを計算するための行列 arrows (np.array): 各時点で←・↙︎・↓のどのマスが最小だったかを示す記号を保存する行列 ddtw (float): DDTW \u0026#34;\u0026#34;\u0026#34; Q, C = np.array(Q), np.array(C) assert Q.shape[0] \u0026gt; 3, \u0026#34;一つ目の波形のフォーマットがおかしいです。\u0026#34; assert C.shape[0] \u0026gt; 3, \u0026#34;二つ目の波形のフォーマットがおかしいです。\u0026#34; # 3.1 Algorithm details の式 def _Dq(q): return ((q[1] - q[0]) + (q[2] - q[0]) / 2) / 2 # 二つの時点間の距離 def _γ(x, y): return abs(_Dq(x) - _Dq(y)) # 各変数 n, m = Q.shape[0] - 2, C.shape[0] - 2 γ_mat = np.zeros((n, m)) arrows = np.array(np.zeros((n, m)), dtype=str) # 可視化用の行列でDDTWの値とは無関係 # 一番左下のスタート地点 γ_mat[0, 0] = _γ(Q[0:3], C[0:3]) # 一列目を計算 for i in range(1, n): γ_mat[i, 0] = γ_mat[i - 1, 0] + _γ(Q[i - 1 : i + 2], C[0:3]) arrows[i, 0] = \u0026#34;↓\u0026#34; # 一行目を計算 for j in range(1, m): γ_mat[0, j] = γ_mat[0, j - 1] + _γ(Q[0:3], C[j - 1 : j + 2]) arrows[0, j] = \u0026#34;←\u0026#34; # 残りのマスを計算 for i in range(1, n): for j in range(1, m): # DDTWを求めるためのマトリクスを埋める d_ij = _γ(Q[i - 1 : i + 2], C[j - 1 : j + 2]) γ_mat[i, j] = d_ij + np.min( [γ_mat[i - 1, j - 1], γ_mat[i - 1, j], γ_mat[i, j - 1]] ) # 矢印を書くための行列(DDTWの値とは関係無い処理) if ( square_index := np.argmin( [γ_mat[i - 1, j - 1], γ_mat[i - 1, j], γ_mat[i, j - 1]] ) ) == 0: arrows[i, j] = \u0026#34;↙︎\u0026#34; elif square_index == 1: arrows[i, j] = \u0026#34;↓\u0026#34; elif square_index == 2: arrows[i, j] = \u0026#34;←\u0026#34; return γ_mat, arrows, γ_mat[n - 1, m - 1] サンプルデータを確認する # 実験に使用するデータを確認します。波形の長さはそれぞれ異なり、平均値もバラバラです。\n$w_1$と$w_2$は形が近いが、平均値が乖離している $w_2$と$w_4$は平均値が近いが、形が違う # 銘柄名、期間、保存先ファイル ticker_symbol = \u0026#34;ZIM\u0026#34; start = \u0026#34;2021-01-01\u0026#34; end = \u0026#34;2022-01-01\u0026#34; # データを取得する df1 = get_finance_data(ticker_symbol, start=start, end=end, savedir=\u0026#34;data\u0026#34;) df1[\u0026#34;Close\u0026#34;] = df1[\u0026#34;Close\u0026#34;].rolling(window=3).mean().fillna(0) w1, w2 = df1[\u0026#34;Close\u0026#34;][53:80].values, df1[\u0026#34;Close\u0026#34;][60:79].values + 20 w3, w4 = df1[\u0026#34;Close\u0026#34;][100:120].values, df1[\u0026#34;Close\u0026#34;][145:167].values - 5 plt.plot(w1, label=\u0026#34;w1\u0026#34;) plt.plot(w2, label=\u0026#34;w2\u0026#34;) plt.plot(w3, label=\u0026#34;w3\u0026#34;) plt.plot(w4, label=\u0026#34;w4\u0026#34;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x7f8cb27e7dc0\u0026gt;\r同じ波形を比較 # 全く同じ波形を比較すれば、DDTWは０になるはずで、対角線上で常に「↙︎(=左下のマスが一番最小の値)」であるはずなのでそれを確認します。 行列を可視化するためにヒートマップを使用しています。\nseaborn.heatmap\nγ_mat, arrows, ddtw = DDTW(w2, w2) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (20, 10)}) sns.set(font=\u0026#34;IPAexGothic\u0026#34;) ax = sns.heatmap(γ_mat, annot=True, fmt=\u0026#34;.2f\u0026#34;, cmap=\u0026#34;YlGnBu\u0026#34;) ax.set_title(f\u0026#34;DDTW = {ddtw}\u0026#34;) ax.invert_yaxis() ax.set_xlabel(\u0026#34;w2\u0026#34;) ax.set_ylabel(\u0026#34;w2\u0026#34;) plt.show() ax = sns.heatmap(γ_mat, annot=arrows, fmt=\u0026#34;\u0026#34;, cmap=\u0026#34;YlGnBu\u0026#34;) ax.invert_yaxis() plt.show() w2とw4のどちらがw1に近い形かDDTWで調べる # w1 と w2は形が近いが、平均値が乖離している w2 と w4 は平均値が近いが、形が違う ことが上のプロットでわかっているので、$DDTW(w_1, w_2) \u0026lt; DDTW(w_2, w_4)$ であってほしいです。\nγ_mat, arrows, ddtw = DDTW(w2, w4) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (20, 10)}) sns.set(font=\u0026#34;IPAexGothic\u0026#34;) ax = sns.heatmap(γ_mat, annot=True, fmt=\u0026#34;.2f\u0026#34;, cmap=\u0026#34;YlGnBu\u0026#34;) ax.set_title(f\u0026#34;DDTW(w2, w4) = {ddtw}\u0026#34;) ax.invert_yaxis() ax.set_xlabel(\u0026#34;w2\u0026#34;) ax.set_ylabel(\u0026#34;w4\u0026#34;) plt.show() γ_mat, arrows, ddtw = DDTW(w1, w2) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (20, 10)}) sns.set(font=\u0026#34;IPAexGothic\u0026#34;) ax = sns.heatmap(γ_mat, annot=True, fmt=\u0026#34;.2f\u0026#34;, cmap=\u0026#34;YlGnBu\u0026#34;) ax.set_title(f\u0026#34;DDTW(w1, w2) = {ddtw}\u0026#34;) ax.invert_yaxis() ax.set_xlabel(\u0026#34;w1\u0026#34;) ax.set_ylabel(\u0026#34;w2\u0026#34;) plt.show() "},{"id":40,"href":"/timeseries/shape/002_dtwvsddtw.jp/","title":"DTWとDDTW","section":"形状・類似度","content":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns from dtaidistance import dtw from dtaidistance import dtw_visualisation as dtwvis from utils import DDTW np.random.seed(777) 実験に使用する二つの波形をプロット # data1 = np.array([12.0 * np.sin(i / 2.1) + 20 for i in range(30)]) data2 = np.array([10.0 * np.sin(i / 2.0) + np.random.rand() for i in range(30)]) plt.figure(figsize=(12, 4)) # 波形をプロット plt.plot(data1, label=\u0026#34;data1\u0026#34;, color=\u0026#34;k\u0026#34;) plt.plot(data2, label=\u0026#34;data2\u0026#34;, color=\u0026#34;r\u0026#34;) plt.legend() plt.show() DTW # d, paths = dtw.warping_paths( data1, data2, window=25, ) best_path = dtw.best_path(paths) dtwvis.plot_warpingpaths(data1, data2, paths, best_path) (\u0026lt;Figure size 720x720 with 4 Axes\u0026gt;,\r[\u0026lt;AxesSubplot:\u0026gt;, \u0026lt;AxesSubplot:\u0026gt;, \u0026lt;AxesSubplot:\u0026gt;, \u0026lt;AxesSubplot:\u0026gt;])\rDDTW # γ_mat, arrows, ddtw = DDTW(np.array(data1), np.array(data2)) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (18, 15)}) sns.set(font=\u0026#34;IPAexGothic\u0026#34;) ax = sns.heatmap(-1 * γ_mat, cmap=\u0026#34;YlGnBu\u0026#34;) ax.set_title(f\u0026#34;DDTW = {ddtw}\u0026#34;) ax.invert_xaxis() ax.invert_yaxis() ax.set_xlabel(\u0026#34;w2\u0026#34;) ax.set_ylabel(\u0026#34;w2\u0026#34;) plt.show() findfont: Font family ['IPAexGothic'] not found. Falling back to DejaVu Sans.\rfindfont: Font family ['IPAexGothic'] not found. Falling back to DejaVu Sans.\r"},{"id":41,"href":"/prep/special/sec_edgar/","title":"EDGARのデータを取得","section":"特殊なデータ","content":"\rEDGAR(Electronic Data Gathering, Analysis, and Retrieval system)とは、米国の証券取引委員会の運営するサイトです。 米国の法による法定開示書類が管理されています。ここでは米国企業の財務諸表も管理されています。\n今回は指定した企業の財務諸表を取得して、データをプロットしてみようと思います。\nimport os from sec_edgar_downloader import Downloader dl = Downloader(\u0026#34;./data/\u0026#34;) ticker_symbol = \u0026#34;MSFT\u0026#34; if os.path.exists(f\u0026#34;./data/sec-edgar-filings/{ticker_symbol}/10-K/\u0026#34;): print(\u0026#34;ダウンロード済みです。\u0026#34;) elif dl.get(\u0026#34;10-K\u0026#34;, ticker_symbol, after=\u0026#34;2021-01-01\u0026#34;, before=\u0026#34;2021-12-31\u0026#34;) \u0026gt; 0: print(\u0026#34;ダウンロードに成功しました。\u0026#34;) else: print(\u0026#34;ダウンロードに失敗しました。\u0026#34;) ダウンロード済みです。\r10-Kに含まれる表を抜き出す # pandasのread_htmlを用いることでテーブルをDataFrameの形で抜き出すことができます。\npandas.read_html\nimport glob import pandas as pd filing_details_filepath = glob.glob( f\u0026#34;./data/sec-edgar-filings/{ticker_symbol}/10-K/*/filing-details.html\u0026#34; )[0] tables = pd.read_html(filing_details_filepath) CASH FLOWS STATEMENTSのテーブルを抽出 # 「CASH FLOWS STATEMENTS」のページのテーブルを抽出します。 様々な方法が考えられますが、ここでは「Cash and cash equivalents, end of period」というワードを見つけたらそのテーブルを抜き出すように指定しています。\ncs_table = None # CASH FLOWS STATEMENTSのページのテーブル for table in tables: tab_html = table.to_html() if \u0026#34;Cash and cash equivalents, end of period\u0026#34; in tab_html: cs_table = table cs_table.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rデータの前処理 # NaNが多く含まれる上、文字列扱いになっていてこのままでは数値を読み取れません。 NaNや不要な記号を取り除き、文字列を数値に変換します。\nimport numpy as np # 「(」「)」「,」の文字列を削除する cs_table = cs_table.replace([\u0026#34;,\u0026#34;, \u0026#34;\\)\u0026#34;, \u0026#34;\\(\u0026#34;], \u0026#34;\u0026#34;, regex=True) cs_table = cs_table.replace([\u0026#34;\u0026#34;], np.nan) # 空白セルはnan扱いにする cs_table.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r# データがない列を削除する row_num = cs_table.shape[0] for colname in cs_table.columns: if cs_table[colname].isna().sum() \u0026gt; row_num * 0.8: # 8割以上の行がNaNの列は cs_table.drop(colname, inplace=True, axis=1) # 不要な列として削除する cs_table.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r# NaNの多い行も削除する col_num = cs_table.shape[1] cs_table[cs_table.isna().sum(axis=1) \u0026lt; col_num * 0.8] cs_table.columns = [\u0026#34;item\u0026#34;, \u0026#34;2021\u0026#34;, \u0026#34;2020\u0026#34;, \u0026#34;2019\u0026#34;] cs_table = cs_table[[\u0026#34;item\u0026#34;, \u0026#34;2019\u0026#34;, \u0026#34;2020\u0026#34;, \u0026#34;2021\u0026#34;]] # years = cs_table.fillna(\u0026#39;\u0026#39;).query(\u0026#34;item.str.contains(\u0026#39;Year Ended\u0026#39;)\u0026#34;).iloc[0, 1:] データをプロットする # import matplotlib.pyplot as plt years = [\u0026#34;2019\u0026#34;, \u0026#34;2020\u0026#34;, \u0026#34;2021\u0026#34;] for item_name in cs_table[\u0026#34;item\u0026#34;][:10]: try: data = [float(v) for v in cs_table.query(f\u0026#34;item==\u0026#39;{item_name}\u0026#39;\u0026#34;).iloc[0, 1:]] plt.plot(data) plt.xticks([i for i in range(len(years))], years) plt.title(item_name) plt.grid() plt.show() except IndexError: pass "},{"id":42,"href":"/webapp/gradio/","title":"Gradio","section":"アプリケーション作成","content":" Gradio # アプリの作り方 # 後ほど追記する予定です。\n"},{"id":43,"href":"/timeseries/exponential_smoothing/002-hw-es/","title":"Holtの線形指数平滑法","section":"指数平滑化","content":"import japanize_matplotlib as jm import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from statsmodels.tsa.holtwinters import Holt from statsmodels.tsa.holtwinters import SimpleExpSmoothing jm.japanize() サンプルデータ # Natural Gas Consumption (NATURALGAS)のデータを使用しています。数値の単位はBCF（10億立方フィート）です。\ndata = { \u0026#34;value\u0026#34;: { \u0026#34;2013-01-01\u0026#34;: 2878.8, \u0026#34;2013-02-01\u0026#34;: 2567.2, \u0026#34;2013-03-01\u0026#34;: 2521.1, \u0026#34;2013-04-01\u0026#34;: 1967.5, \u0026#34;2013-05-01\u0026#34;: 1752.5, \u0026#34;2013-06-01\u0026#34;: 1742.9, \u0026#34;2013-07-01\u0026#34;: 1926.3, \u0026#34;2013-08-01\u0026#34;: 1927.4, \u0026#34;2013-09-01\u0026#34;: 1767.0, \u0026#34;2013-10-01\u0026#34;: 1866.8, \u0026#34;2013-11-01\u0026#34;: 2316.9, \u0026#34;2013-12-01\u0026#34;: 2920.8, \u0026#34;2014-01-01\u0026#34;: 3204.1, \u0026#34;2014-02-01\u0026#34;: 2741.2, \u0026#34;2014-03-01\u0026#34;: 2557.9, \u0026#34;2014-04-01\u0026#34;: 1961.7, \u0026#34;2014-05-01\u0026#34;: 1810.2, \u0026#34;2014-06-01\u0026#34;: 1745.4, \u0026#34;2014-07-01\u0026#34;: 1881.0, \u0026#34;2014-08-01\u0026#34;: 1933.1, \u0026#34;2014-09-01\u0026#34;: 1809.3, \u0026#34;2014-10-01\u0026#34;: 1912.8, \u0026#34;2014-11-01\u0026#34;: 2357.5, \u0026#34;2014-12-01\u0026#34;: 2679.2, \u0026#34;2015-01-01\u0026#34;: 3115.0, \u0026#34;2015-02-01\u0026#34;: 2925.2, \u0026#34;2015-03-01\u0026#34;: 2591.3, \u0026#34;2015-04-01\u0026#34;: 2007.9, \u0026#34;2015-05-01\u0026#34;: 1858.2, \u0026#34;2015-06-01\u0026#34;: 1899.9, \u0026#34;2015-07-01\u0026#34;: 2067.7, \u0026#34;2015-08-01\u0026#34;: 2052.7, \u0026#34;2015-09-01\u0026#34;: 1901.3, \u0026#34;2015-10-01\u0026#34;: 1987.3, \u0026#34;2015-11-01\u0026#34;: 2249.1, \u0026#34;2015-12-01\u0026#34;: 2588.2, \u0026#34;2016-01-01\u0026#34;: 3091.7, \u0026#34;2016-02-01\u0026#34;: 2652.3, \u0026#34;2016-03-01\u0026#34;: 2356.3, \u0026#34;2016-04-01\u0026#34;: 2083.9, \u0026#34;2016-05-01\u0026#34;: 1965.8, \u0026#34;2016-06-01\u0026#34;: 2000.7, \u0026#34;2016-07-01\u0026#34;: 2186.6, \u0026#34;2016-08-01\u0026#34;: 2208.4, \u0026#34;2016-09-01\u0026#34;: 1947.8, \u0026#34;2016-10-01\u0026#34;: 1925.2, \u0026#34;2016-11-01\u0026#34;: 2159.5, \u0026#34;2016-12-01\u0026#34;: 2866.3, \u0026#34;2017-01-01\u0026#34;: 2913.8, \u0026#34;2017-02-01\u0026#34;: 2340.2, \u0026#34;2017-03-01\u0026#34;: 2523.3, \u0026#34;2017-04-01\u0026#34;: 1932.0, \u0026#34;2017-05-01\u0026#34;: 1892.0, \u0026#34;2017-06-01\u0026#34;: 1910.4, \u0026#34;2017-07-01\u0026#34;: 2141.6, \u0026#34;2017-08-01\u0026#34;: 2093.8, \u0026#34;2017-09-01\u0026#34;: 1920.5, \u0026#34;2017-10-01\u0026#34;: 2031.5, \u0026#34;2017-11-01\u0026#34;: 2357.3, \u0026#34;2017-12-01\u0026#34;: 3086.0, \u0026#34;2018-01-01\u0026#34;: 3340.9, \u0026#34;2018-02-01\u0026#34;: 2710.7, \u0026#34;2018-03-01\u0026#34;: 2796.7, \u0026#34;2018-04-01\u0026#34;: 2350.5, \u0026#34;2018-05-01\u0026#34;: 2055.0, \u0026#34;2018-06-01\u0026#34;: 2063.1, \u0026#34;2018-07-01\u0026#34;: 2350.7, \u0026#34;2018-08-01\u0026#34;: 2313.8, \u0026#34;2018-09-01\u0026#34;: 2156.1, \u0026#34;2018-10-01\u0026#34;: 2285.9, \u0026#34;2018-11-01\u0026#34;: 2715.9, \u0026#34;2018-12-01\u0026#34;: 2999.5, \u0026#34;2019-01-01\u0026#34;: 3424.3, \u0026#34;2019-02-01\u0026#34;: 3019.1, \u0026#34;2019-03-01\u0026#34;: 2927.8, \u0026#34;2019-04-01\u0026#34;: 2212.4, \u0026#34;2019-05-01\u0026#34;: 2134.0, \u0026#34;2019-06-01\u0026#34;: 2119.3, \u0026#34;2019-07-01\u0026#34;: 2393.9, \u0026#34;2019-08-01\u0026#34;: 2433.9, \u0026#34;2019-09-01\u0026#34;: 2206.3, \u0026#34;2019-10-01\u0026#34;: 2306.5, \u0026#34;2019-11-01\u0026#34;: 2783.8, \u0026#34;2019-12-01\u0026#34;: 3170.7, \u0026#34;2020-01-01\u0026#34;: 3320.6, \u0026#34;2020-02-01\u0026#34;: 3058.5, \u0026#34;2020-03-01\u0026#34;: 2722.0, \u0026#34;2020-04-01\u0026#34;: 2256.9, \u0026#34;2020-05-01\u0026#34;: 2072.2, \u0026#34;2020-06-01\u0026#34;: 2127.9, \u0026#34;2020-07-01\u0026#34;: 2464.1, \u0026#34;2020-08-01\u0026#34;: 2399.5, \u0026#34;2020-09-01\u0026#34;: 2151.2, \u0026#34;2020-10-01\u0026#34;: 2315.9, \u0026#34;2020-11-01\u0026#34;: 2442.0, \u0026#34;2020-12-01\u0026#34;: 3182.8, \u0026#34;2021-01-01\u0026#34;: 3343.9, \u0026#34;2021-02-01\u0026#34;: 3099.2, \u0026#34;2021-03-01\u0026#34;: 2649.4, \u0026#34;2021-04-01\u0026#34;: 2265.1, \u0026#34;2021-05-01\u0026#34;: 2117.4, \u0026#34;2021-06-01\u0026#34;: 2238.4, \u0026#34;2021-07-01\u0026#34;: 2412.2, \u0026#34;2021-08-01\u0026#34;: 2433.8, \u0026#34;2021-09-01\u0026#34;: 2142.3, \u0026#34;2021-10-01\u0026#34;: 2262.6, \u0026#34;2021-11-01\u0026#34;: 2693.3, \u0026#34;2021-12-01\u0026#34;: 3007.3, \u0026#34;2022-01-01\u0026#34;: 3612.1, \u0026#34;2022-02-01\u0026#34;: 3064.2, \u0026#34;2022-03-01\u0026#34;: 2785.4, \u0026#34;2022-04-01\u0026#34;: 2379.3, \u0026#34;2022-05-01\u0026#34;: 2247.8, \u0026#34;2022-06-01\u0026#34;: 2326.9, \u0026#34;2022-07-01\u0026#34;: 2597.9, \u0026#34;2022-08-01\u0026#34;: 2566.1, \u0026#34;2022-09-01\u0026#34;: 2263.3, } } data = pd.DataFrame(data) data.rename(columns={\u0026#34;value\u0026#34;: \u0026#34;Natural Gas Consumption(BCF)\u0026#34;}, inplace=True) data.index = pd.to_datetime(data.index) data = data.asfreq(\u0026#34;MS\u0026#34;) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data) plt.grid() plt.show() データの分割 # データの末尾（2020年以降のデータ）を予測性能の検証に使用するためにデータを分割します。\ndata_train = data[data.index \u0026lt; \u0026#34;2020-1-1\u0026#34;] data_test = data[data.index \u0026gt;= \u0026#34;2020-1-1\u0026#34;] plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.legend() plt.grid() モデルの学習と予測 # ses = SimpleExpSmoothing(data_train) ses = ses.fit(smoothing_level=0.1) ses_pred = ses.forecast(33) holt = Holt(data_train) holt = holt.fit(smoothing_level=0.1, smoothing_trend=0.2) holt_pred = holt.forecast(33) plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.plot(ses_pred.index, ses_pred.values, \u0026#34;-x\u0026#34;, label=\u0026#34;prediction(ses)\u0026#34;) plt.plot(holt_pred.index, holt_pred.values, \u0026#34;-x\u0026#34;, label=\u0026#34;prediction(Holt)\u0026#34;) plt.legend() plt.grid() "},{"id":44,"href":"/timeseries/unsupervised/isolation-forest/","title":"Isolation Forest","section":"教師なしの手法","content":"\rIsolation Forest # isolation Forestを使って異常値を検出してみます。 このページでは時系列データの中にいくつか異常値を混ぜ、それが検出できるかどうか検証します。 また、最後に異常値と判定したルールを可視化します。\nimport japanize_matplotlib import numpy as np import pandas as pd import seaborn as sns from matplotlib import pyplot as plt from sklearn.ensemble import IsolationForest from sklearn.tree import plot_tree 人工データの作成 # [11, 49, 149, 240, 300, 310]の日付にて異常値を混ぜておきます。 カテゴリ変数や整数の特徴も含まれるデータです。数値のみプロットしてみます。\nrs = np.random.RandomState(365) dates = pd.date_range(\u0026#34;1 1 2016\u0026#34;, periods=365, freq=\u0026#34;D\u0026#34;) data = {} data[\u0026#34;月\u0026#34;] = [d.strftime(\u0026#34;%m\u0026#34;) for d in dates] data[\u0026#34;曜日\u0026#34;] = [d.strftime(\u0026#34;%A\u0026#34;) for d in dates] data[\u0026#34;特徴1\u0026#34;] = [np.sin(d.day / 50) + np.random.rand() for d in dates] data[\u0026#34;特徴2\u0026#34;] = [np.cos(d.day / 50) + np.random.rand() for d in dates] data[\u0026#34;特徴3\u0026#34;] = [3 * np.random.rand() + np.log(d.dayofyear) * 0.03 for d in dates] data[\u0026#34;特徴4\u0026#34;] = [np.random.choice([\u0026#34;☀\u0026#34;, \u0026#34;☂\u0026#34;, \u0026#34;☁\u0026#34;]) for d in dates] column_names = list(data.keys()) anomaly_index = [11, 49, 149, 240, 300, 310] anomaly_dates = [dates[i] for i in anomaly_index] for i in anomaly_index: data[\u0026#34;特徴1\u0026#34;][i] = 2.5 data[\u0026#34;特徴2\u0026#34;][i] = 0.3 data = pd.DataFrame(data, index=dates) plt.figure(figsize=(10, 4)) sns.lineplot(data=data, palette=\u0026#34;tab10\u0026#34;, linewidth=2.5) \u0026lt;Axes: \u0026gt;\rdata .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rカテゴリ変数の変換 # 「曜日」のような特徴をIsolation Forestで扱うためにダミー変数に変換します。\nX = pd.get_dummies(data) X .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rIsolation Forestの作成 # ilf = IsolationForest( n_estimators=100, max_samples=\u0026#34;auto\u0026#34;, contamination=0.01, max_features=5, bootstrap=False, random_state=np.random.RandomState(365), ) ilf.fit(X) data[\u0026#34;is_anomaly\u0026#34;] = ilf.predict(X) \u0026lt; 0 data[\u0026#34;anomaly_score\u0026#34;] = ilf.decision_function(X) 検出した日付と正解の比較 # 異常値として検出したタイミングと正解を比較します。\nplt.figure(figsize=(10, 4)) plt.title(\u0026#34;検出箇所\u0026#34;) sns.lineplot(data=data[column_names], palette=\u0026#34;tab10\u0026#34;, linewidth=2.5) for d in data[data[\u0026#34;is_anomaly\u0026#34;]].index: plt.axvline(x=d, color=\u0026#34;red\u0026#34;, linewidth=4) plt.figure(figsize=(10, 4)) plt.title(\u0026#34;正解\u0026#34;) sns.lineplot(data=data[column_names], palette=\u0026#34;tab10\u0026#34;, linewidth=2.5) for d in anomaly_dates: plt.axvline(x=d, color=\u0026#34;black\u0026#34;, linewidth=4) 異常値のルール # サンプル数（samples）が１の分岐が一番右にあり、それは特徴１による分岐だと分かります。 実際、今回の異常値は特徴1が大きすぎる値のときに異常値になりやすいです。\nplt.figure(figsize=(24, 8)) plot_tree( ilf.estimators_[0], feature_names=column_names, filled=True, fontsize=13, max_depth=3, precision=2, rounded=True, ) plt.show() "},{"id":45,"href":"/basic/clustering/k-means2/","title":"k-means++","section":"クラスタリング","content":"\rimport numpy as np import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.datasets import make_blobs k-means++とランダムな初期値選択の比較 # randomとkmeans++との間のクラスタリングの結果を比較します。 k-means++は左上のクラスタを2つに分けてしまう場合があることを確認できます。 ※この例ではあえて max_iter=1 を指定しています。\nn_samples = 3000 random_state = 11711 X, y = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=1.5, centers=8 ) plt.figure(figsize=(8, 8)) for i in range(10): # random/k-means++を比較, クラスタリングに使用するデータは毎回ランダムに決める rand_index = np.random.permutation(1000) X_rand = X[rand_index] y_pred_rnd = KMeans( n_clusters=5, random_state=random_state, init=\u0026#34;random\u0026#34;, max_iter=1, n_init=1 ).fit_predict(X_rand) y_pred_kpp = KMeans( n_clusters=5, random_state=random_state, init=\u0026#34;k-means++\u0026#34;, max_iter=1, n_init=1 ).fit_predict(X_rand) plt.figure(figsize=(10, 2)) plt.subplot(1, 2, 1) plt.title(f\u0026#34;random\u0026#34;) plt.scatter(X_rand[:, 0], X_rand[:, 1], c=y_pred_rnd, marker=\u0026#34;x\u0026#34;) plt.subplot(1, 2, 2) plt.title(f\u0026#34;k-means++\u0026#34;) plt.scatter(X_rand[:, 0], X_rand[:, 1], c=y_pred_kpp, marker=\u0026#34;x\u0026#34;) plt.show() plt.tight_layout() plt.show() "},{"id":46,"href":"/finance/main/001_mplfinance/","title":"mplfinance","section":"時系列データ","content":"\rデータの読み込み # Pandas-datareader | Remote Data Access\nimport os import time import pandas as pd import pandas_datareader.data as web from pandas_datareader._utils import RemoteDataError Yahoo Terms of Service\ndef get_finance_data( ticker_symbol: str, start=\u0026#34;2021-01-01\u0026#34;, end=\u0026#34;2021-06-30\u0026#34;, savedir=\u0026#34;data\u0026#34; ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34;株価を記録したデータを取得します Args: ticker_symbol (str): Description of param1 start (str): 期間はじめの日付, optional. end (str): 期間終わりの日付, optional. Returns: res: 株価データ \u0026#34;\u0026#34;\u0026#34; res = None filepath = os.path.join(savedir, f\u0026#34;{ticker_symbol}_{start}_{end}_historical.csv\u0026#34;) os.makedirs(savedir, exist_ok=True) if not os.path.exists(filepath): try: time.sleep(5.0) res = web.DataReader(ticker_symbol, \u0026#34;yahoo\u0026#34;, start=start, end=end) res.to_csv(filepath, encoding=\u0026#34;utf-8-sig\u0026#34;) except (RemoteDataError, KeyError): print(f\u0026#34;ticker_symbol ${ticker_symbol} が正しいか確認してください。\u0026#34;) else: res = pd.read_csv(filepath, index_col=\u0026#34;Date\u0026#34;) res.index = pd.to_datetime(res.index) assert res is not None, \u0026#34;データ取得に失敗しました\u0026#34; return res # 銘柄名、期間、保存先ファイル ticker_symbol = \u0026#34;NVDA\u0026#34; start = \u0026#34;2021-01-01\u0026#34; end = \u0026#34;2021-06-30\u0026#34; # データを取得する df = get_finance_data(ticker_symbol, start=start, end=end, savedir=\u0026#34;../data\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rOHLCをプロット # OHLCとは始値・高値・安値・終値のことで、普段最もよくみるグラフの一つです。 ローソク足と、線でのプロットをしてみます。\nmplfinance | matplotlib utilities for the visualization, and visual analysis, of financial data\nimport mplfinance as mpf mpf.plot(df, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4)) mpf.plot(df, type=\u0026#34;line\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4)) 移動平均線 # 株価や外国為替のテクニカル分析において使用される指標の一つに、移動平均線というものがあります。\n現在のテクニカル分析では、短期、中期、長期の3本の移動平均線を同時に表示させる例が多く、日足チャートでは、5日移動平均線、25日移動平均線、75日移動平均線がよく使われる。（引用元：Wikipedia 移動平均線）\n5/25/75日移動平均線をプロットするために mav オプションを指定します。\nmpf.plot(df, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4), mav=[5, 25, 75]) 凡例の表示 # どの線が何日移動平均線かわかりにくいので、凡例を追加します。 matplotlib.pyplot.legend\nimport japanize_matplotlib import matplotlib.patches as mpatches fig, axes = mpf.plot( df, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4), mav=[5, 25, 75], returnfig=True, ) fig.legend( [f\u0026#34;{days} days\u0026#34; for days in [5, 25, 75]], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102) ) \u0026lt;matplotlib.legend.Legend at 0x7f8be96e5b80\u0026gt;\rボリュームの表示 # fig, axes = mpf.plot( df, title=\u0026#34;NVDA 2021/1/B~2021/6/E\u0026#34;, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4), mav=[5, 25, 75], volume=True, datetime_format=\u0026#34;%Y/%m/%d\u0026#34;, returnfig=True, ) fig.legend( [f\u0026#34;{days} days\u0026#34; for days in [5, 25, 75]], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102) ) \u0026lt;matplotlib.legend.Legend at 0x7f8beac867f0\u0026gt;\rfig, axes = mpf.plot( df, title=\u0026#34;NVDA 2021/1/B~2021/6/E\u0026#34;, type=\u0026#34;candle\u0026#34;, style=\u0026#34;yahoo\u0026#34;, figsize=(12, 4), mav=[5, 25, 75], volume=True, datetime_format=\u0026#34;%Y/%m/%d\u0026#34;, returnfig=True, ) fig.legend( [f\u0026#34;{days} days\u0026#34; for days in [5, 25, 75]], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102) ) "},{"id":47,"href":"/prep/data/pandas-basic/","title":"pandasの使用","section":"データの読み込み・集約","content":" データを読み込む # テーブル全体 ＝ DataFrameと呼ぶ テーブルの各行 ＝ rowと呼ぶ テーブルの各列 ＝ columnと呼び、Seriesというデータに収まっている import pandas as pd df = pd.read_csv(\u0026#34;./z-001.csv\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rデータを整形する # pandas.DataFrame\n列名の変更・重複の除去・型の変更・欠損の置き換えなどを実行する。データ全体の欠損を置き換えたり、データの各列・行ごとに一括で統計量を求めるメソッドが用意されている。\npandas.DataFrame.mad\rpandas.DataFrame.max\rpandas.DataFrame.mean\rpandas.DataFrame.median\rpandas.DataFrame.min\rpandas.DataFrame.mode 欠損や余分なデータへの対応 # pandas.DataFrame.fillna pandas.DataFrame.dropna pandas.DataFrame.applymap df = df.fillna(0) # 欠損を0に置き換え df = df.drop([\u0026#34;Unnamed: 0\u0026#34;], axis=1) # \u0026#39;Unnamed: 0\u0026#39;列を削除 df.columns = df.columns.str.replace(\u0026#34;Unnamed: \u0026#34;, \u0026#34;列\u0026#34;) # 列名の Unnamed を別の文字列に置換 df = df.applymap( lambda x: int(x) if type(x) == float else x ) # 数値データならば全て整数にするような関数を全体に適用する df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rデータを絞り込む # pandas.DataFrame.query\n# 平成21年度 \u0026gt; 15となるような行のみを抽出します df.query(\u0026#34;平成21年度 \u0026gt; 15\u0026#34;) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r# 平成21年度 \u0026gt; 10 \u0026amp; 平成22年度 \u0026lt; 15 となるような行のみを抽出します df.query(\u0026#34;平成21年度 \u0026gt; 10 \u0026amp; 平成22年度 \u0026lt; 15\u0026#34;) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rデータの列ごとの統計量を算出する # pandas.Series — pandas 1.4.1 documentation\n# 数値の列のみに絞り込む df_num = df.select_dtypes(\u0026#34;number\u0026#34;) # 列をリストにする関数 def concat_str(series: pd.Series) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34;列に含まれるデータをリストにする\u0026#34;\u0026#34;\u0026#34; return [s for s in series] # 列ごとの統計量を求める、指定した関数にデータを渡して実行することもできる df_num.agg([\u0026#34;min\u0026#34;, \u0026#34;max\u0026#34;, \u0026#34;median\u0026#34;, \u0026#34;mean\u0026#34;, concat_str]) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r列ごとの統計量を求める # pandas.Series\nドキュメントにメソッドとプロパティの一覧が記載されている、minやmaxなどを始めとして様々な計算を簡単に実行することができる。欠損が含まれているかどうかを示すプロパティも定義されている。\n数値データ # 平成21年の列 = df[\u0026#34;平成21年度\u0026#34;] print(平成21年の列) 0 29\r1 18\r2 11\r3 11\r4 12\r5 9\rName: 平成21年度, dtype: int64\rprint(f\u0026#34; min: {平成21年の列.min()}\u0026#34;) print(f\u0026#34; max: {平成21年の列.max()}\u0026#34;) print(f\u0026#34; median: {平成21年の列.median()}\u0026#34;) print(f\u0026#34; dtypes(データ型): {平成21年の列.dtypes}\u0026#34;) print(f\u0026#34; hasnans(欠損の有無): {平成21年の列.hasnans}\u0026#34;) min: 9\rmax: 29\rmedian: 11.5\rdtypes(データ型): int64\rhasnans(欠損の有無): False\r文字列データ # 文字列の出現回数を求めてみます。出現回数を知るには value_counts を使います。\nもしも文字列に対すしてどのような操作が用意されているか忘れてしまった場合やまったく見当がつかない場合、ドキュメントを調べることで見つけることができます。たとえば文字列の出現回数を知りたい時はドキュメントで「count value」などと検索すれば、\nCategorical data How to calculate summary statistics? pandas.DataFrame.count pandas.Series.count などのページが出てきます。そして、pandas.DataFrame.countなどを見れば 『See also(こちらもご覧ください)』の欄にDataFrame.value_countsへのリンクがあります。value_countsでドキュメントを検索すれば pandas.Series.value_countsが見つかり、これを使えば文字列の出現回数を求められることが分かります。\ndf[\u0026#34;平成21年度\u0026#34;].value_counts() 11 2\r29 1\r18 1\r12 1\r9 1\rName: 平成21年度, dtype: int64\r"},{"id":48,"href":"/timeseries/forecast/002-prophet-alg/","title":"Prophetのモデルの中身","section":"Prophet","content":"\rProphetがどのようにしてモデルを作成しているのか、もう少し詳細に見てみたいと思います。\nTaylor, Sean J., and Benjamin Letham. \u0026ldquo;Forecasting at scale.\u0026rdquo; The American Statistician 72.1 (2018): 37-45.\nimport numpy as np import pandas as pd import random import seaborn as sns import matplotlib.pyplot as plt import japanize_matplotlib from prophet import Prophet 実験に使用するデータ # １年間のデータを用意します。このデータには以下の特徴があります。\n2020/1/1 ~ 2020/12/31 の期間のデータ 週ごとに周期的な数値を取る 土日は大きな値をとる 不定期にイベントが発生し、その時は非常に大きな値を取る(horidays_indexに該当する日付を指しています) date = pd.date_range(\u0026#34;2020-01-01\u0026#34;, periods=365, freq=\u0026#34;D\u0026#34;) horidays_index = random.sample([i for i in range(365)], 10) y = [ np.log(10 + i + 10 * np.cos(i)) # トレンド + np.cos(di.weekday() * np.pi / 28) * 3 # 週ごとに周期性あり + (di.weekday() in {5, 6}) * 0.5 # 〃 + (i in horidays_index) * 2 # 祝日だけ数値を増やす + np.random.rand() / 10 # ノイズ for i, di in enumerate(date) ] df = pd.DataFrame({\u0026#34;ds\u0026#34;: date, \u0026#34;y\u0026#34;: y}) df.index = date plt.title(\u0026#34;サンプルデータ\u0026#34;) sns.lineplot(data=df) plt.show() トレンドの指定 # growth=\u0026ldquo;linear\u0026rdquo; # # モデルを作成 m = Prophet( yearly_seasonality=False, weekly_seasonality=True, daily_seasonality=False, growth=\u0026#34;linear\u0026#34;, ) m.fit(df) # 将来を予測 future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig = m.plot(forecast) plt.axvspan(18627, 18627 + 90, color=\u0026#34;coral\u0026#34;, alpha=0.4, label=\u0026#34;予測期間\u0026#34;) plt.legend() plt.show() Initial log joint probability = -2.99674\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 99 1040.17 0.000845008 95.4398 0.3546 0.3546 128 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 166 1045.08 8.28451e-05 66.0092 1.087e-06 0.001 252 LS failed, Hessian reset 184 1045.09 1.58284e-05 61.1258 3.535e-07 0.001 307 LS failed, Hessian reset 199 1045.09 1.20349e-07 45.161 0.0243 1 330 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 264 1045.71 0.000316932 140.641 3.665e-06 0.001 456 LS failed, Hessian reset 292 1045.82 5.70302e-05 48.9256 1.155e-06 0.001 533 LS failed, Hessian reset 299 1045.82 7.71897e-06 40.8043 0.3543 1 544 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 305 1045.82 5.91182e-07 46.3158 2.336e-08 0.001 594 LS failed, Hessian reset 317 1045.82 2.63213e-06 51.4268 5.486e-08 0.001 649 LS failed, Hessian reset 343 1045.82 1.59952e-05 25.621 2.535e-07 0.001 728 LS failed, Hessian reset 353 1045.82 6.70942e-07 31.5639 1.825e-08 0.001 779 LS failed, Hessian reset 355 1045.82 3.11455e-08 40.1124 0.08424 1 783 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\rgrowth=\u0026ldquo;flat\u0026rdquo; # # モデルを作成 m = Prophet( yearly_seasonality=False, weekly_seasonality=True, daily_seasonality=False, growth=\u0026#34;flat\u0026#34;, ) m.fit(df) # 将来を予測 future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig = m.plot(forecast) plt.axvspan(18627, 18627 + 90, color=\u0026#34;coral\u0026#34;, alpha=0.4, label=\u0026#34;予測期間\u0026#34;) plt.legend() plt.show() Initial log joint probability = -3.30635\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 8 725.322 0.00030891 0.0789655 0.955 0.955 14 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\r季節変化の指定 # seasonalityが全てFalse # # モデルを作成 m = Prophet( yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, growth=\u0026#34;linear\u0026#34;, ) m.fit(df) # 将来を予測 future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig = m.plot(forecast) plt.axvspan(18627, 18627 + 90, color=\u0026#34;coral\u0026#34;, alpha=0.4, label=\u0026#34;予測期間\u0026#34;) plt.legend() plt.show() Initial log joint probability = -2.99674\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 99 1014.78 0.000250992 97.5427 1.468 0.01468 134 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 199 1020 0.00045602 60.8104 1 1 268 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 208 1020.1 0.000167489 76.8606 1.772e-06 0.001 314 LS failed, Hessian reset 299 1020.33 0.000353008 41.1849 2.43 0.243 446 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 352 1020.86 8.23232e-05 43.9782 1.438e-06 0.001 586 LS failed, Hessian reset 399 1020.99 0.000244078 56.987 1 1 657 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 488 1021.04 3.90795e-05 57.3821 6.974e-07 0.001 856 LS failed, Hessian reset 499 1021.04 1.23592e-05 41.4144 1 1 876 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 510 1021.04 4.89524e-06 59.8544 1.187e-07 0.001 929 LS failed, Hessian reset 571 1021.06 1.29427e-06 39.655 2.478e-08 0.001 1071 LS failed, Hessian reset 598 1021.07 1.94887e-05 41.0613 3.113e-07 0.001 1157 LS failed, Hessian reset 599 1021.07 4.80514e-06 27.1307 0.643 0.643 1158 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 645 1021.07 8.81062e-06 74.3981 1.365e-07 0.001 1264 LS failed, Hessian reset 660 1021.07 2.61181e-07 41.0723 5.687e-09 0.001 1321 LS failed, Hessian reset 661 1021.07 9.2387e-08 26.7092 0.8838 0.8838 1322 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\rweekly_seasonality=True # # モデルを作成 m = Prophet( yearly_seasonality=False, weekly_seasonality=True, daily_seasonality=False, growth=\u0026#34;linear\u0026#34;, ) m.fit(df) # 将来を予測 future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig = m.plot(forecast) plt.axvspan(18627, 18627 + 90, color=\u0026#34;coral\u0026#34;, alpha=0.4, label=\u0026#34;予測期間\u0026#34;) plt.legend() plt.show() Initial log joint probability = -2.99674\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 99 1040.17 0.000845008 95.4398 0.3546 0.3546 128 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 166 1045.08 8.28451e-05 66.0092 1.087e-06 0.001 252 LS failed, Hessian reset 184 1045.09 1.58284e-05 61.1258 3.535e-07 0.001 307 LS failed, Hessian reset 199 1045.09 1.20349e-07 45.161 0.0243 1 330 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 264 1045.71 0.000316932 140.641 3.665e-06 0.001 456 LS failed, Hessian reset 292 1045.82 5.70302e-05 48.9256 1.155e-06 0.001 533 LS failed, Hessian reset 299 1045.82 7.71897e-06 40.8043 0.3543 1 544 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 305 1045.82 5.91182e-07 46.3158 2.336e-08 0.001 594 LS failed, Hessian reset 317 1045.82 2.63213e-06 51.4268 5.486e-08 0.001 649 LS failed, Hessian reset 343 1045.82 1.59952e-05 25.621 2.535e-07 0.001 728 LS failed, Hessian reset 353 1045.82 6.70942e-07 31.5639 1.825e-08 0.001 779 LS failed, Hessian reset 355 1045.82 3.11455e-08 40.1124 0.08424 1 783 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\ryearly_seasonality=True # # モデルを作成 m = Prophet( yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True, growth=\u0026#34;linear\u0026#34;, ) m.fit(df) # 将来を予測 future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig = m.plot(forecast) plt.axvspan(18627, 18627 + 90, color=\u0026#34;coral\u0026#34;, alpha=0.4, label=\u0026#34;予測期間\u0026#34;) plt.legend() plt.show() Initial log joint probability = -2.99674\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 99 1076.96 1.35095e-06 94.027 0.3775 0.3775 136 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 125 1076.96 2.65686e-06 89.152 2.895e-08 0.001 203 LS failed, Hessian reset 164 1076.96 2.28191e-08 91.6592 0.4114 0.4114 258 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\r休日・イベント効果 # 休日・イベントの指定なし(holidays=None) # # モデルを作成 m = Prophet( yearly_seasonality=False, weekly_seasonality=True, daily_seasonality=False, growth=\u0026#34;linear\u0026#34;, holidays=None, # 休日・イベントの指定なし ) m.fit(df) # 将来を予測 future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig = m.plot(forecast) plt.axvspan(18627, 18627 + 90, color=\u0026#34;coral\u0026#34;, alpha=0.4, label=\u0026#34;予測期間\u0026#34;) plt.legend() plt.show() Initial log joint probability = -2.99674\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 99 1040.17 0.000845008 95.4398 0.3546 0.3546 128 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 166 1045.08 8.28451e-05 66.0092 1.087e-06 0.001 252 LS failed, Hessian reset 184 1045.09 1.58284e-05 61.1258 3.535e-07 0.001 307 LS failed, Hessian reset 199 1045.09 1.20349e-07 45.161 0.0243 1 330 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 264 1045.71 0.000316932 140.641 3.665e-06 0.001 456 LS failed, Hessian reset 292 1045.82 5.70302e-05 48.9256 1.155e-06 0.001 533 LS failed, Hessian reset 299 1045.82 7.71897e-06 40.8043 0.3543 1 544 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 305 1045.82 5.91182e-07 46.3158 2.336e-08 0.001 594 LS failed, Hessian reset 317 1045.82 2.63213e-06 51.4268 5.486e-08 0.001 649 LS failed, Hessian reset 343 1045.82 1.59952e-05 25.621 2.535e-07 0.001 728 LS failed, Hessian reset 353 1045.82 6.70942e-07 31.5639 1.825e-08 0.001 779 LS failed, Hessian reset 355 1045.82 3.11455e-08 40.1124 0.08424 1 783 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\r休日・イベントの指定あり # # モデルを作成 df_holidays = pd.DataFrame( {\u0026#34;holiday\u0026#34;: \u0026#34;event\u0026#34;, \u0026#34;ds\u0026#34;: [di for i, di in enumerate(date) if i in horidays_index]} ) m = Prophet( yearly_seasonality=False, weekly_seasonality=True, daily_seasonality=False, growth=\u0026#34;linear\u0026#34;, holidays=df_holidays, # 休日・イベントの指定あり ) m.fit(df) # 将来を予測 future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig = m.plot(forecast) plt.axvspan(18627, 18627 + 90, color=\u0026#34;coral\u0026#34;, alpha=0.4, label=\u0026#34;予測期間\u0026#34;) plt.legend() plt.show() Initial log joint probability = -2.99674\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 99 1343.59 0.00181622 336.302 0.9914 0.9914 120 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 199 1366.61 0.00129331 915.041 0.2388 0.9805 232 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 265 1372.99 0.000111116 289.963 4.926e-07 0.001 351 LS failed, Hessian reset 299 1374.36 0.00236219 89.8611 1 1 393 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 399 1375.61 0.00707432 82.6526 1 1 509 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 431 1375.93 8.82293e-05 240.658 4.05e-07 0.001 581 LS failed, Hessian reset 453 1376.02 6.12661e-05 95.4391 1.44e-06 0.001 644 LS failed, Hessian reset 471 1376.06 1.2062e-05 14.2904 2.239e-07 0.001 709 LS failed, Hessian reset 485 1376.1 1.9227e-05 69.5783 2.446e-07 0.001 772 LS failed, Hessian reset 493 1376.12 3.43778e-05 51.3583 1.873e-06 0.001 837 LS failed, Hessian reset 499 1376.13 2.95479e-05 34.8029 0.5524 0.5524 847 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 509 1376.13 8.88412e-06 44.6026 5.452e-07 0.001 897 LS failed, Hessian reset 521 1376.14 1.6737e-05 27.0944 1.441e-07 0.001 947 LS failed, Hessian reset 531 1376.14 7.82708e-06 47.5548 1.997e-07 0.001 1000 LS failed, Hessian reset 567 1376.28 3.47594e-05 113.402 3.767e-07 0.001 1079 LS failed, Hessian reset 599 1376.35 0.000359311 95.2959 1 1 1118 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 604 1376.35 4.84989e-06 13.344 1.606e-07 0.001 1168 LS failed, Hessian reset 612 1376.35 2.91053e-08 12.4075 0.04535 1 1191 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\r"},{"id":49,"href":"/basic/dimensionality_reduction/svd/","title":"SVD","section":"次元削減","content":"\rimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from scipy import linalg from PIL import Image 実験用のデータ # 「実験」という文字の画像をデータの系列（グレースケールなので[0, 0, 1, 1, 0 \u0026hellip;]のようなベクトルの集まり）に見立てて、特異値分解してみます。\nhttps://docs.scipy.org/doc/scipy/tutorial/linalg.html\nimg = Image.open(\u0026#34;./sample.png\u0026#34;).convert(\u0026#34;L\u0026#34;).resize((163, 372)).rotate(90, expand=True) img 特異値分解を実行 # ## 文字のデータが入った行列Aを特異値分解する X = np.asarray(img) U, Sigma, VT = linalg.svd(X, full_matrices=True) print(f\u0026#34;A: {X.shape}, U: {U.shape}, Σ:{Sigma.shape}, V^T:{VT.shape}\u0026#34;) A: (163, 372), U: (163, 163), Σ:(163,), V^T:(372, 372)\r低ランクで画像を近似 # # ランクを増やすと元の文字がより精密に復元される for rank in [1, 2, 3, 4, 5, 10, 20, 50]: # rank番目までの要素を抽出 U_i = U[:, :rank] Sigma_i = np.matrix(linalg.diagsvd(Sigma[:rank], rank, rank)) VT_i = VT[:rank, :] # 画像を復元 temp_image = np.asarray(U_i * Sigma_i * VT_i) Image.fromarray(np.uint8(temp_image)) plt.title(f\u0026#34;rank={rank}\u0026#34;) plt.imshow(temp_image, cmap=\u0026#34;gray\u0026#34;) plt.show() Vの中身 # total = np.zeros((163, 372)) for rank in [1, 2, 3, 4, 5]: # rank番目までの要素を抽出 U_i = U[:, :rank] Sigma_i = np.matrix(linalg.diagsvd(Sigma[:rank], rank, rank)) VT_i = VT[:rank, :] # rank番目の特異値以外をすべて０にして、rank番目の要素のみ残す if rank \u0026gt; 1: for ri in range(rank - 1): Sigma_i[ri, ri] = 0 # 画像を復元 temp_image = np.asarray(U_i * Sigma_i * VT_i) Image.fromarray(np.uint8(temp_image)) # rank番目の要素のみを足す total += temp_image # rank番目までの要素で復元した画像と、行列Vのrank列目の値のプロットを比較してみる plt.figure(figsize=(5, 5)) plt.suptitle(f\u0026#34;$u_{rank}$\u0026#34;) plt.subplot(211) plt.imshow(temp_image, cmap=\u0026#34;gray\u0026#34;) plt.subplot(212) plt.plot(VT[0]) plt.show() # 1~5番目の要素を足し合わせると、ちゃんと元の画像を復元できることを確認する plt.imshow(total) "},{"id":50,"href":"/prep/categorical/targetencoder/","title":"Target Encoder","section":"カテゴリカルデータ","content":" サンプルデータ # 「人口総数」を予測したいとして、「元号」をエンコードしたいとします。\nimport pandas as pd X = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;) TARGET_NAME = \u0026#34;人口総数\u0026#34; FEATURE_NAME = \u0026#34;元号\u0026#34; X.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rTargetEncoder # category_encoders.target_encoder.TargetEncoder sklearn.compose.make_column_transformer from category_encoders.target_encoder import TargetEncoder c_te = TargetEncoder() y = X[TARGET_NAME] X[f\u0026#34;{FEATURE_NAME}_te\u0026#34;] = c_te.fit_transform(X[FEATURE_NAME], y) 結果を確認する # カテゴリ変数の列が TargetEncoder でエンコードされていることを確認します。 この方法は、目的変数の平均値をそのままエンコードに使用します。つまり、あるデータをエンコードするためにそのデータの目的変数の情報を使用しています(leakage[1]と呼びます)。そのため、データ数が少ない場合は特に、実際に将来のデータに対して予測した場合とCVで評価した場合を比較すると、CV時に誤差が少なく見積もられる可能性がある点に注意して下さい。\n[1] Kaufman, Shachar, et al. \u0026ldquo;Leakage in data mining: Formulation, detection, and avoidance.\u0026rdquo; ACM Transactions on Knowledge Discovery from Data (TKDD) 6.4 (2012): 1-21.\nX[[FEATURE_NAME, f\u0026#34;{FEATURE_NAME}_te\u0026#34;]] .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r元号ごとの平均値 # 元号ごとのターゲットの平均値を用いてエンコードされていることを確認します\nX.groupby(FEATURE_NAME).agg(\u0026#34;mean\u0026#34;)[TARGET_NAME] 元号\r大正 144791.552083\r平成 100294.140000\r昭和 108003.279110\rName: 人口総数, dtype: float64\r"},{"id":51,"href":"/prep/data/","title":"データの読み込み・集約","section":"データ前処理","content":" Chapter 2 # データの読み込み・集約 # "},{"id":52,"href":"/visualize/category-groupby/","title":"カテゴリと数値","section":"可視化","content":" Chapter1 # カテゴリと数値 # "},{"id":53,"href":"/basic/ensemble/stucking/","title":"スタッキング","section":"アンサンブル","content":"\rimport matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import StackingClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.tree import export_graphviz from subprocess import call 実験用のデータを作成 # # 特徴が20あるデータを作成 n_features = 20 X, y = make_classification( n_samples=2500, n_features=n_features, n_informative=10, n_classes=2, n_redundant=0, n_clusters_per_class=4, random_state=777, ) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=777 ) データを複数の特徴に関してプロットしてみる # 単純なルールでは分類できなさそうであることを確認する。\nplt.figure(figsize=(10, 10)) plt.subplot(2, 2, 1) plt.scatter(X[:, 2], X[:, 7], c=y) plt.xlabel(\u0026#34;x2\u0026#34;) plt.ylabel(\u0026#34;x7\u0026#34;) plt.subplot(2, 2, 2) plt.scatter(X[:, 4], X[:, 9], c=y) plt.xlabel(\u0026#34;x4\u0026#34;) plt.ylabel(\u0026#34;x9\u0026#34;) plt.subplot(2, 2, 3) plt.scatter(X[:, 5], X[:, 1], c=y) plt.xlabel(\u0026#34;x5\u0026#34;) plt.ylabel(\u0026#34;x1\u0026#34;) plt.subplot(2, 2, 4) plt.scatter(X[:, 1], X[:, 3], c=y) plt.xlabel(\u0026#34;x1\u0026#34;) plt.ylabel(\u0026#34;x3\u0026#34;) plt.show() スタッキングとランダムフォレストの比較 # ランダムフォレストで分類してみた場合 # model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=777) model.fit(X_train, y_train) y_pred = model.predict(X_test) rf_score = roc_auc_score(y_test, y_pred) print(f\u0026#34;ROC-AUC = {rf_score}\u0026#34;) ROC-AUC = 0.855797033310609\r複数の木でスタッキングした場合 # DecisionTreeClassifierのみでスタッキングしても精度があまり向上しないことが確認できる。\nsklearn.ensemble.StackingClassifier\n# スタッキング前段に使用するモデル estimators = [ (\u0026#34;dt1\u0026#34;, DecisionTreeClassifier(max_depth=3, random_state=777)), (\u0026#34;dt2\u0026#34;, DecisionTreeClassifier(max_depth=4, random_state=777)), (\u0026#34;dt3\u0026#34;, DecisionTreeClassifier(max_depth=5, random_state=777)), (\u0026#34;dt4\u0026#34;, DecisionTreeClassifier(max_depth=6, random_state=777)), ] # スタッキングに含まれるモデル数 n_estimators = len(estimators) # スタッキング後段に使用するモデル final_estimator = DecisionTreeClassifier(max_depth=3, random_state=777) # スタッキングモデルを作成し訓練する clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator) clf.fit(X_train, y_train) # テストデータで評価する y_pred = clf.predict(X_test) clf_score = roc_auc_score(y_test, y_pred) print(\u0026#34;ROC-AUC\u0026#34;) print(f\u0026#34;決定木のスタッキング＝{clf_score}, ランダムフォレスト＝{rf_score}\u0026#34;) ROC-AUC\r決定木のスタッキング＝0.7359716965608031, ランダムフォレスト＝0.855797033310609\rスタッキングに使用した木を可視化してみる # export_graphviz( clf.final_estimator_, out_file=\u0026#34;tree_final_estimator.dot\u0026#34;, class_names=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;], feature_names=[e[0] for e in estimators], proportion=True, filled=True, ) call( [ \u0026#34;dot\u0026#34;, \u0026#34;-Tpng\u0026#34;, \u0026#34;tree_final_estimator.dot\u0026#34;, \u0026#34;-o\u0026#34;, f\u0026#34;tree_final_estimator.png\u0026#34;, \u0026#34;-Gdpi=200\u0026#34;, ] ) display(Image(filename=\u0026#34;tree_final_estimator.png\u0026#34;)) スタッキングで使用した木の特徴重要度を見る # スタッキングしたものの、結局4番目の木しかほとんど予測に利用されていないことが分かる。\nplt.figure(figsize=(6, 3)) plot_index = [i for i in range(n_estimators)] plt.bar(plot_index, clf.final_estimator_.feature_importances_) plt.xticks(plot_index, [e[0] for e in estimators]) plt.xlabel(\u0026#34;model name\u0026#34;) plt.ylabel(\u0026#34;feature-importance\u0026#34;) plt.show() スタッキング前段の各木の性能を確認する # # スタッキング前段の木の精度を計測する scores = [] for clf_estim in clf.estimators_: print(\u0026#34;====\u0026#34;) y_pred = clf_estim.predict(X_test) scr = roc_auc_score(y_test, y_pred) scores.append(scr) print(clf_estim) print(scr) n_estimators = len(estimators) plot_index = [i for i in range(n_estimators)] # グラフ作成 plt.figure(figsize=(8, 4)) plt.bar(plot_index, scores) plt.xticks(plot_index, [e[0] for e in estimators]) plt.xlabel(\u0026#34;model name\u0026#34;) plt.ylabel(\u0026#34;roc-auc\u0026#34;) plt.show() ====\rDecisionTreeClassifier(max_depth=3, random_state=777)\r0.7660117774277722\r====\rDecisionTreeClassifier(max_depth=4, random_state=777)\r0.7744128916993818\r====\rDecisionTreeClassifier(max_depth=5, random_state=777)\r0.8000158677919086\r====\rDecisionTreeClassifier(max_depth=6, random_state=777)\r0.8084639977432473\r・Stacked Generalization (Stacking) ・MLWave/Kaggle-Ensemble-Guide\n"},{"id":54,"href":"/visualize/category-groupby/treemap/","title":"ツリーマップ","section":"カテゴリと数値","content":"ツリーマップとは階層的なカテゴリを持つ数値データの可視化に使用できる図です。日経平均やS\u0026amp;P 500が代表的な例です。このnotebookではsquarifyを使用します。\nツリーマップはplotlyを使うことでも作成できます。（参考： Treemap charts with Python - Plotly）\nimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib import squarify np.random.seed(0) # 乱数を固定する labels = [\u0026#34;A\u0026#34; * i for i in range(1, 5)] sizes = [i * 10 for i in range(1, 5)] colors = [\u0026#34;#%02x%02x%02x\u0026#34; % (i * 50, 0, 0) for i in range(1, 5)] plt.figure(figsize=(5, 5)) squarify.plot(sizes, color=colors, label=labels) plt.axis(\u0026#34;off\u0026#34;) plt.show() ポートフォリオを可視化する # 持っている銘柄ごとの取得価格・現在価格のデータがあったとします。 そこから、finvizのようなヒートマップを作ってみます。\nデータを作成する # 仮に、csvから以下のようなデータを読み込んだとします。\n※ここに示しているのは架空のデータです\nimport pandas as pd data = [ [\u0026#34;PBR\u0026#34;, 80.20, 130.00], [\u0026#34;GOOG\u0026#34;, 1188.0, 1588.0], [\u0026#34;FLNG\u0026#34;, 70.90, 230.00], [\u0026#34;ZIM\u0026#34;, 400.22, 630.10], [\u0026#34;GOGL\u0026#34;, 120.20, 90.90], [\u0026#34;3466\\nラサールロジ\u0026#34;, 156.20, 147.00], # 日本語表示のテスト用 ] df = pd.DataFrame(data) df.columns = [\u0026#34;銘柄名\u0026#34;, \u0026#34;取得価額\u0026#34;, \u0026#34;現在の価額\u0026#34;] df[\u0026#34;評価損益\u0026#34;] = df[\u0026#34;現在の価額\u0026#34;] - df[\u0026#34;取得価額\u0026#34;] df.head(6) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rツリーマップの色を指定する # 利益が出ている箇所を緑色、損が出ている箇所を赤色にします。\ncolors = [] percents = [] for p_or_l, oac in zip(df[\u0026#34;評価損益\u0026#34;], df[\u0026#34;取得価額\u0026#34;]): percent = p_or_l / oac * 100 if p_or_l \u0026gt; 0: g = np.min([percent * 255 / 100 + 100, 255.0]) color = \u0026#34;#%02x%02x%02x\u0026#34; % (0, int(g), 0) colors.append(color) else: r = np.min([-percent * 255 / 100 + 100, 255]) color = \u0026#34;#%02x%02x%02x\u0026#34; % (int(r), 0, 0) colors.append(color) percents.append(percent) print(df[\u0026#34;銘柄名\u0026#34;].values) print(colors) print(percents) ['PBR' 'GOOG' 'FLNG' 'ZIM' 'GOGL' '3466\\nラサールロジ']\r['#00ff00', '#00b900', '#00ff00', '#00f600', '#a20000', '#730000']\r[62.094763092269325, 33.670033670033675, 224.4005641748942, 57.43840887511868, -24.376039933444257, -5.8898847631241935]\rツリーマップを表示する # 損益を色で表現し、損益の割合もツリーマップ上に表示してみます。 日本語が文字化けしていないのは冒頭で import japanize_matplotlib としているためです。\ncurrent_prices = [cp for cp in df[\u0026#34;現在の価額\u0026#34;]] labels = [ f\u0026#34;{name}\\n{np.round(percent, 2)}％\u0026#34;.replace(\u0026#34;-\u0026#34;, \u0026#34;▼\u0026#34;) for name, percent in zip(df[\u0026#34;銘柄名\u0026#34;], percents) ] plt.figure(figsize=(10, 10)) plt.rcParams[\u0026#34;font.size\u0026#34;] = 18 squarify.plot(current_prices, color=colors, label=labels) plt.axis(\u0026#34;off\u0026#34;) plt.show() キャッシュの表示を追加する # キャッシュの表示も追加してみます。色はグレーにします。\nplt.figure(figsize=(10, 10)) plt.rcParams[\u0026#34;font.size\u0026#34;] = 18 squarify.plot( current_prices + [3500], color=colors + [\u0026#34;#ccc\u0026#34;], label=labels + [\u0026#34;キャッシュ\u0026#34;] ) plt.axis(\u0026#34;off\u0026#34;) plt.show() "},{"id":55,"href":"/timeseries/preprocess/002-preprocess-trend/","title":"トレンドの除去","section":"プロットと前処理","content":"\rimport japanize_matplotlib import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns サンプルデータを作成 # date_list = pd.date_range(\u0026#34;2021-01-01\u0026#34;, periods=720, freq=\u0026#34;D\u0026#34;) value_list = [ 10 + np.cos(np.pi * i / 28.0) * (i % 3 \u0026gt; 0) + np.cos(np.pi * i / 14.0) * (i % 5 \u0026gt; 0) + np.cos(np.pi * i / 7.0) + (i / 10) ** 1.1 / 20 for i, di in enumerate(date_list) ] df = pd.DataFrame( { \u0026#34;日付\u0026#34;: date_list, \u0026#34;観測値\u0026#34;: value_list, } ) df.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;観測値\u0026#34;]) XGBoostで時系列データを予測してみる # df[\u0026#34;曜日\u0026#34;] = df[\u0026#34;日付\u0026#34;].dt.weekday df[\u0026#34;年初からの日数%14\u0026#34;] = df[\u0026#34;日付\u0026#34;].dt.dayofyear % 14 df[\u0026#34;年初からの日数%28\u0026#34;] = df[\u0026#34;日付\u0026#34;].dt.dayofyear % 28 def get_trend(timeseries, deg=3, trainN=0): \u0026#34;\u0026#34;\u0026#34;時系列データのトレンドの線を作成する Args: timeseries(pd.Series) : 時系列データ。 deg(int) : 多項式の次数 trainN(int): 多項式の係数を推定するために使用するデータ数 Returns: pd.Series: トレンドに相当する時系列データ。 \u0026#34;\u0026#34;\u0026#34; if trainN == 0: trainN = len(timeseries) x = list(range(len(timeseries))) y = timeseries.values coef = np.polyfit(x[:trainN], y[:trainN], deg) trend = np.poly1d(coef)(x) return pd.Series(data=trend, index=timeseries.index) trainN = 500 df[\u0026#34;トレンド\u0026#34;] = get_trend(df[\u0026#34;観測値\u0026#34;], trainN=trainN, deg=2) plt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;観測値\u0026#34;]) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;トレンド\u0026#34;]) X = df[[\u0026#34;曜日\u0026#34;, \u0026#34;年初からの日数%14\u0026#34;, \u0026#34;年初からの日数%28\u0026#34;]] y = df[\u0026#34;観測値\u0026#34;] trainX, trainy = X[:trainN], y[:trainN] testX, testy = X[trainN:], y[trainN:] trend_train, trend_test = df[\u0026#34;トレンド\u0026#34;][:trainN], df[\u0026#34;トレンド\u0026#34;][trainN:] トレンドを考慮せずに予測する # XGBoostは訓練データとテストデータでデータが緩やかに変化していることを知りません。 そのため、未来を予測するほど予測が下に外れてしまいます。 XGBoostで予測をうまく行うには、訓練データとテストデータのｙの分布を近くする必要があります。\nトレンドは特徴量にも含まれているかもしれません。 与えられた入力に対する出力の生成規則が訓練データとテストデータで変わらないものの、 入力の分布が訓練データとテストデータで異なる状態は共変量シフト(covariate shift)とも呼ばれます。\nimport xgboost as xgb from sklearn.metrics import mean_squared_error regressor = xgb.XGBRegressor(max_depth=5).fit(trainX, trainy) prediction = regressor.predict(testX) plt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;][trainN:], y=prediction) sns.lineplot(x=df[\u0026#34;日付\u0026#34;][trainN:], y=testy) plt.legend([\u0026#34;モデルの出力\u0026#34;, \u0026#34;正解\u0026#34;], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102)) print(f\u0026#34;誤差(MSE) = {mean_squared_error(testy, prediction)}\u0026#34;) 誤差(MSE) = 2.815118389938834\rトレンドを考慮して予測する # 先にトレンドに相当する分を観測値から取り除いて、トレンドがない状態の数値を予測します。 トレンドを取り除きさえすれば、訓練データとテストデータでのｙの分布は同じものになり、XGBoostでの予測がうまくいくことが予想されます。XGBoostの予測値にトレンドに相当する分を足して最終的な予測値としています。\nregressor = xgb.XGBRegressor(max_depth=5).fit(trainX, trainy - trend_train) prediction = regressor.predict(testX) prediction = [pred_i + trend_i for pred_i, trend_i in zip(prediction, trend_test)] plt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;][trainN:], y=prediction) sns.lineplot(x=df[\u0026#34;日付\u0026#34;][trainN:], y=testy) plt.legend([\u0026#34;モデルの出力＋トレンド\u0026#34;, \u0026#34;正解\u0026#34;], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102)) print(f\u0026#34;誤差(MSE) = {mean_squared_error(testy, prediction)}\u0026#34;) 誤差(MSE) = 0.46014173311011325\r"},{"id":56,"href":"/basic/regression/ridge_and_lasso/","title":"リッジ回帰・ラッソ回帰","section":"線形回帰","content":"\r必要なモジュールをインポート # import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import cross_val_score from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler 実験用の回帰データを作成する # sklearn.datasets.make_regression\nsklearn.datasets.make_regressionを用いて人工的に回帰データを作成します。予測に必要な特徴量は2つ(n_informative = 2)と指定します。それ以外は予測の役に立たない冗長な特徴量です。\nn_features = 5 n_informative = 2 X, y = make_regression( n_samples=500, n_features=n_features, n_informative=n_informative, noise=0.5, random_state=777, ) X = np.concatenate([X, np.log(X + 100)], 1) # 冗長な特徴量を追加する y_mean = y.mean(keepdims=True) y_std = np.std(y, keepdims=True) y = (y - y_mean) / y_std 最小二乗法・リッジ回帰・ラッソ回帰モデルを比較する # それぞれのモデルについて同じデータをつかって訓練して違いを見てみます。 本来は正則化項の係数alphaをいろいろ変えて試すべきですが、ここでは固定しています。\nsklearn.pipeline.make_pipeline sklearn.linear_model.LinearRegression sklearn.linear_model.Ridge sklearn.linear_model.Lasso lin_r = make_pipeline(StandardScaler(with_mean=False), LinearRegression()).fit(X, y) rid_r = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=2)).fit(X, y) las_r = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha=0.1)).fit(X, y) 各モデルの係数の値を比較する # 各係数の絶対値をプロットします。線形回帰は係数がほとんど０にならないことがグラフから見て取れます。 また、Lasso回帰は予測に必要な特徴量以外で係数＝０になっていることが確認できます。\n有用な特徴は２個（n_informative = 2）だったので、線形回帰は有用でない特徴でも無条件に係数がついていることがわかります。Lasso・Ridge回帰は２個まで絞り込むことはできませんでしたが、それでも多くの不要な特徴で係数が０になるという結果になりました。\nfeat_index = np.array([i for i in range(X.shape[1])]) plt.figure(figsize=(12, 4)) plt.bar( feat_index - 0.2, np.abs(lin_r.steps[1][1].coef_), width=0.2, label=\u0026#34;LinearRegression\u0026#34;, ) plt.bar(feat_index, np.abs(rid_r.steps[1][1].coef_), width=0.2, label=\u0026#34;RidgeRegression\u0026#34;) plt.bar( feat_index + 0.2, np.abs(las_r.steps[1][1].coef_), width=0.2, label=\u0026#34;LassoRegression\u0026#34;, ) plt.xlabel(r\u0026#34;$\\beta$のインデックス\u0026#34;) plt.ylabel(r\u0026#34;$|\\beta|$\u0026#34;) plt.grid() plt.legend() "},{"id":57,"href":"/basic/anomaly/adtk2/","title":"異常検知②","section":"異常検知","content":"Anomaly Detection Toolkit (ADTK)を使った異常検知をしてみます。 多次元の人工データに対して異常検知を適用します。今度は複数次元のデータに対して異常検知を適用します。\nimport numpy as np import pandas as pd from adtk.data import validate_series s_train = pd.read_csv(\u0026#34;./training.csv\u0026#34;, index_col=\u0026#34;timestamp\u0026#34;, parse_dates=True) s_train = validate_series(s_train) s_train[\u0026#34;value2\u0026#34;] = s_train[\u0026#34;value\u0026#34;].apply(lambda v: np.sin(v) + np.cos(v)) s_train .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rfrom adtk.visualization import plot plot(s_train) 異常検知手法の比較 # SeasonalADを用いた異常検知を行います。他の手法はDetectorを参照してください。\nimport matplotlib.pyplot as plt from adtk.detector import OutlierDetector, PcaAD, RegressionAD from sklearn.linear_model import LinearRegression from sklearn.neighbors import LocalOutlierFactor model_dict = { \u0026#34;OutlierDetector\u0026#34;: OutlierDetector(LocalOutlierFactor(contamination=0.05)), \u0026#34;RegressionAD\u0026#34;: RegressionAD(regressor=LinearRegression(), target=\u0026#34;value2\u0026#34;, c=3.0), \u0026#34;PcaAD\u0026#34;: PcaAD(k=2), } for model_name, model in model_dict.items(): anomalies = model.fit_detect(s_train) plot( s_train, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color=\u0026#34;red\u0026#34;, anomaly_alpha=0.3, curve_group=\u0026#34;all\u0026#34;, ) plt.title(model_name) plt.show() "},{"id":58,"href":"/finance/visualize/","title":"可視化","section":"経済データ","content":" Chapter 2 # 可視化 # "},{"id":59,"href":"/eval/regression/","title":"回帰","section":"評価指標","content":" Chapter 2 # 回帰 # "},{"id":60,"href":"/basic/","title":"機械学習","section":"トップページ","content":" Section 2 # 機械学習 # Wikipedia 機械学習\n"},{"id":61,"href":"/eval/regression/r2/","title":"決定係数","section":"回帰","content":"\r決定係数（けっていけいすう、（英: coefficient of determination、R2）は、統計学において、独立変数（説明変数）が従属変数（目的変数）のどれくらいを説明できるかを表す値である。寄与率と呼ばれることもある。 出典： 決定係数 出典: フリー百科事典『ウィキペディア（Wikipedia）』\n一般に高ければ高いほどよい評価指標 最も良い場合に1になる ただし、特徴を増やせば増やすほど高くなる傾向にある この指標だけを見て「モデルの精度が高い」とは判断できない import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_regression from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split サンプルデータに対してモデルを作成し決定係数を計算 # 予測が当たりやすいデータを作成 # X, y = make_regression( n_samples=1000, n_informative=3, n_features=20, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) model = RandomForestRegressor(max_depth=5) model.fit(train_X, train_y) pred_y = model.predict(test_X) 決定係数を計算 # sklearn.metrics.roc_auc_score\nfrom sklearn.metrics import r2_score r2 = r2_score(test_y, pred_y) y_min, y_max = np.min(test_y), np.max(test_y) plt.figure(figsize=(6, 6)) plt.title(f\u0026#34;$R^2 =${r2}\u0026#34;) plt.plot([y_min, y_max], [y_min, y_max], linestyle=\u0026#34;-\u0026#34;, c=\u0026#34;k\u0026#34;, alpha=0.2) plt.scatter(test_y, pred_y, marker=\u0026#34;x\u0026#34;) plt.xlabel(\u0026#34;正解\u0026#34;) plt.ylabel(\u0026#34;予測\u0026#34;) Text(0, 0.5, '予測')\r予測が当たりにくいデータを作成 # 決定係数が下がることを確認します。\nX, y = make_regression( n_samples=1000, n_informative=3, n_features=20, effective_rank=4, noise=1.5, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) model = RandomForestRegressor(max_depth=5) model.fit(train_X, train_y) pred_y = model.predict(test_X) r2 = r2_score(test_y, pred_y) y_min, y_max = np.min(test_y), np.max(test_y) plt.figure(figsize=(6, 6)) plt.title(f\u0026#34;$R^2 =${r2}\u0026#34;) plt.plot([y_min, y_max], [y_min, y_max], linestyle=\u0026#34;-\u0026#34;, c=\u0026#34;k\u0026#34;, alpha=0.2) plt.scatter(test_y, pred_y, marker=\u0026#34;x\u0026#34;) plt.xlabel(\u0026#34;正解\u0026#34;) plt.ylabel(\u0026#34;予測\u0026#34;) Text(0, 0.5, '予測')\r予測がデタラメな場合 # 単純に平均を予測するよりも更に精度が悪い時、決定係数がマイナスになることを確認します。\nX, y = make_regression( n_samples=1000, n_informative=3, n_features=20, effective_rank=4, noise=1.5, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) # train_yをランダムに並び替え、さらに値を変換する train_y = np.random.permutation(train_y) train_y = np.sin(train_y) * 10 + 1 model = RandomForestRegressor(max_depth=1) model.fit(train_X, train_y) pred_y = model.predict(test_X) r2 = r2_score(test_y, pred_y) y_min, y_max = np.min(test_y), np.max(test_y) plt.figure(figsize=(6, 6)) plt.title(f\u0026#34;$R^2 =${r2}\u0026#34;) plt.plot([y_min, y_max], [y_min, y_max], linestyle=\u0026#34;-\u0026#34;, c=\u0026#34;k\u0026#34;, alpha=0.2) plt.scatter(test_y, pred_y, marker=\u0026#34;x\u0026#34;) plt.xlabel(\u0026#34;正解\u0026#34;) plt.ylabel(\u0026#34;予測\u0026#34;) Text(0, 0.5, '予測')\r最小二乗法を使用した時の決定係数 # 最小二乗法による単回帰の回帰直線の場合、決定係数の範囲が $ 0 \\le R^2 \\le 1$ になります。 データにランダムにノイズを載せて、100回線形回帰を実行し決定係数を求めてみます。\nfrom sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler r2_scores = [] for i in range(100): # データ作成 X, y = make_regression( n_samples=500, n_informative=1, n_features=1, effective_rank=4, noise=i * 0.1, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) # 線形回帰 model = make_pipeline( StandardScaler(with_mean=False), LinearRegression(positive=True) ).fit(train_X, train_y) # 決定係数を算出 pred_y = model.predict(test_X) r2 = r2_score(test_y, pred_y) r2_scores.append(r2) plt.figure(figsize=(8, 4)) plt.title(\u0026#34;100回ランダムなデータで線形回帰を実行した時の$R^2$\u0026#34;) plt.hist(r2_scores, bins=20) plt.show() "},{"id":62,"href":"/basic/tree/decision_tree_regressor/","title":"決定木(回帰)","section":"決定木","content":"\rfrom sklearn.tree import DecisionTreeRegressor from sklearn.datasets import make_regression from dtreeviz.trees import * import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from dtreeviz.trees import dtreeviz dtreeviz : Decision Tree Visualization\n決定木を作るためのサンプルデータを作成 # X, y = make_regression(n_samples=100, n_features=2, random_state=777) plt.figure(figsize=(10, 10)) plt.scatter(X[:, 0], X[:, 1], c=y) plt.show() 回帰木の分岐の仕方を確認する # tree = DecisionTreeRegressor(max_depth=3, random_state=117117) model = tree.fit(X, y) viz = dtreeviz(tree, X, y, target_name=\u0026#34;y\u0026#34;) viz.save(\u0026#34;./regression_tree.svg\u0026#34;) 回帰木の分岐を可視化 # from IPython.display import SVG SVG(filename=\u0026#34;regression_tree.svg\u0026#34;) "},{"id":63,"href":"/timeseries/exponential_smoothing/","title":"指数平滑化","section":"時系列","content":" Chapter 2 # 指数平滑化 # "},{"id":64,"href":"/prep/table/show_progress/","title":"処理の進捗を表示","section":"テーブルデータ全般","content":"pandasでデータ加工をする時、時間がかかる場合があります。 この時にtqdmを用いて進捗を表示することができます。\ntqdm Pandas Integration\nDue to popular demand we\u0026rsquo;ve added support for pandas \u0026ndash; here\u0026rsquo;s an example for DataFrame.progress_apply and DataFrameGroupBy.progress_apply:\nimport pandas as pd df = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;)[:100] df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r処理の進捗を表示する # rand_with_sleep(x: int)はランダムな時間だけ待機して待機した時間(sec)を返します。\ndf[\u0026quot;人口総数\u0026quot;] には待機した時間が記録され、この処理の進捗のプログレスバーが表示されます。\nimport numpy as np from time import sleep from tqdm import tqdm tqdm.pandas() def rand_with_sleep(x: int) -\u0026gt; float: rnd = np.random.rand() / 50.0 sleep(rnd) return f\u0026#34;{rnd} sec\u0026#34; df[\u0026#34;人口総数\u0026#34;] = df[\u0026#34;人口総数\u0026#34;].progress_apply(lambda x: rand_with_sleep(x)) df.head() 100%|█████████████████████████████████████| 100/100 [00:01\u0026lt;00:00, 78.99it/s]\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":65,"href":"/visualize/distribution/","title":"数値の分布","section":"可視化","content":" Chapter2 # 数値の分布 # "},{"id":66,"href":"/eval/model-selection/stratified_k_fold/","title":"層状k分割交差検証","section":"モデル選択","content":"\rsklearn.model_selection.StratifiedKFold\nimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold, StratifiedKFold from sklearn.metrics import roc_auc_score サンプルデータに対してモデルを作成し交差検証 # 実験用データ # n_classes = 10 X, y = make_classification( n_samples=210, n_classes=n_classes, n_informative=n_classes, n_features=12, n_clusters_per_class=1, weights=[0.82, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02], ## クラスごとの比率 random_state=RND, ) plt.title(\u0026#34;yに含まれるクラスごとのデータ数\u0026#34;) plt.hist(y) plt.xlabel(\u0026#34;ラベル名\u0026#34;) plt.ylabel(\u0026#34;データ数\u0026#34;) plt.show() 分割したデータに含まれるラベルの比率 # データを4分割して、訓練・検証用データに含まれるラベルの比率を確認します。\nStratifiedKFold # 訓練データと検証用データで、ラベルの比率が同じになっていることが確認できます。\nskf = StratifiedKFold(n_splits=4) for train_index, valid_index in skf.split(X, y): X_train, X_valid = X[train_index], X[valid_index] y_train, y_valid = y[train_index], y[valid_index] # model = RandomForestClassifier(max_depth=5, random_state=RND) # model.fit(X_train, y_train) plt.figure(figsize=(8, 2)) plt.subplot(121) plt.title(\u0026#34;訓練データ\u0026#34;) train_label_cnt = [(y_train == i).sum() for i in range(n_classes)] plt.ylabel(\u0026#34;データ数\u0026#34;) plt.bar(np.arange(n_classes), train_label_cnt) plt.subplot(122) plt.title(\u0026#34;検証用データ\u0026#34;) valid_label_cnt = [(y_valid == i).sum() for i in range(n_classes)] plt.bar(np.arange(n_classes), valid_label_cnt) plt.show() KFold # テストデータにデータが無いクラスがあることが確認できます。\nkf = KFold(n_splits=4) for train_index, valid_index in kf.split(X, y): X_train, X_valid = X[train_index], X[valid_index] y_train, y_valid = y[train_index], y[valid_index] # model = RandomForestClassifier(max_depth=5, random_state=RND) # model.fit(X_train, y_train) plt.figure(figsize=(8, 2)) plt.subplot(121) plt.title(\u0026#34;訓練データ\u0026#34;) train_label_cnt = [(y_train == i).sum() for i in range(n_classes)] plt.ylabel(\u0026#34;データ数\u0026#34;) plt.bar(np.arange(n_classes), train_label_cnt) plt.subplot(122) plt.title(\u0026#34;検証用データ\u0026#34;) valid_label_cnt = [(y_valid == i).sum() for i in range(n_classes)] plt.bar(np.arange(n_classes), valid_label_cnt) plt.show() "},{"id":67,"href":"/finance/nlp/mp32srt/","title":"動画からの音声認識","section":"テキスト処理","content":"\r仮想環境の作成 # Anacondaを使用している場合は、以下のコマンドを順番に実行することで py39-whisper という名前のpython3.9実行環境が作成されます。\nconda create -n py39-whisper python=3.9 anaconda\rconda activate py39-whisper\rpip install git+https://github.com/openai/whisper.git\rconda install ffmpeg -c conda-forge\rconda install jupyter ipykernel pandas\rconda install transformers[sentencepiece]\rwhisperを使って音声認識を実行する # 音声データは『Flex LNG Q3 2022 Key Takeaways』の音声を使用しています。\nimport warnings import whisper from transformers import pipeline model = whisper.load_model(\u0026#34;base\u0026#34;) result = model.transcribe(\u0026#34;FlexLNG_Q3_2022_Key_Takeaways.mp3\u0026#34;) print(result[\u0026#34;text\u0026#34;]) Hi and welcome to FlexLng\u0026#39;s TURD Quater Highlights. Revenue\u0026#39;s 4D Quater came in at 91 million in line with previous guidance of approximately 90 million. Ernings was strong, net income and adjusted net income was 47.42 million, translating into Ernings per share and adjusted Ernings per share of 88.79 respectively. Freight market during the quarter boomed and this affected both short term and long term rates positively. During the quarter we had three ships... 日本語を英語に翻訳する # 翻訳の正しさは保証されておらず、また本コードも動作や出力に関しては一切の責任を負えません。\nopenai/whisper 翻訳に使用するモデル ニューラル機械翻訳モデルFuguMT staka/fugumt-en-ja - Hugging Face MAX_LENGTH = 400 # モデルに入力することができる文字数の上限 translator = pipeline(\u0026#34;translation\u0026#34;, model=\u0026#34;staka/fugumt-en-ja\u0026#34;) translated_text = [] for t in result[\u0026#34;text\u0026#34;].split(\u0026#34;. \u0026#34;): translated_text.append(translator(t[:MAX_LENGTH])[0][\u0026#34;translation_text\u0026#34;]) print(translated_text) [\u0026#39;Flexingの第3四半期ハイライトへようこそ。\u0026#39;,\r\u0026#39;Revenueの4D Quaterは9100万で、前回のガイダンスで約9000万だった。\u0026#39;,\r\u0026#39;Erningsは好調で、純利益と調整済み純利益は47.42百万で、それぞれ1株当たりErningsに翻訳され、調整済みErningsは88.79だった。\u0026#39;,\r\u0026#39;第4四半期の貨物市場は急成長し、短期と長期の両方にプラスの影響を与えました。\u0026#39;,\r\u0026#39;四半期中は3隻の船が新船のチャーターを開始し\u0026#39;,\r\u0026#39;6月にフレックスと価格とフレックスアンバーの両方の7年間のチャーターを発表し、これらの船は短期契約の代わりに7月にこれらの新しい長期チャーターを開始しました。\u0026#39;,\r\u0026#39;また、第4四半期末には、シェニエとの契約により、最終第5船としてフレクサーオーラをチェニエに納入しました。\u0026#39;,\r\u0026#39;CFOの四半期中は、バランスシート最適化プログラムフェーズ2の下で再融資に忙しかったため、追加の1億ドルの現金を調達する目標がありました。この6億3000万の資金を4隻の船舶に調達することで、すでに1億1000万の現金リリースを確保しています。\u0026#39;,\r\u0026#39;また、新たに3隻の船舶をリファイナンスし、バランスシート最適化プログラムの目標を3億に引き上げることはできません。\u0026#39;,\r\u0026#39;フェーズ1では1億3700万ドルを\u0026#39;,\r\u0026#39;本日 110を発売すると発表しました\u0026#39;, "},{"id":68,"href":"/basic/classification/linear_discriminant_analysis/","title":"判別分析","section":"線形分類","content":"\rimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_blobs from sklearn.discriminant_analysis import LinearDiscriminantAnalysis サンプル用データの作成 # n_samples = 200 X, y = make_blobs(n_samples=200, centers=2, n_features=2, cluster_std=2) X[:, 0] -= np.mean(X[:, 0]) X[:, 1] -= np.mean(X[:, 1]) fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;データの散布図\u0026#34;, fontsize=20) plt.scatter(X[:, 0], X[:, 1], c=y) plt.show() 線形判別分析法で決定境界を求める # sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n# 決定境界を求める clf = LinearDiscriminantAnalysis(store_covariance=True) clf.fit(X, y) # どのような決定境界が引かれたかを確認する w = clf.coef_[0] wt = -1 / (w[1] / w[0]) ## wに垂直な傾きを求める xs = np.linspace(-10, 10, 100) ys_w = [(w[1] / w[0]) * xi for xi in xs] ys_wt = [wt * xi for xi in xs] fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;決定境界の傾きを可視化\u0026#34;, fontsize=20) plt.scatter(X[:, 0], X[:, 1], c=y) # サンプルデータ plt.plot(xs, ys_w, \u0026#34;-.\u0026#34;, color=\u0026#34;k\u0026#34;, alpha=0.5) # ｗの向き plt.plot(xs, ys_wt, \u0026#34;--\u0026#34;, color=\u0026#34;k\u0026#34;) # ｗに垂直な向き plt.xlim(-10, 10) plt.ylim(-10, 10) plt.show() # 求めたベクトルｗを元に１次元にデータを移した結果 X_1d = clf.transform(X).reshape(1, -1)[0] fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;１次元にデータを移した場合のデータの位置\u0026#34;, fontsize=15) plt.scatter(X_1d, [0 for _ in range(n_samples)], c=y) plt.show() 2次元以上のデータでの例 # X_3d, y_3d = make_blobs(n_samples=200, centers=3, n_features=3, cluster_std=3) # サンプルデータの分布 fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;データの散布図\u0026#34;, fontsize=20) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y_3d) plt.show() # 線形判別分析を適用 clf_3d = LinearDiscriminantAnalysis() clf_3d.fit(X_3d, y_3d) X_2d = clf_3d.transform(X_3d) # 判別分析で次元削減した結果 fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;2次元にデータを移した場合のデータの位置\u0026#34;, fontsize=15) plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_3d) plt.show() "},{"id":69,"href":"/visualize/distribution/densityplot/","title":"密度プロット","section":"数値の分布","content":"数値データがどのように分布しているかを可視化します。\nseaborn.kdeplot\nimport matplotlib.pyplot as plt import seaborn as sns df = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (12, 8)}) sns.kdeplot(df[\u0026#34;sepal_length\u0026#34;]) sns.kdeplot(df[\u0026#34;sepal_width\u0026#34;]) sns.kdeplot(df[\u0026#34;petal_length\u0026#34;]) sns.kdeplot(df[\u0026#34;petal_width\u0026#34;]) plt.legend(labels=[\u0026#34;sepal_length\u0026#34;, \u0026#34;sepal_width\u0026#34;, \u0026#34;petal_length\u0026#34;, \u0026#34;petal_width\u0026#34;]) "},{"id":70,"href":"/install/useful_links/","title":"役立つ資料集","section":"はじめに","content":"いろいろ調べる中で見つけた便利・有用なページです。実際に参考にしたページは各ページ内で参考文献（もしくは出典）として表示します。\nPython全般 # Python # GitHub の使い方 プログラミング演習 Python 2019 - 京都大学学術情報リポジトリ What\u0026rsquo;s New in Python unittest \u0026mdash; ユニットテストフレームワーク 開発 # Visual Studio Code を使用して 開発環境をセットアップする | MICROSOFT LEARN Google\u0026rsquo;s Python Style Guide The Black Code Style Poetry: Dependency Management for Python pandas/numpy/scikit-learn # scikit-learn tutorials 10 minutes to pandas — pandas 1.3.4 documentation Cookbook — pandas 1.3.4 documentation NumPy quickstart — NumPy v1.21 Manual SciPy User Guide — SciPy v1.7.1 Manual statmodel Introduction 機械学習・データ分析 # 講義 # 朱鷺の杜Wiki（ときのもり うぃき） 神嶌 敏弘先生の解説・講義資料 TUKUBA OCW 機械学習 筑波大学 システム情報系教授　佐久間 淳 先生 UTokyo OCW 情報数理科学 VII 機械学習帳(岡崎直観先生の資料) 機械学習 Online Courses | Coursera 深層学習 # 深層学習 DS-GA 1008 · 2020 年度春学期 · ニューヨーク大学データ・サイエンス・センター 著作権と権利 # 著作権 | 文化庁 人工知能学会倫理委員会 機械学習と公平性に関する声明 機械学習と公平性に関する声明とシンポジウム 私のブックマーク「機械学習のプライバシーとセキュリティ」 AIと著作権　学習用データセットの生成 EU 一般データ保護規則の概要（前編） GDPR（General Data Protection Regulation：一般データ保護規則） "},{"id":71,"href":"/timeseries/forecast/003-prophet-parameter/","title":" Prophetのパラメータ","section":"Prophet","content":"Prophetにどのようなパラメータがあるか整理します。\nこのページは書きかけです\nimport numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import japanize_matplotlib from prophet import Prophet 実験に使用するデータ # date = pd.date_range(\u0026#34;2020-01-01\u0026#34;, periods=365, freq=\u0026#34;D\u0026#34;) # 予測対象 y = [ np.cos(di.weekday()) / 3 + di.month % 2 / 5 + np.log(i + 1) / 5.0 + (i \u0026gt; 182) * 0.5 + np.random.rand() / 10 for i, di in enumerate(date) ] # トレンド成分 x = [18627 + i - 364 for i in range(365)] trend_y = [np.log(i + 1) / 3.0 for i, di in enumerate(date)] weekly_y = [np.cos(di.weekday()) for i, di in enumerate(date)] seasonal_y = [di.month % 2 / 2 for i, di in enumerate(date)] noise_y = [np.random.rand() / 10 for i in range(365)] df = pd.DataFrame({\u0026#34;ds\u0026#34;: date, \u0026#34;y\u0026#34;: y}) df.index = date # 実験に使用するデータ plt.title(\u0026#34;サンプルデータ\u0026#34;) sns.lineplot(data=df) plt.show() growthパラメータ # growth=\u0026ldquo;linear\u0026quot;とした場合の予測 # # モデルを訓練 m = Prophet( yearly_seasonality=False, weekly_seasonality=True, daily_seasonality=False, growth=\u0026#34;linear\u0026#34;, ) m.fit(df) # 将来を予測 future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig = m.plot(forecast) plt.axvspan(18627, 18627 + 90, color=\u0026#34;coral\u0026#34;, alpha=0.4, label=\u0026#34;予測期間\u0026#34;) plt.legend() plt.show() Initial log joint probability = -16.6231\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 81 792.152 0.00138864 97.2266 2.443e-05 0.001 145 LS failed, Hessian reset 99 792.228 7.40168e-07 55.8404 0.03797 0.03797 168 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 138 792.388 3.51776e-05 59.7965 6.64e-07 0.001 263 LS failed, Hessian reset 199 792.62 3.0252e-05 72.8985 0.4243 1 340 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 247 792.799 0.000165183 55.2724 3.174e-06 0.001 440 LS failed, Hessian reset 299 792.972 0.000162358 62.7075 1 1 508 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 347 792.991 3.19534e-05 38.7196 4.536e-07 0.001 634 LS failed, Hessian reset 392 792.994 7.68689e-07 61.3331 1.015e-08 0.001 733 LS failed, Hessian reset 399 792.994 9.628e-08 55.3164 0.2134 1 742 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 404 792.994 3.74603e-08 39.3307 0.2405 0.7727 749 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\rgrowth=\u0026ldquo;linear\u0026quot;で表現できるトレンド成分 # growth=\u0026ldquo;linear\u0026quot;でどのようなトレンドを表現できるか確認してみます。 linear_trend はprophetで growth=\u0026quot;linear\u0026quot;　を指定した時にトレンド作成に使用するコードを再現してみます。\n※参考文献のコードと記号を合わせました、ミスがある場合はお手数ですがissueに指摘いただけると大変助かります\nprophet/R/inst/stan/prophet.stan\n$A$の各行の次元数は変化点の数です。また、各次元の値は $ a_j(t) \\left{ \\begin{array}{ll} 1, \u0026amp; \\text{if}　\\ t \\ge s_j \\ 0, \u0026amp; \\text{otherwise} \\end{array} \\right. $ となっています。時刻$t$の$a$の$j$次元目は、$t$が$j$番目の変化点よりあとの時刻ならば$1$になる、ということです。\ndef linear_trend( k: float, m: float, delta: np.array, t: np.array, t_change: list ) -\u0026gt; np.array: \u0026#34;\u0026#34;\u0026#34;線形トレンドを作成する Args: k (float): 係数 m (float): 係数 delta (np.array): delta t (np.array): タイムスタンプ t_change (list): トレンド変化点 Returns: np.array: トレンド \u0026#34;\u0026#34;\u0026#34; A = np.vstack([np.where(t_change \u0026lt; ti, 1, 0) for ti in t]) return (k + (A * delta).sum(axis=1)) * t + ( m + (A * (-t_change * delta)).sum(axis=1) ) linear_trendで表現できるトレンドを可視化する # # 適当にトレンドのような線を作成するため、変化点の数と各区間での線の傾きを指定します change_point_num = 5 t = np.linspace(-10, 10, 100) t_change = np.linspace(-10, 10, change_point_num + 1) delta = np.array([np.random.rand() - 0.5 for _ in range(change_point_num + 1)]) # linear_trendで線を引く trend_y = linear_trend(0.1, 0, delta, t, t_change) # できた線をプロットしてみる [plt.axvline(x=tci, color=\u0026#34;red\u0026#34;, alpha=0.5) for tci in t_change] # 変化点をプロット plt.plot(t, trend_y) plt.show() "},{"id":72,"href":"/basic/ensemble/adaboost_classification/","title":"Adaboost(分類)","section":"アンサンブル","content":"\rimport matplotlib.pyplot as plt import japanize_matplotlib import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier 実験用のデータを作成 # # 特徴が20あるデータを作成 n_features = 20 X, y = make_classification( n_samples=2500, n_features=n_features, n_informative=10, n_classes=2, n_redundant=4, n_clusters_per_class=5, ) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42 ) Adaboostモデルを訓練 # sklearn.ensemble.StackingClassifier\nab_clf = AdaBoostClassifier( n_estimators=10, learning_rate=1.0, random_state=117117, base_estimator=DecisionTreeClassifier(max_depth=2), ) ab_clf.fit(X_train, y_train) y_pred = ab_clf.predict(X_test) ab_clf_score = roc_auc_score(y_test, y_pred) ab_clf_score 0.7546477034876885\rlearning-rateの影響 # learning-rateが小さければ小さいほど重みの更新幅は小さくなります。逆に大きすぎると、収束しない場合があります。\nscores = [] learning_rate_list = np.linspace(0.01, 1, 100) for lr in learning_rate_list: ab_clf_i = AdaBoostClassifier( n_estimators=10, learning_rate=lr, random_state=117117, base_estimator=DecisionTreeClassifier(max_depth=2), ) ab_clf_i.fit(X_train, y_train) y_pred = ab_clf_i.predict(X_test) scores.append(roc_auc_score(y_test, y_pred)) plt.figure(figsize=(5, 5)) plt.plot(learning_rate_list, scores) plt.xlabel(\u0026#34;learning rate\u0026#34;) plt.ylabel(\u0026#34;ROC-AUC\u0026#34;) plt.grid() plt.show() n_estimatorsの影響 # n_estimatorsは弱学習器の数を指定しています。 通常は、このパラメタを大きくしたり小さくしたりする必要はないです。 ある程度大きな数でn_estimatorsを固定して、そのあとで他のパラメタを調整します。\nscores = [] n_estimators_list = [int(ne) for ne in np.linspace(5, 70, 40)] for n_estimators in n_estimators_list: ab_clf_i = AdaBoostClassifier( n_estimators=int(n_estimators), learning_rate=0.6, random_state=117117, base_estimator=DecisionTreeClassifier(max_depth=2), ) ab_clf_i.fit(X_train, y_train) y_pred = ab_clf_i.predict(X_test) scores.append(roc_auc_score(y_test, y_pred)) plt.figure(figsize=(5, 5)) plt.plot(n_estimators_list, scores) plt.xlabel(\u0026#34;n_estimators\u0026#34;) plt.ylabel(\u0026#34;ROC-AUC\u0026#34;) plt.grid() plt.show() base-estimatorの影響 # base-estimatorは弱学習器として何を使用するか指定します。つまり、Adaboostで最も重要なパラメタの一つです。\nscores = [] base_estimator_list = [ DecisionTreeClassifier(max_depth=md) for md in [2, 3, 4, 5, 6, 7, 8, 9, 10] ] for base_estimator in base_estimator_list: ab_clf_i = AdaBoostClassifier( n_estimators=10, learning_rate=0.5, random_state=117117, base_estimator=base_estimator, ) ab_clf_i.fit(X_train, y_train) y_pred = ab_clf_i.predict(X_test) scores.append(roc_auc_score(y_test, y_pred)) plt.figure(figsize=(5, 5)) plt_index = [i for i in range(len(base_estimator_list))] plt.bar(plt_index, scores) plt.xticks(plt_index, [str(bm) for bm in base_estimator_list], rotation=90) plt.xlabel(\u0026#34;base_estimator\u0026#34;) plt.ylabel(\u0026#34;ROC-AUC\u0026#34;) plt.show() Adaboostのデータの重みの可視化 # 分類がしにくいデータに対して重みを割り当てる様子を可視化します。\n# NOTE: モデルに渡されるsample_weightを確認するために作成したモデルです # このDummyClassifierがAdaboostのパラメタを変更することはありません class DummyClassifier: def __init__(self): self.model = DecisionTreeClassifier(max_depth=3) self.n_classes_ = 2 self.classes_ = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;] self.sample_weight = None ## sample_weight def fit(self, X, y, sample_weight=None): self.sample_weight = sample_weight self.model.fit(X, y, sample_weight=sample_weight) return self.model def predict(self, X, check_input=True): proba = self.model.predict(X) return proba def get_params(self, deep=False): return {} def set_params(self, deep=False): return {} n_samples = 500 X_2, y_2 = make_classification( n_samples=n_samples, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=117, n_clusters_per_class=2, ) plt.figure( figsize=( 7, 7, ) ) plt.title(f\u0026#34;サンプルデータの散布図\u0026#34;) plt.scatter(X_2[:, 0], X_2[:, 1], c=y_2) plt.show() ブースティングが進んだ後の重み # より重みがあるデータほど大きな円で表現されます。\nclf = AdaBoostClassifier( n_estimators=4, random_state=0, algorithm=\u0026#34;SAMME\u0026#34;, base_estimator=DummyClassifier() ) clf.fit(X_2, y_2) for i, estimators_i in enumerate(clf.estimators_): plt.figure( figsize=( 7, 7, ) ) plt.title(f\u0026#34;{i}番目の重み付き標本の可視化\u0026#34;) plt.scatter( X_2[:, 0], X_2[:, 1], marker=\u0026#34;o\u0026#34;, c=y_2, alpha=0.4, s=estimators_i.sample_weight * n_samples ** 1.65, ) plt.show() "},{"id":73,"href":"/prep/categorical/countencoder/","title":"Count Encoder","section":"カテゴリカルデータ","content":" サンプルデータ # 「人口総数」を予測したいとして、「元号」をエンコードしたいとします。\nimport pandas as pd X = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;) TARGET_NAME = \u0026#34;人口総数\u0026#34; FEATURE_NAME = \u0026#34;元号\u0026#34; X.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rCountEncoder # category_encoders.count.CountEncoder\nfrom category_encoders.count import CountEncoder from sklearn.compose import make_column_transformer c_ce = CountEncoder() y = X[TARGET_NAME] X[f\u0026#34;{FEATURE_NAME}_ce\u0026#34;] = c_ce.fit_transform(X[FEATURE_NAME], y) 結果を確認する # カテゴリ変数の列が CountEncoder でエンコードされていることを確認します。\nX[[FEATURE_NAME, f\u0026#34;{FEATURE_NAME}_ce\u0026#34;]] .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r元号の出現回数を確認する # X[FEATURE_NAME].value_counts() 昭和 584\r平成 300\r大正 96\rName: 元号, dtype: int64\r"},{"id":74,"href":"/webapp/flask/","title":"Flask","section":"アプリケーション作成","content":" Flask # アプリの作り方 # 後ほど追記する予定です。\n"},{"id":75,"href":"/timeseries/exponential_smoothing/003-haw-es/","title":"Holt-Wintersの手法","section":"指数平滑化","content":"import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from statsmodels.tsa.holtwinters import ExponentialSmoothing from statsmodels.tsa.holtwinters import Holt from statsmodels.tsa.holtwinters import SimpleExpSmoothing サンプルデータ # Natural Gas Consumption (NATURALGAS)のデータを使用しています。数値の単位はBCF（10億立方フィート）です。\ndata = { \u0026#34;value\u0026#34;: { \u0026#34;2013-01-01\u0026#34;: 2878.8, \u0026#34;2013-02-01\u0026#34;: 2567.2, \u0026#34;2013-03-01\u0026#34;: 2521.1, \u0026#34;2013-04-01\u0026#34;: 1967.5, \u0026#34;2013-05-01\u0026#34;: 1752.5, \u0026#34;2013-06-01\u0026#34;: 1742.9, \u0026#34;2013-07-01\u0026#34;: 1926.3, \u0026#34;2013-08-01\u0026#34;: 1927.4, \u0026#34;2013-09-01\u0026#34;: 1767.0, \u0026#34;2013-10-01\u0026#34;: 1866.8, \u0026#34;2013-11-01\u0026#34;: 2316.9, \u0026#34;2013-12-01\u0026#34;: 2920.8, \u0026#34;2014-01-01\u0026#34;: 3204.1, \u0026#34;2014-02-01\u0026#34;: 2741.2, \u0026#34;2014-03-01\u0026#34;: 2557.9, \u0026#34;2014-04-01\u0026#34;: 1961.7, \u0026#34;2014-05-01\u0026#34;: 1810.2, \u0026#34;2014-06-01\u0026#34;: 1745.4, \u0026#34;2014-07-01\u0026#34;: 1881.0, \u0026#34;2014-08-01\u0026#34;: 1933.1, \u0026#34;2014-09-01\u0026#34;: 1809.3, \u0026#34;2014-10-01\u0026#34;: 1912.8, \u0026#34;2014-11-01\u0026#34;: 2357.5, \u0026#34;2014-12-01\u0026#34;: 2679.2, \u0026#34;2015-01-01\u0026#34;: 3115.0, \u0026#34;2015-02-01\u0026#34;: 2925.2, \u0026#34;2015-03-01\u0026#34;: 2591.3, \u0026#34;2015-04-01\u0026#34;: 2007.9, \u0026#34;2015-05-01\u0026#34;: 1858.2, \u0026#34;2015-06-01\u0026#34;: 1899.9, \u0026#34;2015-07-01\u0026#34;: 2067.7, \u0026#34;2015-08-01\u0026#34;: 2052.7, \u0026#34;2015-09-01\u0026#34;: 1901.3, \u0026#34;2015-10-01\u0026#34;: 1987.3, \u0026#34;2015-11-01\u0026#34;: 2249.1, \u0026#34;2015-12-01\u0026#34;: 2588.2, \u0026#34;2016-01-01\u0026#34;: 3091.7, \u0026#34;2016-02-01\u0026#34;: 2652.3, \u0026#34;2016-03-01\u0026#34;: 2356.3, \u0026#34;2016-04-01\u0026#34;: 2083.9, \u0026#34;2016-05-01\u0026#34;: 1965.8, \u0026#34;2016-06-01\u0026#34;: 2000.7, \u0026#34;2016-07-01\u0026#34;: 2186.6, \u0026#34;2016-08-01\u0026#34;: 2208.4, \u0026#34;2016-09-01\u0026#34;: 1947.8, \u0026#34;2016-10-01\u0026#34;: 1925.2, \u0026#34;2016-11-01\u0026#34;: 2159.5, \u0026#34;2016-12-01\u0026#34;: 2866.3, \u0026#34;2017-01-01\u0026#34;: 2913.8, \u0026#34;2017-02-01\u0026#34;: 2340.2, \u0026#34;2017-03-01\u0026#34;: 2523.3, \u0026#34;2017-04-01\u0026#34;: 1932.0, \u0026#34;2017-05-01\u0026#34;: 1892.0, \u0026#34;2017-06-01\u0026#34;: 1910.4, \u0026#34;2017-07-01\u0026#34;: 2141.6, \u0026#34;2017-08-01\u0026#34;: 2093.8, \u0026#34;2017-09-01\u0026#34;: 1920.5, \u0026#34;2017-10-01\u0026#34;: 2031.5, \u0026#34;2017-11-01\u0026#34;: 2357.3, \u0026#34;2017-12-01\u0026#34;: 3086.0, \u0026#34;2018-01-01\u0026#34;: 3340.9, \u0026#34;2018-02-01\u0026#34;: 2710.7, \u0026#34;2018-03-01\u0026#34;: 2796.7, \u0026#34;2018-04-01\u0026#34;: 2350.5, \u0026#34;2018-05-01\u0026#34;: 2055.0, \u0026#34;2018-06-01\u0026#34;: 2063.1, \u0026#34;2018-07-01\u0026#34;: 2350.7, \u0026#34;2018-08-01\u0026#34;: 2313.8, \u0026#34;2018-09-01\u0026#34;: 2156.1, \u0026#34;2018-10-01\u0026#34;: 2285.9, \u0026#34;2018-11-01\u0026#34;: 2715.9, \u0026#34;2018-12-01\u0026#34;: 2999.5, \u0026#34;2019-01-01\u0026#34;: 3424.3, \u0026#34;2019-02-01\u0026#34;: 3019.1, \u0026#34;2019-03-01\u0026#34;: 2927.8, \u0026#34;2019-04-01\u0026#34;: 2212.4, \u0026#34;2019-05-01\u0026#34;: 2134.0, \u0026#34;2019-06-01\u0026#34;: 2119.3, \u0026#34;2019-07-01\u0026#34;: 2393.9, \u0026#34;2019-08-01\u0026#34;: 2433.9, \u0026#34;2019-09-01\u0026#34;: 2206.3, \u0026#34;2019-10-01\u0026#34;: 2306.5, \u0026#34;2019-11-01\u0026#34;: 2783.8, \u0026#34;2019-12-01\u0026#34;: 3170.7, \u0026#34;2020-01-01\u0026#34;: 3320.6, \u0026#34;2020-02-01\u0026#34;: 3058.5, \u0026#34;2020-03-01\u0026#34;: 2722.0, \u0026#34;2020-04-01\u0026#34;: 2256.9, \u0026#34;2020-05-01\u0026#34;: 2072.2, \u0026#34;2020-06-01\u0026#34;: 2127.9, \u0026#34;2020-07-01\u0026#34;: 2464.1, \u0026#34;2020-08-01\u0026#34;: 2399.5, \u0026#34;2020-09-01\u0026#34;: 2151.2, \u0026#34;2020-10-01\u0026#34;: 2315.9, \u0026#34;2020-11-01\u0026#34;: 2442.0, \u0026#34;2020-12-01\u0026#34;: 3182.8, \u0026#34;2021-01-01\u0026#34;: 3343.9, \u0026#34;2021-02-01\u0026#34;: 3099.2, \u0026#34;2021-03-01\u0026#34;: 2649.4, \u0026#34;2021-04-01\u0026#34;: 2265.1, \u0026#34;2021-05-01\u0026#34;: 2117.4, \u0026#34;2021-06-01\u0026#34;: 2238.4, \u0026#34;2021-07-01\u0026#34;: 2412.2, \u0026#34;2021-08-01\u0026#34;: 2433.8, \u0026#34;2021-09-01\u0026#34;: 2142.3, \u0026#34;2021-10-01\u0026#34;: 2262.6, \u0026#34;2021-11-01\u0026#34;: 2693.3, \u0026#34;2021-12-01\u0026#34;: 3007.3, \u0026#34;2022-01-01\u0026#34;: 3612.1, \u0026#34;2022-02-01\u0026#34;: 3064.2, \u0026#34;2022-03-01\u0026#34;: 2785.4, \u0026#34;2022-04-01\u0026#34;: 2379.3, \u0026#34;2022-05-01\u0026#34;: 2247.8, \u0026#34;2022-06-01\u0026#34;: 2326.9, \u0026#34;2022-07-01\u0026#34;: 2597.9, \u0026#34;2022-08-01\u0026#34;: 2566.1, \u0026#34;2022-09-01\u0026#34;: 2263.3, } } data = pd.DataFrame(data) data.rename(columns={\u0026#34;value\u0026#34;: \u0026#34;Natural Gas Consumption(BCF)\u0026#34;}, inplace=True) data.index = pd.to_datetime(data.index) data = data.asfreq(\u0026#34;MS\u0026#34;) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data) plt.grid() plt.show() データの分割 # データの末尾（2020年以降のデータ）を予測性能の検証に使用するためにデータを分割します。\ndata_train = data[data.index \u0026lt; \u0026#34;2020-1-1\u0026#34;] data_test = data[data.index \u0026gt;= \u0026#34;2020-1-1\u0026#34;] plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.legend() plt.grid() モデルの学習と予測 # statsmodels.tsa.holtwinters.ExponentialSmoothingを確認しつつ、パラメータを指定します。\nses = SimpleExpSmoothing(data_train) ses = ses.fit(smoothing_level=0.1) ses_pred = ses.forecast(33) holt = Holt(data_train) holt = holt.fit(smoothing_level=0.1, smoothing_trend=0.2) holt_pred = holt.forecast(33) hw = ExponentialSmoothing( data_train, trend=\u0026#34;additive\u0026#34;, seasonal=\u0026#34;add\u0026#34;, seasonal_periods=12 ) hw = hw.fit() hw_pred = hw.forecast(33) plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.plot(ses_pred.index, ses_pred.values, \u0026#34;-.\u0026#34;, label=\u0026#34;prediction(ses)\u0026#34;) plt.plot(holt_pred.index, holt_pred.values, \u0026#34;-.\u0026#34;, label=\u0026#34;prediction(Holt)\u0026#34;) plt.plot(hw_pred.index, hw_pred.values, \u0026#34;-o\u0026#34;, label=\u0026#34;prediction(Holt-Winters)\u0026#34;) plt.legend() plt.grid() "},{"id":76,"href":"/basic/dimensionality_reduction/lda/","title":"LDA","section":"次元削減","content":"import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_blobs 実験用のデータ # sklearn.datasets.make_blobs\nX, y = make_blobs( n_samples=600, n_features=3, random_state=11711, cluster_std=4, centers=3 ) fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\u0026#34;o\u0026#34;, c=y) ax.set_xlabel(\u0026#34;$x_1$\u0026#34;) ax.set_ylabel(\u0026#34;$x_2$\u0026#34;) ax.set_zlabel(\u0026#34;$x_3$\u0026#34;) LDAで二次元に次元削減する # sklearn.discriminant_analysis.LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA lda = LDA(n_components=2).fit(X, y) X_lda = lda.transform(X) fig = plt.figure(figsize=(8, 8)) plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, alpha=0.5) PCAとLDAの比較 # from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler pca = PCA(n_components=2) X_pca = pca.fit_transform(X) fig = plt.figure(figsize=(8, 8)) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, alpha=0.5) "},{"id":77,"href":"/finance/main/002_rsi/","title":"RSI","section":"時系列データ","content":" import numpy as np import pandas as pd import mplfinance as mpf from utils import get_finance_data # 銘柄名、期間、保存先ファイル ticker_symbol = \u0026#34;NVDA\u0026#34; start = \u0026#34;2021-05-01\u0026#34; end = \u0026#34;2021-06-30\u0026#34; # データを取得する df = get_finance_data(ticker_symbol, start=start, end=end, savedir=\u0026#34;../data\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rRSIを求める # RSIには様々な計算方法があり[1]、ここではRelative Strength Index (RSI)で示されている方法で計算してみます。\npandas.Series.diff\ndef get_rsi(close_prices: pd.Series, n=14): \u0026#34;\u0026#34;\u0026#34;RSI(相対力指数)を計算する RS＝（n日間の終値の上昇幅の平均）÷（n日間の終値の下落幅の平均） RSI= 100　-　（100　÷　（RS+1）） 参考文献： - https://info.monex.co.jp/technical-analysis/indicators/005.html - https://www.investopedia.com/terms/r/rsi.asp \u0026lt;-- 以下のコードの記号はこのページのものを使用 Args: close_price (pd.Series): 終値の系列 days (str): n日間, optional, default is 14. Returns: rsi(pd.Series): RSI \u0026#34;\u0026#34;\u0026#34; close_prices_diff = close_prices.diff(periods=1)[1:] fist_n_days_diff = close_prices_diff[: n + 1] previous_average_gain, previous_average_loss = 0, 0 rsi = np.zeros_like(close_prices) for i in range(len(close_prices)): if i \u0026lt; n: previous_average_gain = fist_n_days_diff[fist_n_days_diff \u0026gt;= 0].sum() / n previous_average_loss = -fist_n_days_diff[fist_n_days_diff \u0026lt; 0].sum() / n rsi[i] = 100.0 - 100.0 / (1 + previous_average_gain / previous_average_loss) else: if (cpd_i := close_prices_diff[i - 1]) \u0026gt; 0: current_gain = cpd_i current_loss = 0.0 else: current_gain = 0.0 current_loss = -cpd_i previous_average_gain = (previous_average_gain * (n - 1) + current_gain) / n previous_average_loss = (previous_average_loss * (n - 1) + current_loss) / n rsi[i] = 100.0 - 100.0 / (1 + previous_average_gain / previous_average_loss) return rsi OHLC チャートと並べてプロットする # df[\u0026#34;rsi\u0026#34;] = get_rsi(df[\u0026#34;Close\u0026#34;], n=7) # RSIのグラフを追加する apd = mpf.make_addplot( df[\u0026#34;rsi\u0026#34;], panel=2, color=\u0026#34;#000\u0026#34;, ylim=(0, 100), secondary_y=True, width=0.8 ) # グラフを作成 fig, axes = mpf.plot( df, type=\u0026#34;candle\u0026#34;, style=\u0026#34;yahoo\u0026#34;, volume=True, mav=[5, 25, 75], addplot=apd, panel_ratios=(1, 0.6), datetime_format=\u0026#34;%Y/%m/%d\u0026#34;, returnfig=True, figscale=1.2, ) fig.legend( [f\u0026#34;{days} days\u0026#34; for days in [5, 25, 75]], bbox_to_anchor=(0.0, 0.78, 0.305, 0.102) ) \u0026lt;matplotlib.legend.Legend at 0x7fdeaf404130\u0026gt;\r条件を満たした範囲を塗りつぶす # 「RSIが●×以上になった」「価格が●×を超えた」など、条件が満たされた期間を塗りつぶしてわかりやすくしたいです。 ここでは、80を超えた区間を塗りつぶしてみます。\n塗りつぶし範囲の区間のリストを作る # cond_true_spans = [] span = None is_true_span = False for i, rsi_i in enumerate(df[\u0026#34;rsi\u0026#34;]): if rsi_i \u0026gt; 80 and not is_true_span: is_true_span = True span = [i, 0] elif rsi_i \u0026lt; 80 and is_true_span: is_true_span = False span[1] = i - 1 cond_true_spans.append(span) else: pass グラフの塗りつぶしをする # matplotlib.pyplot.axvspan\napd = mpf.make_addplot( df[\u0026#34;rsi\u0026#34;], panel=2, color=\u0026#34;#000\u0026#34;, ylim=(0, 100), secondary_y=True, width=0.8 ) fig, axes = mpf.plot( df, type=\u0026#34;candle\u0026#34;, style=\u0026#34;yahoo\u0026#34;, volume=True, mav=[5, 25, 75], addplot=apd, panel_ratios=(1, 0.6), datetime_format=\u0026#34;%Y/%m/%d\u0026#34;, returnfig=True, figscale=1.2, ) fig.legend( [f\u0026#34;{days} days\u0026#34; for days in [5, 25, 75]], bbox_to_anchor=(0.0, 0.78, 0.305, 0.102) ) for span in cond_true_spans: axes[0].axvspan(span[0], span[1], color=\u0026#34;red\u0026#34;, alpha=0.2) axes[-1].axvspan(span[0], span[1], color=\u0026#34;red\u0026#34;, alpha=0.2) "},{"id":78,"href":"/timeseries/preprocess/003-seasonal-decompose/","title":"STL分解","section":"プロットと前処理","content":"\rimport japanize_matplotlib import matplotlib.pyplot as plt import numpy as np from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.seasonal import STL サンプルデータを作成 # 周期的な数値を複数組合せ、さらに区分的にトレンドが変化しています。また np.random.rand() でノイズも乗せています。\ndate_list = pd.date_range(\u0026#34;2021-01-01\u0026#34;, periods=365, freq=\u0026#34;D\u0026#34;) value_list = [ 10 + i % 14 + np.log(1 + i % 28 + np.random.rand()) + np.sqrt(1 + i % 7 + np.random.rand()) * 2 + (((i - 100) / 10)) * (i \u0026gt; 100) - ((i - 200) / 7) * (i \u0026gt; 200) + np.random.rand() for i, di in enumerate(date_list) ] df = pd.DataFrame( { \u0026#34;日付\u0026#34;: date_list, \u0026#34;観測値\u0026#34;: value_list, } ) df.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;観測値\u0026#34;]) plt.grid(axis=\u0026#34;x\u0026#34;) plt.show() トレンド・周期性・残差に分解する # statsmodels.tsa.seasonal.STLを用いて時系列データを\nトレンド(.trend) 季節/周期性(.seasonal) 残差(.resid) に分解してみます。（以下のコード中のdrはDecomposeResultを指しています。）\n参考文献：Cleveland, Robert B., et al. \u0026ldquo;STL: A seasonal-trend decomposition.\u0026rdquo; J. Off. Stat 6.1 (1990): 3-73. (pdf)\nperiod = 28 stl = STL(df[\u0026#34;観測値\u0026#34;], period=period) dr = stl.fit() _, axes = plt.subplots(figsize=(12, 8), ncols=1, nrows=4, sharex=True) axes[0].set_title(\u0026#34;観測値\u0026#34;) axes[0].plot(dr.observed) axes[0].grid() axes[1].set_title(\u0026#34;トレンド\u0026#34;) axes[1].plot(dr.trend) axes[1].grid() axes[2].set_title(\u0026#34;季節性\u0026#34;) axes[2].plot(dr.seasonal) axes[2].grid() axes[3].set_title(\u0026#34;その他の要因・残差\u0026#34;) axes[3].plot(dr.resid) axes[3].grid() plt.tight_layout() plt.show() 外れ値に強いか確認 # def check_outlier(): period = 28 df_outlier = df[\u0026#34;観測値\u0026#34;].copy() for i in range(1, 6): df_outlier[i * 50] = df_outlier[i * 50] * 1.4 stl = STL(df_outlier, period=period, trend=31) dr_outlier = stl.fit() _, axes = plt.subplots(figsize=(12, 8), ncols=1, nrows=4, sharex=True) axes[0].set_title(\u0026#34;観測値\u0026#34;) axes[0].plot(dr_outlier.observed) axes[0].grid() axes[1].set_title(\u0026#34;トレンド\u0026#34;) axes[1].plot(dr_outlier.trend) axes[1].grid() axes[2].set_title(\u0026#34;季節性\u0026#34;) axes[2].plot(dr_outlier.seasonal) axes[2].grid() axes[3].set_title(\u0026#34;その他の要因・残差\u0026#34;) axes[3].plot(dr_outlier.resid) axes[3].grid() check_outlier() トレンドを捉えることができているか確認する # もしもトレンドが正しく抽出できている場合は、トレンドに周期的な要素が含まれていないはずであり、PACF（偏自己相関）が０に近くなるはずです。\nstatsmodels.tsa.stattools.acf statsmodels.tsa.stattools.pacf from statsmodels.graphics.tsaplots import plot_acf, plot_pacf _, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) plt.suptitle(\u0026#34;トレンドの自己相関・偏自己相関\u0026#34;) plot_acf(dr.trend.dropna(), ax=axes[0]) plot_pacf(dr.trend.dropna(), method=\u0026#34;ywm\u0026#34;, ax=axes[1]) plt.tight_layout() plt.show() _, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) plt.suptitle(\u0026#34;残差の自己相関・偏自己相関\u0026#34;) plot_acf(dr.resid.dropna(), ax=axes[0]) plot_pacf(dr.resid.dropna(), method=\u0026#34;ywm\u0026#34;, ax=axes[1]) plt.tight_layout() plt.show() 残差に対するかばん検定 # from statsmodels.stats.diagnostic import acorr_ljungbox acorr_ljungbox(dr.resid.dropna(), lags=int(np.log(df.shape[0]))) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rトレンドを近似する # 区分的な線形関数で近似してみます。 コードはruoyu0088/segments_fit.ipynbのものを使用しています。\nplt.figure(figsize=(12, 4)) trend_data = dr.trend.dropna() plt.plot(trend_data) plt.show() 区分線形関数 # ruoyu0088/segments_fit.ipynbのコードを元にしています。\nnumpy.percentile scipy.special.huber ruoyu0088/segments_fit.ipynb from scipy import optimize from scipy.special import huber def segments_fit(X, Y, count, is_use_huber=True, delta=0.5): \u0026#34;\u0026#34;\u0026#34; original: https://gist.github.com/ruoyu0088/70effade57483355bbd18b31dc370f2a ruoyu0088氏作成の ruoyu0088/segments_fit.ipynb　より引用しています \u0026#34;\u0026#34;\u0026#34; xmin = X.min() xmax = X.max() seg = np.full(count - 1, (xmax - xmin) / count) px_init = np.r_[np.r_[xmin, seg].cumsum(), xmax] py_init = np.array( [Y[np.abs(X - x) \u0026lt; (xmax - xmin) * 0.01].mean() for x in px_init] ) def func(p): seg = p[: count - 1] py = p[count - 1 :] px = np.r_[np.r_[xmin, seg].cumsum(), xmax] return px, py def err(p): px, py = func(p) Y2 = np.interp(X, px, py) if is_use_huber: return np.mean(huber(delta, Y - Y2)) else: return np.mean((Y - Y2) ** 2) r = optimize.minimize(err, x0=np.r_[seg, py_init], method=\u0026#34;Nelder-Mead\u0026#34;) return func(r.x) x = np.arange(0, len(trend_data)) y = trend_data plt.figure(figsize=(12, 4)) px, py = segments_fit(x, y, 4) plt.plot(x, y, \u0026#34;-x\u0026#34;, label=\u0026#34;トレンド\u0026#34;) plt.plot(px, py, \u0026#34;-o\u0026#34;, label=\u0026#34;近似した線形関数\u0026#34;) plt.ylim(20, 40) plt.legend() 元データからトレンドを除去する # トレンドのみのデータを用意する # trend_data_first, trend_data_last = trend_data.iloc[0], trend_data.iloc[-1] for i in range(int(period / 2)): trend_data[trend_data.index.min() - 1] = trend_data_first trend_data[trend_data.index.max() + 1] = trend_data_last trend_data.sort_index() -14 23.728925\r-13 23.728925\r-12 23.728925\r-11 23.728925\r-10 23.728925\r... 374 26.206443\r375 26.206443\r376 26.206443\r377 26.206443\r378 26.206443\rName: trend, Length: 393, dtype: float64\rトレンドを取り除いた波形を確認する # plt.figure(figsize=(12, 4)) plt.title(\u0026#34;観測値\u0026#34;) plt.plot(dr.observed, label=\u0026#34;トレンドあり\u0026#34;) plt.plot(dr.observed - trend_data, label=\u0026#34;トレンド除去済み\u0026#34;) plt.grid() "},{"id":79,"href":"/timeseries/shape/004-ts-extract-features/","title":"tsfresh","section":"形状・類似度","content":"\rtsfresh — tsfresh 0.18.1.dev documentation\ntsfresh # Overview on extracted featuresを参考に、どんな特徴量が作成されるか確認してみます。\nimport os import numpy as np import pandas as pd import matplotlib.pyplot as plt from tsfresh import extract_features X = [] for id, it in enumerate(np.linspace(0.1, 100, 100)): for jt in range(10): X.append( [ id, jt, jt + np.sin(it), jt % 2 + np.cos(it), jt % 3 + np.tan(it), np.log(it + jt), ] ) X = pd.DataFrame(X) X.columns = [\u0026#34;id\u0026#34;, \u0026#34;time\u0026#34;, \u0026#34;fx1\u0026#34;, \u0026#34;fx2\u0026#34;, \u0026#34;fx3\u0026#34;, \u0026#34;fx4\u0026#34;] X.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rX[X[\u0026#34;id\u0026#34;] == 3].plot(subplots=True, sharex=True, figsize=(12, 10)) plt.show() 特徴量を計算する # tsfresh.feature_extraction package\nextract_featuresを使って、一気にすべての特徴量を計算することができます。また、tsfresh.feature_selection以下の関数を用いることで特徴選択を実行することもできます。\nextracted_features = extract_features(X, column_id=\u0026#34;id\u0026#34;, column_sort=\u0026#34;time\u0026#34;) extracted_features.head() Feature Extraction: 100%|█\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":80,"href":"/basic/clustering/x-means/","title":"X-means","section":"クラスタリング","content":"\rpyclustering.cluster.xmeans.xmeans Class Reference\nPelleg, Dan, and Andrew W. Moore. \u0026ldquo;X-means: Extending k-means with efficient estimation of the number of clusters.\u0026rdquo; Icml. Vol. 1. 2000.\nimport numpy as np import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.datasets import make_blobs k-meansでkをあらかじめ指定 # def plot_by_kmeans(X, k=5): y_pred = KMeans(n_clusters=k, random_state=random_state, init=\u0026#34;random\u0026#34;).fit_predict( X ) plt.scatter(X[:, 0], X[:, 1], c=y_pred, marker=\u0026#34;x\u0026#34;) plt.title(f\u0026#34;k-means, n_clusters={k}\u0026#34;) # サンプルデータを作成 n_samples = 1000 random_state = 117117 X, _ = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=1, centers=10 ) # k-means++を実行 plot_by_kmeans(X) x-meanでクラスタ数を指定せずに実行 # BAYESIAN_INFORMATION_CRITERION # from pyclustering.cluster.xmeans import xmeans from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer BAYESIAN_INFORMATION_CRITERION = 0 MINIMUM_NOISELESS_DESCRIPTION_LENGTH = 1 def plot_by_xmeans( X, c_min=3, c_max=10, criterion=BAYESIAN_INFORMATION_CRITERION, tolerance=0.025 ): initial_centers = kmeans_plusplus_initializer(X, c_min).initialize() xmeans_instance = xmeans( X, initial_centers, c_max, criterion=criterion, tolerance=tolerance ) xmeans_instance.process() # プロット用のデータを作成 clusters = xmeans_instance.get_clusters() n_samples = X.shape[0] c = [] for i, cluster_i in enumerate(clusters): X_ci = X[cluster_i] color_ci = [i for _ in cluster_i] plt.scatter(X_ci[:, 0], X_ci[:, 1], marker=\u0026#34;x\u0026#34;) plt.title(\u0026#34;x-means\u0026#34;) # x-meansを実行 plot_by_xmeans(X, c_min=3, c_max=10, criterion=BAYESIAN_INFORMATION_CRITERION) MINIMUM_NOISELESS_DESCRIPTION_LENGTH # plot_by_xmeans(X, c_min=3, c_max=10, criterion=MINIMUM_NOISELESS_DESCRIPTION_LENGTH) toleranceの影響 # X, _ = make_blobs( n_samples=2000, random_state=random_state, cluster_std=0.4, centers=10, ) plt.figure(figsize=(25, 5)) for i, ti in enumerate(np.linspace(0.0001, 1, 5)): ti = np.round(ti, 4) plt.subplot(1, 10, i + 1) plot_by_xmeans( X, c_min=3, c_max=10, criterion=BAYESIAN_INFORMATION_CRITERION, tolerance=ti ) plt.title(f\u0026#34;tol={ti}\u0026#34;) 色々なデータに対して k-means と x-means を比較する # for i in range(5): X, _ = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=0.7, centers=5 + i * 5, ) plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plot_by_kmeans(X) plt.subplot(1, 2, 2) plot_by_xmeans(X, c_min=3, c_max=20) plt.show() "},{"id":81,"href":"/prep/numerical/yeojonson/","title":"YeoJonson変換","section":"数値データ","content":" scipy.stats.yeojohnson\n数値を正規分布に近いかたちの分布に変換したい時の手法の一つ、box-cox変換と異なり負の値が含まれている場合でも対応できる。\nI. Yeo and R.A. Johnson, “A New Family of Power Transformations to Improve Normality or Symmetry”, Biometrika 87.4 (2000):\nfrom scipy import stats import matplotlib.pyplot as plt x = stats.loggamma.rvs(1, size=1000) - 0.5 plt.hist(x) plt.axvline(x=0, color=\u0026#34;r\u0026#34;) # 0以下にもデータがあることを確認する plt.show() import numpy as np from scipy.stats import yeojohnson plt.hist(yeojohnson(x)) plt.show() "},{"id":82,"href":"/prep/","title":"データ前処理","section":"トップページ","content":" Section 3 # データ前処理 # Wikipedia データ前処理\n"},{"id":83,"href":"/finance/visualize/003-radar-circle/","title":"レーダーチャート","section":"可視化","content":"レーダーチャートは複数の項目をまとめて比較する方法のひとつ。複数の項目がバランスよく高い値か、低い値かを確認するときに役に立つ。 すべての項目を「高ければ高いほど良い」か「低ければ低いほど良い」状態にすると比較しやすくなる。\nレーダーチャートは、複数の項目がある変量を構成比に直すことなく正多角形上に表現したグラフ。中心を0とし各項目ごとに値が大きいほど外側になるよう表現する。主に、それらの項目を属性としてもつ主体の性能などを比較するために用いる。（出典：Wikipedia）\nimport matplotlib.pyplot as plt import pandas as pd from math import pi df = pd.DataFrame( index=[\u0026#34;$AAA\u0026#34;, \u0026#34;$BBB\u0026#34;, \u0026#34;$CCC\u0026#34;], data={ \u0026#34;EPS\u0026#34;: [1, 2, 3], \u0026#34;Revenue\u0026#34;: [3, 3, 2], \u0026#34;Guidance\u0026#34;: [1, 2, 3], \u0026#34;D/E\u0026#34;: [3, 2, 1], \u0026#34;PER\u0026#34;: [1, 2, 3], \u0026#34;Dividend\u0026#34;: [2, 3, 3], }, ) レーダーチャートのプロット # matplotlib.projections set_theta_offset(offset) plt.figure(figsize=(7, 7)) ax = plt.subplot(111, polar=True) ax.set_theta_offset(pi / 2.0) ax.set_theta_direction(-1) # 各ラベルの位置調整 angles = [2 * n * pi / len(df.columns) for n in range(len((df.columns)))] plt.xticks(angles, df.columns, size=20) ax.set_rlabel_position(0) plt.yticks([1, 2, 3], [\u0026#34;★\u0026#34;, \u0026#34;★★\u0026#34;, \u0026#34;★★★\u0026#34;], color=\u0026#34;grey\u0026#34;, size=13) plt.ylim(0, 3.5) # 指定範囲の塗りつぶし for ticker_symbol in [\u0026#34;$AAA\u0026#34;, \u0026#34;$BBB\u0026#34;, \u0026#34;$CCC\u0026#34;]: values = df.loc[ticker_symbol].values.flatten().tolist() ax.plot( angles + [0], values + [values[0]], linewidth=1, linestyle=\u0026#34;solid\u0026#34;, c=\u0026#34;#000\u0026#34;, label=ticker_symbol, ) ax.fill(angles + [0], values + [values[0]], \u0026#34;#aaa\u0026#34;, alpha=0.2) plt.legend(bbox_to_anchor=(0.9, 1.1)) plt.show() "},{"id":84,"href":"/visualize/category-groupby/pie-dounut/","title":"ドーナツチャート","section":"カテゴリと数値","content":"ドーナツチャート（ドーナツグラフ）はカテゴリごとの比率を表示するために使用される円グラフの一種で、真ん中に空白があります。空白に特別な意味はありませんが、ここに全体の統計量（たとえば、『合計XXX円』など）を表示したりします。ドーナツチャートはmatplotlib.pyplot.pieを使ってpython上でドーナツチャートを作成します。\nimport matplotlib.pyplot as plt # プロット用のデータ percent = [40, 20, 20, 10, 10] explode = [0, 0, 0, 0, 0] labels = [\u0026#34;米国\u0026#34;, \u0026#34;エマージング\u0026#34;, \u0026#34;日本\u0026#34;, \u0026#34;欧州\u0026#34;, \u0026#34;その他\u0026#34;] percent.reverse() explode.reverse() labels.reverse() # 円グラフを作成 plt.figure(figsize=(7, 7)) plt.pie(x=percent, labels=labels, explode=explode, autopct=\u0026#34;%1.0f%%\u0026#34;, startangle=90) # 真ん中に空白の円を追加 background_color = \u0026#34;#fff\u0026#34; p = plt.gcf() p.gca().add_artist(plt.Circle((0, 0), 0.8, color=background_color)) plt.show() "},{"id":85,"href":"/prep/table/select_column_by_column_name/","title":"パターンマッチで列を選択","section":"テーブルデータ全般","content":" サンプルデータ # import pandas as pd X = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;) X.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rsklearn.compose.make_column_selector # sklearn.compose.make_column_selector を使用します。pattern=\u0026quot;暦\u0026quot;で暦が含まれる列を選択し、StandardScalerを適用します。\nfrom sklearn.preprocessing import StandardScaler from sklearn.compose import make_column_transformer from sklearn.compose import make_column_selector n_ss = StandardScaler() # 暦が含まれる列のみスケーリング ct = make_column_transformer( (n_ss, make_column_selector(pattern=\u0026#34;暦\u0026#34;)), sparse_threshold=0 ) X_transform = ct.fit_transform(X) # 変換後のテーブル pd.DataFrame(X_transform).head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":86,"href":"/visualize/distribution/ridgelineplot/","title":"リッジラインプロット","section":"数値の分布","content":"複数のグループの分布とその差異を視覚化するために使用されるチャート。分布のグラフを重ねて表示する（ことが多い）ので、グループごとのわずかな分布の違いや頂点の位置の違い・変化を可視化しやすい。\nridgeplot: beautiful ridgeline plots in Python\nimport numpy as np import seaborn as sns from ridgeplot import ridgeplot # 可視化したいカラムのリスト columns = [\u0026#34;sepal_length\u0026#34;, \u0026#34;sepal_width\u0026#34;, \u0026#34;petal_length\u0026#34;, \u0026#34;petal_width\u0026#34;] # サンプルデータ df = sns.load_dataset(\u0026#34;iris\u0026#34;) df = df[columns] # リッジラインプロット fig = ridgeplot( samples=df.values.T, labels=columns, colorscale=\u0026#34;viridis\u0026#34;, coloralpha=0.6 ) fig.update_layout(height=500, width=800) fig.show() !\n"},{"id":87,"href":"/finance/nlp/srt2jp/","title":"英語から日本語字幕データを作成","section":"テキスト処理","content":"\rVOICEVOX \u0026ndash; 無料で使える中品質なテキスト読み上げソフトウェア\nwhisperから作成した字幕ファイルを読み込む # whisperはデフォルトで .srtファイルを書き出します。このファイルの英語の部分を日本語に翻訳します。 翻訳の正しさは保証されておらず、また本コードも動作や出力に関しては一切の責任を負えません。\nopenai/whisper 翻訳に使用するモデル ニューラル機械翻訳モデルFuguMT staka/fugumt-en-ja - Hugging Face 音声データは『Flex LNG Q3 2022 Key Takeaways』の音声を使用しています。\nimport warnings from transformers import pipeline MAX_LENGTH = 400 # モデルに入力することができる文字数の上限 translator = pipeline(\u0026#34;translation\u0026#34;, model=\u0026#34;staka/fugumt-en-ja\u0026#34;) file = open(\u0026#34;FlexLNG_Q3_2022_Key_Takeaways.mp3.srt\u0026#34;, \u0026#34;r\u0026#34;) lines = [l.replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) for l in file.readlines()] lines[:15] [\u0026#39;1\u0026#39;,\r\u0026#39;00:00:00,000 --\u0026gt; 00:00:13,240\u0026#39;,\r\u0026#34;Hi and welcome to FlexLNG\u0026#39;s third quarter highlights.\u0026#34;,\r\u0026#39;\u0026#39;,\r\u0026#39;2\u0026#39;,\r\u0026#39;00:00:13,240 --\u0026gt; 00:00:18,120\u0026#39;,\r\u0026#39;Revenues for the quarter came in at 91 million in line with previous guidance of approximately\u0026#39;,\r\u0026#39;\u0026#39;,\r\u0026#39;3\u0026#39;,\r\u0026#39;00:00:18,120 --\u0026gt; 00:00:19,520\u0026#39;,\r\u0026#39;90 million.\u0026#39;,\r\u0026#39;\u0026#39;,\r\u0026#39;4\u0026#39;,\r\u0026#39;00:00:19,520 --\u0026gt; 00:00:26,020\u0026#39;,\r\u0026#39;Earnings were strong, net income and adjusted net income was 47 and 42 million, translating\u0026#39;] 前処理 # テキストに前処理を加えたいので、ファイルを保存する前にテキストを書き換えます。\ndef zundamonize(text: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; 語尾をそれらしくする Args: text (str): 日本語テキスト Returns: str: 前処理済みの日本語テキスト \u0026#34;\u0026#34;\u0026#34; # TODO: 辞書を使って英語をカタカナに変換する # https://github.com/KEINOS/google-ime-user-dictionary-ja-en など text = text.replace(\u0026#34;Flex\u0026#34;, \u0026#34;フレックス\u0026#34;) if text.endswith(\u0026#34;ありがとうございました\u0026#34;): return text.replace(\u0026#34;ありがとうございました\u0026#34;, \u0026#34;ありがとうなのだ。\u0026#34;) elif text.endswith(\u0026#34;しました。\u0026#34;): return text.replace(\u0026#34;しました。\u0026#34;, \u0026#34;したのだ。\u0026#34;) elif text.endswith(\u0026#34;れました。\u0026#34;): return text.replace(\u0026#34;れました。\u0026#34;, \u0026#34;れたのだ。\u0026#34;) elif text.endswith(\u0026#34;れました。\u0026#34;): return text.replace(\u0026#34;れました。\u0026#34;, \u0026#34;れたのだ。\u0026#34;) elif text.endswith(\u0026#34;れます。\u0026#34;): return text.replace(\u0026#34;れます。\u0026#34;, \u0026#34;れるのだ。\u0026#34;) elif text.endswith(\u0026#34;できます。\u0026#34;): return text.replace(\u0026#34;できます。\u0026#34;, \u0026#34;できるのだ。\u0026#34;) elif text.endswith(\u0026#34;あります。\u0026#34;): return text.replace(\u0026#34;あります。\u0026#34;, \u0026#34;あるのだ。\u0026#34;) elif text.endswith(\u0026#34;ようこそ。\u0026#34;): return text.replace(\u0026#34;ようこそ。\u0026#34;, \u0026#34;ようこそなのだ。\u0026#34;) elif text.endswith(\u0026#34;ています。\u0026#34;): return text.replace(\u0026#34;ています。\u0026#34;, \u0026#34;ているのだ。\u0026#34;) elif text.endswith(\u0026#34;ている。\u0026#34;): return text.replace(\u0026#34;ている。\u0026#34;, \u0026#34;ているのだ。\u0026#34;) elif text.endswith(\u0026#34;できる。\u0026#34;): return text.replace(\u0026#34;できる。\u0026#34;, \u0026#34;できるのだ。\u0026#34;) elif text.endswith(\u0026#34;できま。\u0026#34;): return text.replace(\u0026#34;できます。\u0026#34;, \u0026#34;できるのだ。\u0026#34;) elif text.endswith(\u0026#34;できた。\u0026#34;): return text.replace(\u0026#34;できた。\u0026#34;, \u0026#34;できたのだ。\u0026#34;) elif text.endswith(\u0026#34;えました。\u0026#34;): return text.replace(\u0026#34;えました。\u0026#34;, \u0026#34;えたのだ。\u0026#34;) elif text.endswith(\u0026#34;しました。\u0026#34;): return text.replace(\u0026#34;しました。\u0026#34;, \u0026#34;したのだ。\u0026#34;) elif text.endswith(\u0026#34;ました。\u0026#34;): return text.replace(\u0026#34;ました。\u0026#34;, \u0026#34;たのだ。\u0026#34;) elif text.endswith(\u0026#34;った。\u0026#34;): return text.replace(\u0026#34;った。\u0026#34;, \u0026#34;ったのだ。\u0026#34;) elif text.endswith(\u0026#34;した。\u0026#34;): return text.replace(\u0026#34;した。\u0026#34;, \u0026#34;したのだ。\u0026#34;) elif text.endswith(\u0026#34;する。\u0026#34;): return text.replace(\u0026#34;する。\u0026#34;, \u0026#34;するのだ。\u0026#34;) elif text.endswith(\u0026#34;です。\u0026#34;): return text.replace(\u0026#34;です。\u0026#34;, \u0026#34;なのだ。\u0026#34;) return text 翻訳したテキストを出力する # 字幕用のフォーマット(.srt)とテキストファイルの両方を出力します。 途中、タイムスタンプをいろいろ書き換えているのは字幕のタイミングを調整するためです。\nSubRip file format(.srtファイルの仕様) ファイル一行目に改行を入れるとAdobe Premiere Pro等で読み取りに失敗するので注意\n作成された .srt ファイルをAdobe Premiere Proに読み込むと字幕を表示できます。 また、.txtファイルをVOICEVOXに読み込めば音声データを作成できます。\ncnt = 0 result = [] text_only_result = [] temp_timestamp_start = \u0026#34;00:00:00,000\u0026#34; temp_timestamp_end = \u0026#34;00:00:00,000\u0026#34; temp_text = \u0026#34;\u0026#34; centense_continue = False for line in lines: if len(line) == 0: # 空白行はスキップ continue elif line[0].isdigit(): # 会話以外の出力はスキップ if \u0026#34;--\u0026gt;\u0026#34; in line: if centense_continue: temp_timestamp_end = line.split(\u0026#34; --\u0026gt; \u0026#34;)[1] else: temp_timestamp_start, temp_timestamp_end = line.split(\u0026#34; --\u0026gt; \u0026#34;) continue elif len(line) \u0026lt; 5: continue # 会話が続いているかの判定 if line.endswith(\u0026#34;.\u0026#34;): centense_continue = False else: centense_continue = True # 翻訳 temp_text += line if not centense_continue: cnt += 1 translation_text = translator(temp_text)[0][\u0026#34;translation_text\u0026#34;] translation_text = zundamonize(translation_text) temp_text = \u0026#34;\u0026#34; print(\u0026#34;\u0026#34;) print(cnt) print(f\u0026#34;{temp_timestamp_start} --\u0026gt; {temp_timestamp_end}\u0026#34;) print(translation_text) if cnt \u0026gt; 1: result.append(\u0026#34;\u0026#34;) result.append(cnt) result.append(f\u0026#34;{temp_timestamp_start} --\u0026gt; {temp_timestamp_end}\u0026#34;) result.append(translation_text) text_only_result.append(f\u0026#34;{translation_text}\u0026#34;) else: continue with open(\u0026#34;日本語字幕.srt\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8-sig\u0026#34;) as f: for line in result: f.write(f\u0026#34;{line}\\n\u0026#34;) with open(\u0026#34;日本語テキスト.txt\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8-sig\u0026#34;) as f: for line in text_only_result: f.write(f\u0026#34;{line}\\n\u0026#34;) 1\r00:00:00,000 --\u0026gt; 00:00:13,240\rフレックスLNGの第3四半期ハイライトへようこそなのだ。\r2\r00:00:13,240 --\u0026gt; 00:00:19,520\rこの四半期の売上は9100万ドルで、前回のガイダンスでは約9000万ドルだったのだ。\r3\r00:00:19,520 --\u0026gt; 00:00:33,480\r利益は好調で、純利益と調整済み純利益は47と4200万ドルで、1株当たり利益と調整済み1株当たり利益はそれぞれ88セントと79セントだったのだ。\r4\r00:00:33,480 --\u0026gt; 00:00:41,520\r四半期中の貨物市場は急成長し、これは短期と長期の両方にプラスの影響を与えたのだ。\r5\r00:00:41,520 --\u0026gt; 00:00:45,100\rこの四半期には3隻の船が新造船のチャーターを開始したのだ。\r6\r00:00:45,100 --\u0026gt; 00:00:57,720\r6月には、フレックス Enterpriseとフレックス Amberの両方の7年間のチャーターを発表し、これらの船は、短期契約の代わりに、7月にこれらの新しい長期チャーターを開始したのだ。 "},{"id":88,"href":"/basic/regression/robust_regression/","title":"外れ値と頑健性","section":"線形回帰","content":"\rimport japanize_matplotlib import matplotlib.pyplot as plt import numpy as np Huber損失の可視化 # def huber_loss(y_pred: float, y: float, delta=1.0): \u0026#34;\u0026#34;\u0026#34;HuberLoss\u0026#34;\u0026#34;\u0026#34; huber_1 = 0.5 * (y - y_pred) ** 2 huber_2 = delta * (np.abs(y - y_pred) - 0.5 * delta) return np.where(np.abs(y - y_pred) \u0026lt;= delta, huber_1, huber_2) delta = 1.5 x_vals = np.arange(-2, 2, 0.01) y_vals = np.where( np.abs(x_vals) \u0026lt;= delta, 0.5 * np.square(x_vals), delta * (np.abs(x_vals) - 0.5 * delta), ) # グラフをプロットする fig = plt.figure(figsize=(8, 8)) plt.plot(x_vals, x_vals ** 2, \u0026#34;red\u0026#34;, label=r\u0026#34;$(y-\\hat{y})^2$\u0026#34;) ## 二乗誤差 plt.plot(x_vals, np.abs(x_vals), \u0026#34;orange\u0026#34;, label=r\u0026#34;$|y-\\hat{y}|$\u0026#34;) ## 絶対誤差 plt.plot( x_vals, huber_loss(x_vals * 2, x_vals), \u0026#34;green\u0026#34;, label=r\u0026#34;huber-loss\u0026#34; ) # Huber-loss plt.axhline(y=0, color=\u0026#34;k\u0026#34;) plt.grid(True) plt.legend() plt.show() 最小二乗法との比較 # 実験用のデータを作成する # Huber損失による回帰とが通常の線形回帰を比較するために、一つだけ外れ値を用意します。\nN = 30 x1 = np.array([i for i in range(N)]) x2 = np.array([i for i in range(N)]) X = np.array([x1, x2]).T epsilon = np.array([np.random.random() for i in range(N)]) y = 5 * x1 + 10 * x2 + epsilon * 10 y[5] = 500 plt.figure(figsize=(8, 8)) plt.plot(x1, y, \u0026#34;ko\u0026#34;, label=\u0026#34;data\u0026#34;) plt.legend() plt.show() 最小二乗法とリッジ回帰を当てはめた場合と比較する # sklearn.linear_model.HuberRegressor\n前処理の方法：外れ値にラベルを付与①\nfrom sklearn.datasets import make_regression from sklearn.linear_model import HuberRegressor, Ridge from sklearn.linear_model import LinearRegression plt.figure(figsize=(8, 8)) huber = HuberRegressor(alpha=0.0, epsilon=3) huber.fit(X, y) plt.plot(x1, huber.predict(X), \u0026#34;green\u0026#34;, label=\u0026#34;huber回帰\u0026#34;) ridge = Ridge(alpha=0.0, random_state=0) ridge.fit(X, y) plt.plot(x1, ridge.predict(X), \u0026#34;orange\u0026#34;, label=\u0026#34;リッジ回帰\u0026#34;) lr = LinearRegression() lr.fit(X, y) plt.plot(x1, lr.predict(X), \u0026#34;r-\u0026#34;, label=\u0026#34;線形回帰\u0026#34;) plt.plot(x1, y, \u0026#34;x\u0026#34;) plt.plot(x1, y, \u0026#34;ko\u0026#34;, label=\u0026#34;data\u0026#34;) plt.legend() plt.show() "},{"id":89,"href":"/basic/tree/parameter/","title":"決定木のパラメータ","section":"決定木","content":"\rmax_depthは木の最大深さを指定します min_samples_splitは分岐を作成するために必要な最低データ数を指定します min_samples_leafは葉の作成に必要な最低データ数を指定します max_leaf_nodesは葉の枚数の上限を指定します ccp_alphaは木の複雑さを考慮した決定木の剪定をするためのパラメタ class_weightは分類においてクラスの重みづけを指定します sklearn.tree.DecisionTreeRegressor parrt/dtreeviz import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor from sklearn.datasets import make_regression from mpl_toolkits.mplot3d import Axes3D from dtreeviz.trees import dtreeviz, rtreeviz_bivar_3D シンプルなデータに決定木を当てはめてみる # # サンプルデータ X, y = make_regression(n_samples=100, n_features=2, random_state=11) # 決定木を学習 dt = DecisionTreeRegressor(max_depth=3) dt.fit(X, y) # 可視化 fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;MPG\u0026#34;, elev=40, azim=120, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() いろいろなパラメタの決定木を学習してみる # 少し複雑な構造を持つデータに対して、決定木のパラメタを変えた時にどのような挙動になるかを確認してみる。 はじめに、max_depth=3以外がすべてデフォルト値の決定木を確認する。\n# サンプルデータ X, y = make_regression( n_samples=500, n_features=2, effective_rank=4, noise=0.1, random_state=1 ) plt.figure(figsize=(10, 10)) plt.scatter(X[:, 0], X[:, 1], c=y) plt.show() # 決定木を学習 dt = DecisionTreeRegressor(max_depth=3, random_state=117117) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() max_depth = 10 # max_depthの値が大きい時、より深い複雑な木ができます。 これは複雑なルールを表現できますが、データ数が少ない場合は過学習している可能性もあります。\ndt = DecisionTreeRegressor(max_depth=10, random_state=117117) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() max-depth=5 # dt = DecisionTreeRegressor(max_depth=5, random_state=117117) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() min_samples_split=60 # 一つの分岐を作るために必要な最低データ数を指定します。 min_samples_split の数を小さくすれば細かいルールを作成できます。大きくすれば過学習は避けられるでしょう。\ndt = DecisionTreeRegressor(max_depth=5, min_samples_split=60, random_state=117117) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() ccp_alpha=0.4 # 木の複雑さにペナルティを与えるパラメータです。 ccp_alphaを設定すると、値が大きいほどシンプルな木が作成されます。\ndt = DecisionTreeRegressor(max_depth=5, random_state=117117, ccp_alpha=0.4) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() max_leaf_nodes=5 # 最終的に作成される葉の数を指定するパラメタです。 max_leaf_nodesの数が、区画の数と一致していることが確認できます。\ndt = DecisionTreeRegressor(max_depth=5, random_state=117117, max_leaf_nodes=5) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() 外れ値がある場合 # 分岐を作成する際にどの基準を適用するか指定します。 外れ値がある状態で、criterion=\u0026quot;squared_error\u0026quot;を指定した場合に木にどのような変化があるかを確認してみます。 absolute_error よりも squared_error の方が外れ値に強いペナルティを与えるため、squared_error と指定した場合は決定木の分岐が作成されることが予想されます。\n## 外れ値として、一部のデータの値を5倍にする X, y = make_regression(n_samples=100, n_features=2, random_state=11) y[1:20] = y[1:20] * 5 dt = DecisionTreeRegressor(max_depth=5, random_state=117117, criterion=\u0026#34;absolute_error\u0026#34;) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() dt = DecisionTreeRegressor(max_depth=5, random_state=117117, criterion=\u0026#34;squared_error\u0026#34;) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() "},{"id":90,"href":"/eval/model-selection/validation_curve/","title":"検証曲線","section":"モデル選択","content":"\rsklearn.model_selection.validation_curve\nモデルの訓練データへの適合度と検証用データへの予測性能を同時に比較するための便利機能。\nimport numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold, StratifiedKFold from sklearn.model_selection import validation_curve from sklearn.metrics import roc_auc_score サンプルデータに対してモデルを作成し交差検証 # 実験用データ # X, y = make_classification( n_samples=1000, n_classes=2, n_informative=4, n_clusters_per_class=3, random_state=RND, ) "},{"id":91,"href":"/prep/numerical/","title":"数値データ","section":"データ前処理","content":" Chapter 3 # 数値データ # "},{"id":92,"href":"/prep/data/read-number-from-data/","title":"数値に対する演算","section":"データの読み込み・集約","content":" データを読み込む # import pandas as pd df = pd.read_csv(\u0026#34;./z-001.csv\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rカラム名を変更する # pandas.Series.str.replace\nUnnamed:XX がわかりにくいので、これらの文字列を置換します。\nrepl = lambda x: x.group(0)[::-1] df.columns = df.columns.str.replace(\u0026#34;Unnamed:.\u0026#34;, \u0026#34;列名\u0026#34;, regex=True) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r# 列名1の列はインデックスにして削除する df.index = df[\u0026#34;列名1\u0026#34;] df = df.drop([\u0026#34;列名1\u0026#34;], axis=1) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r列名を確認する # 想定したデータ型になっているかを確認します。\ndf.dtypes 列名0 int64\r平成21年度 float64\r平成22年度 float64\r平成23年度 float64\r平成24年度 float64\r平成25年度 float64\r平成26年度 float64\r平成27年度 float64\r平成28年度 float64\r平成29年度 float64\r平成30年度 float64\r令和元年度 float64\rdtype: object\r年度ごとに平均・中央値などを求めてみる # pandas.DataFrame.select_dtypes pandas.DataFrame.agg 数値として読み取れる列を選択して、その平均や中央値を求めてみます。\u0026ldquo;list of functions and/or function names, e.g. [np.sum, \u0026lsquo;mean\u0026rsquo;]\u0026ldquo;ということで関数名や関数を指定すると、列ごとにそれを実行します。\ndf.select_dtypes(\u0026#34;number\u0026#34;).agg([\u0026#34;mean\u0026#34;, \u0026#34;median\u0026#34;, \u0026#34;min\u0026#34;, \u0026#34;max\u0026#34;, \u0026#34;sum\u0026#34;, \u0026#34;var\u0026#34;, \u0026#34;std\u0026#34;]) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r年度ごとの変化を可視化する # seabornを用いてデータをプロットしてみます。平成と令和が含まれる列のみを対象に抽出してプロットします。japanize_matplotlibにより日本語を表示できるようにしています。列名は pandas.DataFrame.filterでフィルタリングしています。\nseaborn\nimport matplotlib.pyplot as plt import seaborn as sns import japanize_matplotlib data = df.filter(regex=\u0026#34;(平成|令和).+\u0026#34;, axis=1).T plt.figure(figsize=(12, 4)) sns.lineplot(data=data, palette=\u0026#34;tab10\u0026#34;, linewidth=2.5) plt.grid() plt.legend() data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":93,"href":"/basic/classification/","title":"線形分類","section":"機械学習","content":" Chapter 2 # 線形分類 # "},{"id":94,"href":"/timeseries/model/","title":"単変量","section":"時系列","content":" Chapter 3 # 単変量 # "},{"id":95,"href":"/eval/classification/","title":"分類","section":"評価指標","content":" Chapter 3 # 分類 # "},{"id":96,"href":"/basic/ensemble/adaboost_regression/","title":"Adaboost(回帰)","section":"アンサンブル","content":"\rimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import AdaBoostRegressor # NOTE: モデルのsample_weightを確認するために作成したモデル class DummyRegressor: def __init__(self): self.model = DecisionTreeRegressor(max_depth=5) self.error_vector = None self.X_for_plot = None self.y_for_plot = None def fit(self, X, y): self.model.fit(X, y) y_pred = self.model.predict(X) # 重みは回帰の誤差を元に計算する # https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_weight_boosting.py#L1130 self.error_vector = np.abs(y_pred - y) self.X_for_plot = X.copy() self.y_for_plot = y.copy() return self.model def predict(self, X, check_input=True): return self.model.predict(X) def get_params(self, deep=False): return {} def set_params(self, deep=False): return {} 訓練データに回帰モデルを当てはめる # # 訓練データ X = np.linspace(-10, 10, 500)[:, np.newaxis] y = (np.sin(X).ravel() + np.cos(4 * X).ravel()) * 10 + 10 + np.linspace(-2, 2, 500) ## 回帰モデルを作成 reg = AdaBoostRegressor( DummyRegressor(), n_estimators=100, random_state=100, loss=\u0026#34;linear\u0026#34;, learning_rate=0.8, ) reg.fit(X, y) y_pred = reg.predict(X) # 訓練データへのフィッティング具合を確認する plt.figure(figsize=(10, 5)) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;訓練データ\u0026#34;) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;最終的に作成されたモデルの予測\u0026#34;, linewidth=1) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.title(\u0026#34;訓練データへのフィッティング具合\u0026#34;) plt.legend() plt.show() 標本の重みを可視化(loss=’linear’の場合) # Adaboostでは回帰の誤差を元い重みを決定する。’linear’ に指定した時の重みの大きさを可視化します。 重みが追加されたデータは訓練時にサンプルされる確率が高くなる様子を確認します。\nloss{‘linear’, ‘square’, ‘exponential’}, default=’linear’ The loss function to use when updating the weights after each boosting iteration.\nsklearn.ensemble.AdaBoostRegressor\ndef visualize_weight(reg, X, y, y_pred): \u0026#34;\u0026#34;\u0026#34;標本の重みに相当する値(サンプリングされた数)をプロットするための関数 Parameters ---------- reg : sklearn.ensemble._weight_boosting ブースティングモデル X : numpy.ndarray 訓練データ y : numpy.ndarray 教師データ y_pred: モデルの予測値 \u0026#34;\u0026#34;\u0026#34; assert reg.estimators_ is not None, \u0026#34;len(reg.estimators_) \u0026gt; 0\u0026#34; for i, estimators_i in enumerate(reg.estimators_): if i % 100 == 0: # i番目のモデル作成に使用したデータに、データが何回出現するかをカウント weight_dict = {xi: 0 for xi in X.ravel()} for xi in estimators_i.X_for_plot.ravel(): weight_dict[xi] += 1 # 出現回数をグラフにオレンジ円でプロットする(多いほど大きい円) weight_x_sorted = sorted(weight_dict.items(), key=lambda x: x[0]) weight_vec = np.array([s * 100 for xi, s in weight_x_sorted]) # グラフをプロット plt.figure(figsize=(10, 5)) plt.title(f\u0026#34;{i}番目のモデル作成後の重み付き標本の可視化, loss={reg.loss}\u0026#34;) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;訓練データ\u0026#34;) plt.scatter( estimators_i.X_for_plot, estimators_i.y_for_plot, marker=\u0026#34;o\u0026#34;, alpha=0.2, c=\u0026#34;orange\u0026#34;, s=weight_vec, ) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;最終的に作成されたモデルの予測\u0026#34;, linewidth=2) plt.legend(loc=\u0026#34;upper right\u0026#34;) plt.show() ## loss=\u0026#34;linear\u0026#34;で回帰モデルを作成 reg = AdaBoostRegressor( DummyRegressor(), n_estimators=101, random_state=100, loss=\u0026#34;linear\u0026#34;, learning_rate=1, ) reg.fit(X, y) y_pred = reg.predict(X) visualize_weight(reg, X, y, y_pred) ## loss=\u0026#34;square\u0026#34;で回帰モデルを作成 reg = AdaBoostRegressor( DummyRegressor(), n_estimators=101, random_state=100, loss=\u0026#34;square\u0026#34;, learning_rate=1, ) reg.fit(X, y) y_pred = reg.predict(X) visualize_weight(reg, X, y, y_pred) "},{"id":97,"href":"/webapp/flask_vuejs/","title":"Flask + Vue.js","section":"アプリケーション作成","content":" Flask + Vue.js # アプリの作り方 # アプリケーションに以下の機能を実装します。ユーザーがある企業のコードや名前を入力すると、そのIRや関連企業を表示するようなアプリにします。以下のような機能を持つ想定にします。\nGoogleアカウント等によるログイン機能 入力された企業のIR情報を確認する 入力された企業の関連企業の一覧を出す Node.js # インストール # こちらからインストールします。\nVue.jsのインストール # インストール # npm install --global @vue/cli\rnpm install --global firebase-tools 74 packages are looking for funding\rrun `npm fund` for details\rnpm notice\rnpm notice New minor version of npm available! 10.5.0 -\u0026gt; 10.6.0\rnpm notice Changelog: https://github.com/npm/cli/releases/tag/v10.6.0\rnpm notice Run npm install -g npm@10.6.0 to update!\rnpm notice\rVueのプロジェクトを作成 # vue create app\rcd app\rnpm run serve http://localhost:8080/にアクセスすると以下のような画面をブラウザで見ることができるはずです。\nこの状態で vizir\\app\\src\\App.vue の文字を変更してみると、画面上の文字もリアルタイムで変更される（Hot Reloadが動作している）ことが確認できればokです！\nログイン機能 # firebase login Allow Firebase to collect CLI and Emulator Suite usage and error reporting information? (Y/n) の回答はどちらでもよいです。以下の画面に遷移するはずなので、先ほど登録したメールアドレスでログインをしてください。\nfirebaseのURLを見るとproject-idが分かります。『https://console.firebase.google.com/project/{project-id}/overview?hl=ja』というフォーマットになっています。\n"},{"id":98,"href":"/basic/dimensionality_reduction/kernel-pca/","title":"Kernel-PCA","section":"次元削減","content":"import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_circles Experimental Data # sklearn.datasets.make_circles\nX, y = make_circles(n_samples=400, factor=0.3, noise=0.15) fig = plt.figure(figsize=(8, 8)) plt.scatter(X[:, 0], X[:, 1], c=y) KernelPCA # sklearn.decomposition.KernelPCA\nfrom sklearn.decomposition import KernelPCA kpca = KernelPCA(kernel=\u0026#34;rbf\u0026#34;, degree=2, gamma=3) X_kpca = kpca.fit_transform(X) fig = plt.figure(figsize=(8, 8)) plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y) \u0026lt;matplotlib.collections.PathCollection at 0x7fc8e28323a0\u0026gt;\r"},{"id":99,"href":"/eval/distance/kld/kld/","title":"Kullback-Leiblerダイバージェンス","section":"距離","content":"\rKLダイバージェンス # Kullback-Leiblerダイバージェンスは二つの確率分布の違いを数値で表現したもの。交差エントロピーから情報エントロピーを引くことで求めることができる。\nimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from scipy.special import kl_div 2つの分布 # plt.figure(figsize=(12, 4)) a = np.array([0.1, 0.2, 0.3, 0.2, 0.1, 0.1]) b = np.array([0.05, 0.1, 0.2, 0.3, 0.3, 0.05]) plt.bar(np.arange(a.shape[0]) - 0.1, a, width=0.5) plt.bar(np.arange(b.shape[0]) + 0.1, b, width=0.5) KLD # def kld(dist1, dist2): \u0026#34;\u0026#34;\u0026#34;KLダイバージェンス\u0026#34;\u0026#34;\u0026#34; assert dist1.shape == dist2.shape, \u0026#34;確率分布1と確率分布2は同じ長さである必要があります\u0026#34; assert all(dist1) != 0 and all(dist2) != 0, \u0026#34;確率分布に0を含んではいけません\u0026#34; dist1 = dist1 / np.sum(dist1) dist2 = dist2 / np.sum(dist2) return sum([p1 * np.log2(p1 / p2) for p1, p2 in zip(dist1, dist2)]) print(f\u0026#34;同じ分布ならKLDは0のはず→{kld(a, a)}\u0026#34;) print(f\u0026#34;違う分布ならKLDは0より大きいはず→{kld(a, b)}\u0026#34;) print(f\u0026#34;違う分布ならKLDは0より大きいがkld(a, b)とは違う値になるかも→{kld(b, a)}\u0026#34;) 同じ分布ならKLDは0のはず→0.0\r違う分布ならKLDは0より大きいはず→0.3\r違う分布ならKLDは0より大きいがkld(a, b)とは違う値になるかも→0.3339850002884623\rJensen–Shannon Divergenceの導入 # plt.hist(np.random.normal(1, 1, 1000), alpha=0.85, color=\u0026#34;blue\u0026#34;) plt.hist(np.random.normal(4, 1, 1000), alpha=0.85, color=\u0026#34;red\u0026#34;) plt.hist(np.random.normal(2.5, 1, 1000), alpha=0.85, color=\u0026#34;green\u0026#34;) plt.show() "},{"id":100,"href":"/timeseries/preprocess/003-seasonal-decompose-mstl/","title":"MSTL分解","section":"プロットと前処理","content":" 複数の周期性が重なったデータでをトレンド・季節/周期性・残差に分解する # MSTLはSTL分解(LOESSによる季節・トレンド分解)手法を拡張したもので、複数の季節パターンを持つ時系列の分解が可能です。MSTLはstatsmodelの version==0.14.0 以降でのみ使用可能です。詳細はドキュメントをご確認ください。\nK. Bandura, R.J. Hyndman, and C. Bergmeir (2021) MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with Multiple Seasonal Patterns. arXiv preprint arXiv:2107.13462.\nAnacondaを仮想環境として使用している場合、conda install -c conda-forge statsmodelsでインストールされるものは0.13.Xとなっています(2022/11/1時点)。その場合、作業中の仮想環境の中で以下のコマンドを使用して最近のバージョンをインストールしてください。\npip install git+https://github.com/statsmodels/statsmodels import japanize_matplotlib import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns from statsmodels.tsa.seasonal import MSTL サンプルデータを作成 # 周期的な数値を複数組合せ、さらに区分的にトレンドが変化しています。また np.random.rand() でノイズも乗せています。\ndate_list = pd.date_range(\u0026#34;2018-01-01\u0026#34;, periods=1000, freq=\u0026#34;D\u0026#34;) value_list = [ 10 + i % 14 + 2 * np.sin(10 * np.pi * i / 24) + 5 * np.cos(2 * np.pi * i / (24 * 7)) * 2 + np.log(i**3 + 1) + np.sqrt(i) for i, di in enumerate(date_list) ] df = pd.DataFrame( { \u0026#34;日付\u0026#34;: date_list, \u0026#34;観測値\u0026#34;: value_list, } ) df.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(15, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;観測値\u0026#34;]) plt.grid(axis=\u0026#34;x\u0026#34;) plt.show() トレンド・季節/周期性・残差に分解する # periods = (24, 24 * 7) mstl = MSTL(df[\u0026#34;観測値\u0026#34;], periods=periods).fit() トレンド(.trend) 季節/周期性(.seasonal) 残差(.resid) をそれぞれプロットしてみます。今回は二つの周期の異なる三角関数を足しているので .seasonal には二つの列が含まれています。 残差のプロットにところどころ山があるものの、ほとんどの領域で残差が0に近い（＝きれいに分解できている）ことが確認できます。\n_, axes = plt.subplots(figsize=(12, 8), ncols=1, nrows=4, sharex=True) axes[0].set_title(\u0026#34;観測値\u0026#34;) axes[0].plot(mstl.observed) axes[0].grid() axes[1].set_title(\u0026#34;トレンド\u0026#34;) axes[1].plot(mstl.trend) axes[1].grid() axes[2].set_title(\u0026#34;季節性\u0026#34;) axes[2].plot(mstl.seasonal.iloc[:, 0], label=\u0026#34;周期的な要素１\u0026#34;) axes[2].plot(mstl.seasonal.iloc[:, 1], label=\u0026#34;周期的な要素２\u0026#34;) axes[2].legend() axes[2].grid() axes[3].set_title(\u0026#34;その他の要因・残差\u0026#34;) axes[3].plot(mstl.resid) axes[3].grid() plt.tight_layout() plt.show() "},{"id":101,"href":"/prep/categorical/orderedtargetstatistics/","title":"Ordered Target Statistics","section":"カテゴリカルデータ","content":"\rProkhorenkova, Liudmila, et al. \u0026ldquo;CatBoost: unbiased boosting with categorical features.\u0026rdquo; arXiv preprint arXiv:1706.09516 (2017).\nサンプルデータ # 「人口総数」を予測したいとして、「元号」をエンコードしたいとします。\nimport pandas as pd X = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;) TARGET_NAME = \u0026#34;人口総数\u0026#34; FEATURE_NAME = \u0026#34;元号\u0026#34; X.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rOrdered Target Statistics # category_encoders.cat_boost.CatBoostEncoder sklearn.compose.make_column_transformer from category_encoders.cat_boost import CatBoostEncoder from sklearn.compose import make_column_transformer c_ots = CatBoostEncoder() y = X[TARGET_NAME] X[f\u0026#34;{FEATURE_NAME}_ots\u0026#34;] = c_ots.fit_transform(X[FEATURE_NAME], y) 結果を確認する # カテゴリ変数の列が CatBoostEncoder でエンコードされていることを確認します。\nX[[FEATURE_NAME, f\u0026#34;{FEATURE_NAME}_ots\u0026#34;]] .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rエンコード結果の分布を確認する # import matplotlib.pyplot as plt import japanize_matplotlib plt.figure(figsize=(8, 4)) for i, ci in enumerate(X[FEATURE_NAME].unique()): plt.hist( X.query(f\u0026#34;{FEATURE_NAME}==\u0026#39;{ci}\u0026#39;\u0026#34;)[f\u0026#34;{FEATURE_NAME}_ots\u0026#34;], label=ci, alpha=0.5 ) plt.title(\u0026#34;エンコードされた結果の分布\u0026#34;) plt.legend(title=FEATURE_NAME) "},{"id":102,"href":"/basic/tree/rulefit/","title":"RuleFit","section":"決定木","content":"\rFriedman, Jerome H and Bogdan E Popescu. “Predictive learning via rule ensembles.” The Annals of Applied Statistics. JSTOR, 916–54. (2008).(pdf)\n実験用のデータを取得する # openmlで公開されている CC0 Public Domain のデータセットhouse_sales データセットを使用して回帰モデルを作成します。\n上記openmlページではデータの出典が不明ですが自分が調べた限りではデータの提供元はここのようです。\nsklearn.datasets.fetch_openml\nimport japanize_matplotlib import matplotlib.pyplot as plt import pandas as pd from sklearn.datasets import fetch_openml dataset = fetch_openml(data_id=42092) X = pd.DataFrame(dataset.data, columns=dataset.feature_names) X = X.select_dtypes(\u0026#34;number\u0026#34;) y = dataset.target データの中身を確認する # X.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rRuleFitを実行する # Python implementation of the rulefit algorithm - GitHubの実装を使用してRuleFitを動かしてみます。\n※実行する際は import warnings;warnings.simplefilter('ignore') は外してください\nfrom rulefit import RuleFit import warnings warnings.simplefilter(\u0026#34;ignore\u0026#34;) ## ConvergenceWarning rf = RuleFit(max_rules=100) rf.fit(X.values, y, feature_names=list(X.columns)) RuleFit(max_rules=100,\rtree_generator=GradientBoostingRegressor(learning_rate=0.01,\rmax_depth=100,\rmax_leaf_nodes=5,\rn_estimators=28,\rrandom_state=27,\rsubsample=0.04543939429397564))\r作成されたルールを確認する # rules = rf.get_rules() rules = rules[rules.coef != 0].sort_values(by=\u0026#34;importance\u0026#34;, ascending=False) rules.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rルールが正しいか確認してみる # sqft_above\tlinear\t8.632149e+01\t1.000000\t66243.550192 のルールに基づいて、sqft_above が増加すると y(price)が増える傾向にあるかどうか確認します。\nmatplotlib.pyplot.boxplot — Matplotlib 3.5.1 documentation\nplt.figure(figsize=(6, 6)) plt.scatter(X[\u0026#34;sqft_above\u0026#34;], y, marker=\u0026#34;.\u0026#34;) plt.xlabel(\u0026#34;sqft_above\u0026#34;) plt.ylabel(\u0026#34;price\u0026#34;) sqft_living \u0026lt;= 3935.0 \u0026amp; lat \u0026lt;= 47.5314998626709\trule\t-8.271074e+04\t0.377800\t40101.257833 のルールに該当するデータのみ抽出してみます。 係数がマイナスになっているので、このルールに該当するデータのy(price)は低い傾向にあるはずです。 log(y)を箱髭図で確認すると、確かにルールに該当しているデータのyはルールに該当しないデータのyと比較すると低くなっています。\napplicable_data = np.log( y[X.query(\u0026#34;sqft_living \u0026lt;= 3935.0 \u0026amp; lat \u0026lt;= 47.5314998626709\u0026#34;).index] ) not_applicable_data = np.log( y[X.query(\u0026#34;not(sqft_living \u0026lt;= 3935.0 \u0026amp; lat \u0026lt;= 47.5314998626709)\u0026#34;).index] ) plt.figure(figsize=(10, 6)) plt.boxplot([applicable_data, not_applicable_data], labels=[\u0026#34;ルールに該当\u0026#34;, \u0026#34;ルールに該当しない\u0026#34;]) plt.ylabel(\u0026#34;log(price)\u0026#34;) plt.grid() plt.show() "},{"id":103,"href":"/prep/data/sql-pandas/","title":"SQLで操作","section":"データの読み込み・集約","content":" データを読み込む # import pandas as pd df = pd.read_csv(\u0026#34;./z-001.csv\u0026#34;) df.columns = df.columns.str.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;) # カラム名に含まれるスペースは消す df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rSQLで操作できる形式にする # pandas.DataFrame.to_sql\nfrom sqlite3 import connect conn = connect(\u0026#34;:memory:\u0026#34;) df.to_sql(\u0026#34;Table1\u0026#34;, conn) SQLを実行する # pandas.read_sql\npd.read_sql(\u0026#34;SELECT 平成21年度 FROM Table1\u0026#34;, conn) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":104,"href":"/prep/categorical/","title":"カテゴリカルデータ","section":"データ前処理","content":" Chapter 4 # カテゴリカルデータ # "},{"id":105,"href":"/visualize/category-groupby/sankey-diagram/","title":"サンキーダイアグラム","section":"カテゴリと数値","content":"サンキーダイアグラムはステップ間の流量を表現する図表であり、金額や資源の割り当てがどのように変化しているかを可視化するのに活用することができます。線の太さが割り当てられた流れの量を示しています。このページではplotlyを使ってpython上でサンキーダイアグラムを作成します。\n以下は『Genco Shipping \u0026amp; Trading Limited』のある期の損益を可視化してみたものであり、最新のデータではないので注意してください。\nSankey Diagram in Python | plotly\nimport plotly.graph_objects as go fig = go.Figure( data=[ go.Sankey( node=dict( pad=300, thickness=20, line=dict(color=\u0026#34;white\u0026#34;, width=0.0), label=[ \u0026#34;Voyage revenues\u0026#34;, \u0026#34;Total revenues\u0026#34;, \u0026#34;Total operating expenses\u0026#34;, \u0026#34;Net income\u0026#34;, \u0026#34;Voyage expenses\u0026#34;, \u0026#34;Vessel operating expenses\u0026#34;, \u0026#34;Charter hire expenses\u0026#34;, \u0026#34;General and administrative expenses\u0026#34;, \u0026#34;Depreciation and amortization\u0026#34;, ], color=[ \u0026#34;#666666\u0026#34;, \u0026#34;#666666\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#2BA02D\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#CC0001\u0026#34;, ], ), link=dict( source=[0, 1, 1, 2, 2, 2, 2, 2], target=[1, 2, 3, 4, 5, 6, 7, 8], value=[121008, 84759, 36249, 36702, 18789, 8325, 5854, 13769], color=[ \u0026#34;#B3B3B3\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#9CCC9A\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#E18685\u0026#34;, ], ), ) ] ) fig.update_layout( title_text=\u0026#34;Condensed Consolidated Statements of Operations\u0026#34;, font_size=18 ) fig.show() "},{"id":106,"href":"/finance/visualize/zigzag/","title":"ジグザグ","section":"可視化","content":" ZigZagをプロットする # Zigzagを使用してジグザグをプロットしてみます。 株価のデータとしてTORM plc (TRMD)のデータを使用しています。\nジグザグの見方・使い方\nhttps://github.com/jbn/ZigZag\nimport mplfinance as mpf import numpy as np import pandas as pd import seaborn as sns from matplotlib.dates import DateFormatter from zigzag import peak_valley_pivots # TORM plc (TRMD) trmd = pd.read_csv(\u0026#34;TRMD.csv\u0026#34;, index_col=\u0026#34;Date\u0026#34;) trmd.index = pd.to_datetime(trmd.index) trmd.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rdef plot_zigzag(ax, X, p): \u0026#34;\u0026#34;\u0026#34;zigzagをプロット args: ax: axis X: pandas.Series p: pivots index \u0026#34;\u0026#34;\u0026#34; ax.plot(np.arange(len(X))[p != 0], X[p != 0], \u0026#34;g-\u0026#34;, linewidth=1) return ax fig, axes = mpf.plot( trmd, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4), returnfig=True ) pivots = peak_valley_pivots(trmd[\u0026#34;Close\u0026#34;], 0.03, -0.03) axes[0] = plot_zigzag(axes[0], trmd[\u0026#34;Close\u0026#34;], pivots) "},{"id":107,"href":"/visualize/distribution/violinplot/","title":"バイオリン図","section":"数値の分布","content":"箱ひげ図の両脇に90度回転させた密度グラフを付加したものであり、複数のグループの数値の分布を比較することができます。\nバイオリン図 - Wikipedia\nimport matplotlib.pyplot as plt import seaborn as sns df = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (12, 8)}) sns.violinplot(x=df[\u0026#34;species\u0026#34;], y=df[\u0026#34;sepal_length\u0026#34;]) "},{"id":108,"href":"/prep/table/table2fv/","title":"一括で特徴ベクトル化","section":"テーブルデータ全般","content":" サンプルデータ # import pandas as pd X = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;) X.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rmake_column_transformer # sklearn.compose.make_column_transformerを使用します。指定した複数列に対して、一括で前処理方法を指定します。\nsklearn.compose.make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import StandardScaler from sklearn.compose import make_column_transformer c_oh = OneHotEncoder() n_ss = StandardScaler() ct = make_column_transformer( (c_oh, [\u0026#34;元号\u0026#34;, \u0026#34;町名\u0026#34;]), # カテゴリ変数にonehot (n_ss, [\u0026#34;西暦\u0026#34;, \u0026#34;人口総数\u0026#34;]), # 数値列をスケーリング remainder=\u0026#34;passthrough\u0026#34;, # 指定の無い列はそのまま sparse_threshold=0, ) X_transform = ct.fit_transform(X) # 変換後のテーブル pd.DataFrame(X_transform).head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":109,"href":"/prep/numerical/add_label_to_anomaly/","title":"外れ値にラベルを付与①","section":"数値データ","content":" 実験用のデータ # stats.norm.rvs(size=n_samples)として正規分布からデータを作成し、その中にいくつかを外れ値を追加します。\nfrom scipy import stats import matplotlib.pyplot as plt import japanize_matplotlib import numpy as np # 乱数固定のための指定 np.random.seed(seed=100) # 実験用のデータ n_samples = 1000 x = stats.norm.rvs(size=n_samples) ind = [i for i in range(n_samples)] # 外れ値のインデックス anom_ind = [ 100, 200, 300, 400, 500, ] for an_i in anom_ind: x[an_i] += np.random.randint(10, 20) # プロットして確認 plt.figure(figsize=(12, 4)) plt.plot(ind, x, \u0026#34;.\u0026#34;, label=\u0026#34;通常のデータ\u0026#34;) plt.plot(anom_ind, x[anom_ind], \u0026#34;x\u0026#34;, label=\u0026#34;外れ値\u0026#34;, c=\u0026#34;red\u0026#34;, markersize=12) plt.legend() plt.show() 外れ値を検出する # 標本の平均値と分散を求め、そこから標本ごとの異常度のスコアを求めます。 予め指定した閾値を超えた異常度を持つデータを、外れ値として検出します。\nfrom __future__ import annotations def get_anomaly_index(X: np.array, threshold: float) -\u0026gt; list[int]: \u0026#34;\u0026#34;\u0026#34;一次元データ用外れ値インデックス取得 １次元のデータから異常値のインデックスを取得します Args: data (numpy.array): 検出対象のデータ threshold (float): 外れ値として扱う閾値. Returns: list[int]: 外れ値であるデータのインデックスのリスト Examples: \u0026gt;\u0026gt;\u0026gt; print(get_anomaly_index(np.array([1, 2, 3, ..., 1]), 0.05)) [1, ] \u0026#34;\u0026#34;\u0026#34; avg = np.average(X) var = np.var(X) scores = [(x_i - avg) ** 2 / var for x_i in X] th = stats.chi2.interval(1 - threshold, 1)[1] return [ind for (ind, x) in enumerate(scores) if x \u0026gt; th] 実際に検出できたかを確認する # detected_anom_index = get_anomaly_index(x, 0.05) # プロットして確認 plt.figure(figsize=(12, 4)) plt.plot(ind, x, \u0026#34;.\u0026#34;, label=\u0026#34;通常のデータ\u0026#34;) plt.plot(anom_ind, x[anom_ind], \u0026#34;x\u0026#34;, label=\u0026#34;外れ値\u0026#34;, c=\u0026#34;red\u0026#34;, markersize=12) plt.plot( detected_anom_index, x[detected_anom_index], \u0026#34;o\u0026#34;, label=\u0026#34;検出した外れ値\u0026#34;, c=\u0026#34;red\u0026#34;, alpha=0.2, markersize=12, ) plt.legend() plt.show() "},{"id":110,"href":"/eval/distance/","title":"距離","section":"評価指標","content":" Chapter 4 # 距離 # "},{"id":111,"href":"/basic/tree/","title":"決定木","section":"機械学習","content":" Chapter 3 # 決定木 # "},{"id":112,"href":"/basic/regression/regressionanalysis/","title":"残差の分析","section":"線形回帰","content":" "},{"id":113,"href":"/finance/main/002-cross-correlation/","title":"相互相関","section":"時系列データ","content":"import japanize_matplotlib import numpy as np import os import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from full_fred.fred import Fred # FRED_API_KEY = os.getenv(\u0026#39;FRED_API_KEY\u0026#39;) fred = Fred() print(f\u0026#34;FRED APIキーが環境変数に設定されている：{fred.env_api_key_found()}\u0026#34;) def get_fred_data(name, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;): df = fred.get_series_df(name)[[\u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;]].copy() df[\u0026#34;date\u0026#34;] = pd.to_datetime(df[\u0026#34;date\u0026#34;]) df[\u0026#34;value\u0026#34;] = pd.to_numeric(df[\u0026#34;value\u0026#34;], errors=\u0026#34;coerce\u0026#34;) df = df.set_index(\u0026#34;date\u0026#34;) if end == \u0026#34;\u0026#34;: df = df.loc[f\u0026#34;{start}\u0026#34;:] else: df = df.loc[f\u0026#34;{start}\u0026#34;:f\u0026#34;{end}\u0026#34;] return df FRED APIキーが環境変数に設定されている：True\rProducer Price Index by Commodity: Metals and Metal Products: Iron and Steel # ソース： https://fred.stlouisfed.org/series/WPU101\n鉄及び鋼の生産者物価指数\ndf_WPU101 = get_fred_data(\u0026#34;WPU101\u0026#34;, start=\u0026#34;2018-01-01\u0026#34;, end=\u0026#34;2022-05-01\u0026#34;) df_WPU101.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rsns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) sns.lineplot(data=df_WPU101, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;) \u0026lt;AxesSubplot:xlabel='date', ylabel='value'\u0026gt;\rGNKの価格推移 # Genco Shipping \u0026amp; Trading Limited (＄GNK)の株価と上記グラフを比較してみます。\ndf_GNK = pd.read_csv(\u0026#34;./GNK.csv\u0026#34;) df_GNK[\u0026#34;Date\u0026#34;] = pd.to_datetime(df_GNK[\u0026#34;Date\u0026#34;]) df_GNK = df_GNK.set_index(\u0026#34;Date\u0026#34;).resample(\u0026#34;M\u0026#34;).mean() df_GNK.index = df_GNK.index + pd.DateOffset(1) # 日付を月初に調整する df_GNK = df_GNK.loc[\u0026#34;2018-01-01\u0026#34;:\u0026#34;2022-05-01\u0026#34;] df_GNK.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rjapanize_matplotlib.japanize() fig = plt.figure() ax1 = fig.add_subplot(111) sns.lineplot(data=df_WPU101, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;鉄・鋼の生産者物価指数\u0026#34;) plt.legend(loc=\u0026#34;lower right\u0026#34;, fontsize=\u0026#34;20\u0026#34;) ax2 = ax1.twinx() sns.lineplot(data=df_GNK, x=\u0026#34;Date\u0026#34;, y=\u0026#34;Close\u0026#34;, label=\u0026#34;GNK\u0026#34;, color=\u0026#34;r\u0026#34;) plt.legend(loc=\u0026#34;upper left\u0026#34;, fontsize=\u0026#34;20\u0026#34;) 相互相関を求める # 相互相関関数は、ふたつの信号、配列（ベクトル）の類似性を確認するために使われる。関数の配列の結果がすべて1であれば相関があり、すべてゼロであれば無相関であり、すべて −1 であれば負の相関がある。しばしば、相関と略されることがあり、相関係数と似ているために混同することがある。 (出典：Wikipedia)\nドキュメント： numpy.correlate\ndf_GNK[\u0026#34;Close_norm\u0026#34;] = df_GNK[\u0026#34;Close\u0026#34;] - df_GNK[\u0026#34;Close\u0026#34;].mean() df_WPU101[\u0026#34;value_norm\u0026#34;] = df_WPU101[\u0026#34;value\u0026#34;] - df_WPU101[\u0026#34;value\u0026#34;].mean() corr = np.correlate(df_GNK[\u0026#34;Close_norm\u0026#34;], df_WPU101[\u0026#34;value_norm\u0026#34;], \u0026#34;full\u0026#34;) delay = corr.argmax() - (len(df_WPU101[\u0026#34;value_norm\u0026#34;]) - 1) print(\u0026#34;ラグ： \u0026#34; + str(delay)) plt.plot(corr) ラグ： 0\r[\u0026lt;matplotlib.lines.Line2D at 0x13e252170\u0026gt;]\rProducer Price Index by Industry: Iron Ore Mining # ソース：https://fred.stlouisfed.org/series/PCU2122121221\n鉄鉱石の生産者物価指数\ndf_PCU2122121221 = get_fred_data(\u0026#34;PCU2122121221\u0026#34;, start=\u0026#34;2018-01-01\u0026#34;, end=\u0026#34;2022-05-01\u0026#34;) df_PCU2122121221.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rfig = plt.figure() ax1 = fig.add_subplot(111) sns.lineplot(data=df_PCU2122121221, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;鉄鉱石の生産者物価指数\u0026#34;) plt.legend(loc=\u0026#34;lower right\u0026#34;, fontsize=\u0026#34;20\u0026#34;) ax2 = ax1.twinx() sns.lineplot(data=df_GNK, x=\u0026#34;Date\u0026#34;, y=\u0026#34;Close\u0026#34;, label=\u0026#34;GNK\u0026#34;, color=\u0026#34;r\u0026#34;) plt.legend(loc=\u0026#34;upper left\u0026#34;, fontsize=\u0026#34;20\u0026#34;) df_PCU2122121221[\u0026#34;value_norm\u0026#34;] = ( df_PCU2122121221[\u0026#34;value\u0026#34;] - df_PCU2122121221[\u0026#34;value\u0026#34;].mean() ) corr = np.correlate(df_GNK[\u0026#34;Close_norm\u0026#34;], df_PCU2122121221[\u0026#34;value_norm\u0026#34;], \u0026#34;full\u0026#34;) delay = corr.argmax() - (len(df_PCU2122121221[\u0026#34;value_norm\u0026#34;]) - 1) print(\u0026#34;ラグ： \u0026#34; + str(delay)) plt.plot(corr) ラグ： 0\r[\u0026lt;matplotlib.lines.Line2D at 0x13e373fa0\u0026gt;]\r"},{"id":114,"href":"/timeseries/model_mv/","title":"多変量","section":"時系列","content":" Chapter 4 # 多変量 # "},{"id":115,"href":"/eval/","title":"評価指標","section":"トップページ","content":" Section 4 # 評価指標 # 機械学習モデルの性能を評価するための指標\n"},{"id":116,"href":"/timeseries/model/005-ar-process/","title":"AR過程","section":"単変量","content":"\rimport statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib AR過程のデータを作成する # データを生成するための関数を用意します\ndef create_ARdata(phis=[0.1], N=500, init=1, c=1, sigma=0.3): \u0026#34;\u0026#34;\u0026#34;AR過程のデータを作成する\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;==AR({len(phis)})過程の長さ{N}のデータを作成==\u0026#34;) data = np.zeros(N) data[0] = init + np.random.normal(0, sigma) for t in range(2, N): res = c + np.random.normal(0, sigma) for j, phi_j in enumerate(phis): res += phi_j * data[t - j - 1] data[t] = res return data 係数が1より小さい場合 # plt.figure(figsize=(12, 6)) phis = [0.1] ar1_1 = create_ARdata(phis=phis) plt.plot(ar1_1) plt.title(f\u0026#34;係数が{phis[0]}である場合\u0026#34;, fontsize=15) plt.show() ==AR(1)過程の長さ500のデータを作成==\r係数が１である場合 # plt.figure(figsize=(12, 6)) phis = [1] ar1_2 = create_ARdata(phis=phis) plt.plot(ar1_2) plt.title(f\u0026#34;係数が{phis[0]}である場合\u0026#34;, fontsize=15) plt.show() ==AR(1)過程の長さ500のデータを作成==\r係数が１より大きい場合 # plt.figure(figsize=(12, 6)) phis = [1.04] ar1_2 = create_ARdata(phis=phis) plt.plot(ar1_2) plt.title(f\u0026#34;係数が{phis[0]}である場合\u0026#34;, fontsize=15) plt.show() ==AR(1)過程の長さ500のデータを作成==\rAR(2) # plt.figure(figsize=(12, 6)) phis = [0.1, 0.3] ar2_1 = create_ARdata(phis=phis, N=100) plt.plot(ar2_1) plt.title(f\u0026#34;係数が{phis}である場合\u0026#34;, fontsize=15) plt.show() ==AR(2)過程の長さ100のデータを作成==\rplt.figure(figsize=(12, 6)) phis = [0.1, -1.11] ar2_1 = create_ARdata(phis=phis) plt.plot(ar2_1) plt.title(f\u0026#34;係数が{phis}である場合\u0026#34;, fontsize=15) plt.show() ==AR(2)過程の長さ500のデータを作成==\rモデルの推定 # from statsmodels.tsa.ar_model import AutoReg res = AutoReg(ar1_1, lags=1).fit() out = \u0026#34;AIC: {0:0.3f}, HQIC: {1:0.3f}, BIC: {2:0.3f}\u0026#34; print(out.format(res.aic, res.hqic, res.bic)) AIC: 231.486, HQIC: 236.445, BIC: 244.124\rprint(res.params) print(res.sigma2) res.summary() [1.03832755 0.07236388]\r0.09199676371696269\r"},{"id":117,"href":"/timeseries/preprocess/004-preprocess-log/","title":"Box-Cox変換","section":"プロットと前処理","content":"\rimport japanize_matplotlib import matplotlib.pyplot as plt import numpy as np from scipy import stats import numpy as np plt.figure(figsize=(12, 5)) data_wb = np.random.weibull(2.0, size=50000) plt.hist(data_wb, bins=30, rwidth=0.9) plt.show() plt.figure(figsize=(12, 5)) data_lg = stats.loggamma.rvs(2.0, size=50000) plt.hist(data_lg, bins=30, rwidth=0.9) plt.show() scipy.stats.boxcox — SciPy v1.8.0 Manual\nfrom scipy.stats import boxcox plt.figure(figsize=(12, 5)) plt.hist(boxcox(data_wb), bins=30, rwidth=0.9) plt.show() try: plt.figure(figsize=(12, 5)) plt.hist(boxcox(data_lg), bins=30, rwidth=0.9) plt.show() except ValueError as e: print(f\u0026#34;エラーの内容： ValueError {e.args}\u0026#34;) エラーの内容： ValueError ('Data must be positive.',)\r\u0026lt;Figure size 864x360 with 0 Axes\u0026gt;\rscipy.stats.yeojohnson — SciPy v1.8.0 Manual\nfrom scipy.stats import yeojohnson plt.figure(figsize=(12, 5)) plt.hist(yeojohnson(data_lg), bins=30, rwidth=0.9) plt.show() リッジ回帰をフィッティングしてみる # ｙの分布を正規分布に近づけずにリッジ回帰を適用した場合は、残差の分布に偏りがあることがわかります。\nfrom sklearn.linear_model import Ridge N = 1000 rng = np.random.RandomState(0) y = np.random.weibull(2.0, size=N) X = rng.randn(N, 5) X[:, 0] = np.sqrt(y) + np.random.rand(N) / 10 plt.figure(figsize=(12, 5)) plt.hist(y, bins=20, rwidth=0.9) plt.title(\u0026#34;yの分布\u0026#34;) plt.show() clf = Ridge(alpha=1.0) clf.fit(X, y) pred = clf.predict(X) plt.figure(figsize=(12, 6)) plt.subplot(121) plt.title(\u0026#34;正解と出力の分布\u0026#34;) plt.scatter(y, pred) plt.plot([0, 2], [0, 2], \u0026#34;r\u0026#34;) plt.xlabel(\u0026#34;正解\u0026#34;) plt.ylabel(\u0026#34;出力\u0026#34;) plt.xlim(0, 2) plt.ylim(0, 2) plt.grid() plt.subplot(122) plt.title(\u0026#34;残差の分布\u0026#34;) plt.hist(y - pred) plt.xlim(-0.5, 0.5) plt.show() clf = Ridge(alpha=1.0) clf.fit(X, yeojohnson(y)[0]) pred = clf.predict(X) plt.figure(figsize=(12, 6)) plt.subplot(121) plt.title(\u0026#34;正解と出力の分布\u0026#34;) plt.scatter(yeojohnson(y)[0], pred) plt.plot([0, 2], [0, 2], \u0026#34;r\u0026#34;) plt.xlabel(\u0026#34;正解\u0026#34;) plt.ylabel(\u0026#34;出力\u0026#34;) plt.xlim(0, 2) plt.ylim(0, 2) plt.grid() plt.subplot(122) plt.title(\u0026#34;残差の分布\u0026#34;) plt.hist(yeojohnson(y)[0] - pred) plt.xlim(-0.15, 0.15) plt.show() "},{"id":118,"href":"/basic/ensemble/","title":"アンサンブル","section":"機械学習","content":" Chapter 4 # アンサンブル # "},{"id":119,"href":"/finance/main/003-peak-detectiion/","title":"ピーク検出","section":"時系列データ","content":"\rimport japanize_matplotlib import matplotlib.pyplot as plt import os import pandas as pd import seaborn as sns from full_fred.fred import Fred # FRED_API_KEY = os.getenv(\u0026#39;FRED_API_KEY\u0026#39;) fred = Fred() print(f\u0026#34;FRED APIキーが環境変数に設定されている：{fred.env_api_key_found()}\u0026#34;) def get_fred_data(name, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;): df = fred.get_series_df(name)[[\u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;]].copy() df[\u0026#34;date\u0026#34;] = pd.to_datetime(df[\u0026#34;date\u0026#34;]) df[\u0026#34;value\u0026#34;] = pd.to_numeric(df[\u0026#34;value\u0026#34;], errors=\u0026#34;coerce\u0026#34;) df = df.set_index(\u0026#34;date\u0026#34;) if end == \u0026#34;\u0026#34;: df = df.loc[f\u0026#34;{start}\u0026#34;:] else: df = df.loc[f\u0026#34;{start}\u0026#34;:f\u0026#34;{end}\u0026#34;] return df FRED APIキーが環境変数に設定されている：True\rICE BofA US High Yield Index Option-Adjusted Spread # ソース：https://fred.stlouisfed.org/series/BAMLH0A0HYM2\ndf_BAMLH0A0HYM2 = get_fred_data(\u0026#34;BAMLH0A0HYM2\u0026#34;, start=\u0026#34;2017-01-01\u0026#34;) df_BAMLH0A0HYM2.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rS\u0026amp;P 500 # ソース：https://fred.stlouisfed.org/series/SP500\ndf_SP500 = get_fred_data(\u0026#34;SP500\u0026#34;, start=\u0026#34;2017-01-01\u0026#34;) df_SP500.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rsns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) japanize_matplotlib.japanize() fig = plt.figure() ax1 = fig.add_subplot(111) sns.lineplot(data=df_BAMLH0A0HYM2, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;ハイイールド債スプレッド\u0026#34;, color=\u0026#34;b\u0026#34;) plt.legend(loc=\u0026#34;lower right\u0026#34;, fontsize=\u0026#34;20\u0026#34;) ax2 = ax1.twinx() sns.lineplot(data=df_SP500, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;S\u0026amp;P 500\u0026#34;, color=\u0026#34;r\u0026#34;) plt.legend(loc=\u0026#34;upper left\u0026#34;, fontsize=\u0026#34;20\u0026#34;) \u0026lt;matplotlib.legend.Legend at 0x13735de10\u0026gt;\rピークを検出する # ハイイールド債スプレッドのピーク # scipy.signal.find_peaksを用いて、ピークとなっている時点を検出してみます。\nimport numpy as np from scipy.signal import find_peaks data = df_BAMLH0A0HYM2[\u0026#34;value\u0026#34;] peaks, _ = find_peaks(data, distance=30, threshold=0.03, width=3) plt.plot(data) plt.plot(data.iloc[peaks].index, data.iloc[peaks], \u0026#34;o\u0026#34;, markersize=10) plt.show() SP500とハイイールド債券のピークを比較してみる # 先ほど検出したピークの点に一致する日付には黒い線を入れています。\nsns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) japanize_matplotlib.japanize() sns.lineplot(data=df_SP500, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;S\u0026amp;P 500\u0026#34;, color=\u0026#34;r\u0026#34;) for peak_date in data.iloc[peaks].index: plt.axvline(peak_date, alpha=0.5, linewidth=0.75, color=\u0026#34;k\u0026#34;) plt.text( peak_date + pd.DateOffset(5), 3300, f\u0026#39;{peak_date.strftime(\u0026#34;%Y/%m/%d\u0026#34;)}\u0026#39;, rotation=90, ) plt.legend(loc=\u0026#34;upper left\u0026#34;, fontsize=\u0026#34;20\u0026#34;) \u0026lt;matplotlib.legend.Legend at 0x136f9a020\u0026gt;\r"},{"id":120,"href":"/prep/table/","title":"テーブルデータ全般","section":"データ前処理","content":" Chapter 5 # テーブルデータ全般 # "},{"id":121,"href":"/prep/numerical/add_label_to_anomaly2/","title":"外れ値にラベルを付与②","section":"数値データ","content":" sklearn.ensemble.IsolationForest\n実験用のデータ # from sklearn.datasets import make_moons import matplotlib.pyplot as plt import japanize_matplotlib import numpy as np np.random.seed(seed=100) X, y = make_moons(n_samples=1000, noise=0.1) # 外れ値のインデックス anom_ind = [i * 50 for i in range(18)] for an_i in anom_ind: X[an_i] *= 2.5 # プロットして確認 plt.figure(figsize=(10, 10)) plt.scatter(X[:, 0], X[:, 1], marker=\u0026#34;.\u0026#34;, label=\u0026#34;正常データ\u0026#34;) plt.scatter(X[:, 0][anom_ind], X[:, 1][anom_ind], marker=\u0026#34;x\u0026#34;, s=70, label=\u0026#34;外れ値\u0026#34;) plt.legend() plt.show() 外れ値を検出する # contamination‘auto’ or float, default=’auto’\nThe amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.\ncontaminationパラメタより、外れ値として検出する割合を指定する。\nfrom sklearn.ensemble import IsolationForest clf = IsolationForest( n_estimators=20, max_samples=200, random_state=100, contamination=0.015 ) clf.fit(X) detected_anom_index_train = np.where(clf.predict(X) \u0026lt; 0) 実際に検出できたかを確認する # # プロットして確認 plt.figure(figsize=(10, 10)) plt.scatter(X[:, 0], X[:, 1], marker=\u0026#34;.\u0026#34;, label=\u0026#34;正常データ\u0026#34;) plt.scatter(X[:, 0][anom_ind], X[:, 1][anom_ind], marker=\u0026#34;x\u0026#34;, s=70, label=\u0026#34;外れ値\u0026#34;) plt.scatter( X[:, 0][detected_anom_index_train], X[:, 1][detected_anom_index_train], marker=\u0026#34;o\u0026#34;, s=100, label=\u0026#34;検出した外れ値\u0026#34;, alpha=0.5, ) plt.legend() plt.show() "},{"id":122,"href":"/timeseries/shape/","title":"形状・類似度","section":"時系列","content":" Chapter 3 # 形状・類似度 # "},{"id":123,"href":"/basic/ensemble/gradient_boosting1/","title":"勾配ブースティング","section":"アンサンブル","content":"\rimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.ensemble import GradientBoostingRegressor 訓練データに回帰モデルを当てはめる # 実験用のデータを作成します、三角関数を足し合わせた波形のデータを用意します。\n# 訓練データ X = np.linspace(-10, 10, 500)[:, np.newaxis] noise = np.random.rand(X.shape[0]) * 10 # 目的変数 y = ( (np.sin(X).ravel() + np.cos(4 * X).ravel()) * 10 + 10 + np.linspace(-10, 10, 500) + noise ) # 回帰モデルを作成 reg = GradientBoostingRegressor( n_estimators=50, learning_rate=0.5, ) reg.fit(X, y) y_pred = reg.predict(X) # 訓練データへのフィッティング具合を確認する plt.figure(figsize=(10, 5)) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;訓練データ\u0026#34;) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;最終的に作成されたモデルの予測\u0026#34;, linewidth=1) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.title(\u0026#34;訓練データへのフィッティング具合\u0026#34;) plt.legend() plt.show() 損失関数の結果への影響 # loss を [\u0026ldquo;squared_error\u0026rdquo;, \u0026ldquo;absolute_error\u0026rdquo;, \u0026ldquo;huber\u0026rdquo;, \u0026ldquo;quantile\u0026rdquo;] と変えた場合、訓練データへのフィッティングがどのように変化するかを確認。 \u0026ldquo;absolute_error\u0026rdquo;, \u0026ldquo;huber\u0026quot;は二乗誤差ほど外れ値へのペナルティが大きくないので、外れ値を予測しに行かないです。\nsklearn/ensemble/_gb_losses.py sklearn.ensemble.GradientBoostingRegressor # 訓練データ X = np.linspace(-10, 10, 500)[:, np.newaxis] # 外れ値を用意 noise = np.random.rand(X.shape[0]) * 10 for i, ni in enumerate(noise): if i % 80 == 0: noise[i] = 70 + np.random.randint(-10, 10) # 目的変数 y = ( (np.sin(X).ravel() + np.cos(4 * X).ravel()) * 10 + 10 + np.linspace(-10, 10, 500) + noise ) for loss in [\u0026#34;squared_error\u0026#34;, \u0026#34;absolute_error\u0026#34;, \u0026#34;huber\u0026#34;, \u0026#34;quantile\u0026#34;]: # 回帰モデルを作成 reg = GradientBoostingRegressor( n_estimators=50, learning_rate=0.5, loss=loss, ) reg.fit(X, y) y_pred = reg.predict(X) # 訓練データへのフィッティング具合を確認する plt.figure(figsize=(10, 5)) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;訓練データ\u0026#34;) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;最終的に作成されたモデルの予測\u0026#34;, linewidth=1) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.title(f\u0026#34;訓練データへのフィッティング具合, loss={loss}\u0026#34;, fontsize=18) plt.legend() plt.show() n_estimatorsの結果への影響 # ある程度 n_estimatorsを大きくすると、改善度合いは頭打ちになる様子が確認できます。\nfrom sklearn.metrics import mean_squared_error as MSE # 訓練データ X = np.linspace(-10, 10, 500)[:, np.newaxis] noise = np.random.rand(X.shape[0]) * 10 # 目的変数 y = ( (np.sin(X).ravel() + np.cos(4 * X).ravel()) * 10 + 10 + np.linspace(-10, 10, 500) + noise ) # n_estimatorsを変えてモデルを作成してみる n_estimators_list = [(i + 1) * 5 for i in range(20)] mses = [] for n_estimators in n_estimators_list: # 回帰モデルを作成 reg = GradientBoostingRegressor( n_estimators=n_estimators, learning_rate=0.3, ) reg.fit(X, y) y_pred = reg.predict(X) mses.append(MSE(y, y_pred)) # n_estimatorsを変えた時のmean_squared_errorをプロット plt.figure(figsize=(10, 5)) plt.plot(n_estimators_list, mses, \u0026#34;x\u0026#34;) plt.xlabel(\u0026#34;n_estimators\u0026#34;) plt.ylabel(\u0026#34;Mean Squared Error(training data)\u0026#34;) plt.title(f\u0026#34;訓練データへのフィッティング具合\u0026#34;, fontsize=18) plt.grid() plt.show() learning_rateの結果への影響 # 小さすぎると精度が良くならない、大きすぎると収束しない様子が確認できます。\n# n_estimatorsを変えてモデルを作成してみる learning_rate_list = [np.round(0.1 * (i + 1), 1) for i in range(20)] mses = [] for learning_rate in learning_rate_list: # 回帰モデルを作成 reg = GradientBoostingRegressor( n_estimators=30, learning_rate=learning_rate, ) reg.fit(X, y) y_pred = reg.predict(X) mses.append(np.log(MSE(y, y_pred))) # n_estimatorsを変えた時のmean_squared_errorをプロット plt.figure(figsize=(10, 5)) plt_index = [i for i in range(len(learning_rate_list))] plt.plot(plt_index, mses, \u0026#34;x\u0026#34;) plt.xticks(plt_index, learning_rate_list, rotation=90) plt.xlabel(\u0026#34;learning_rate\u0026#34;, fontsize=15) plt.ylabel(\u0026#34;log(Mean Squared Error) (training data)\u0026#34;, fontsize=15) plt.title(f\u0026#34;訓練データへのフィッティング具合\u0026#34;, fontsize=18) plt.grid() plt.show() "},{"id":124,"href":"/timeseries/","title":"時系列","section":"トップページ","content":" Section 5 # 時系列 # Wikipedia 時系列\n"},{"id":125,"href":"/finance/main/004_etf_and_k-year_u.s._bond_yield/","title":"ETFと利回りを比較","section":"時系列データ","content":" 『メディアプログラミング入門 WebスクレイピングとWebAPI』の講義資料45pで紹介されていたPandas Datareaderを使用しています。\nimport numpy as np import pandas as pd import pandas_datareader.data as web import matplotlib.pyplot as plt import japanize_matplotlib from IPython.display import display, HTML from utils import get_finance_data, get_rsi ETF # pandas.DataFrame.sort_index\n# 銘柄名、期間、保存先ファイル start = \u0026#34;2021-08-01\u0026#34; end = \u0026#34;2022-01-31\u0026#34; # データを取得する voo = get_finance_data(\u0026#34;VOO\u0026#34;, source=\u0026#34;stooq\u0026#34;, start=start, end=end) display(HTML(f\u0026#34;\u0026lt;h3\u0026gt;VOO\u0026lt;/h3\u0026gt;\u0026#34;)) display(voo.head()) vti = get_finance_data(\u0026#34;VTI\u0026#34;, source=\u0026#34;stooq\u0026#34;, start=start, end=end) display(HTML(f\u0026#34;\u0026lt;h3\u0026gt;VTI\u0026lt;/h3\u0026gt;\u0026#34;)) display(vti.head()) spx = get_finance_data(\u0026#34;^SPX\u0026#34;, source=\u0026#34;stooq\u0026#34;, start=start, end=end) display(HTML(f\u0026#34;\u0026lt;h3\u0026gt;S\u0026amp;P500\u0026lt;/h3\u0026gt;\u0026#34;)) display(spx.head()) ndq = get_finance_data(\u0026#34;^NDQ\u0026#34;, source=\u0026#34;stooq\u0026#34;, start=start, end=end) display(HTML(f\u0026#34;\u0026lt;h3\u0026gt;Nasdaq\u0026lt;/h3\u0026gt;\u0026#34;)) display(ndq.head()) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r債権利回り # usy10 = get_finance_data(\u0026#34;10USY.B\u0026#34;, source=\u0026#34;stooq\u0026#34;, start=start, end=end) display(HTML(f\u0026#34;\u0026lt;h3\u0026gt;10-Year U.S. Bond Yield\u0026lt;/h3\u0026gt;\u0026#34;)) display(usy10.head()) usy2 = get_finance_data(\u0026#34;2USY.B\u0026#34;, source=\u0026#34;stooq\u0026#34;, start=start, end=end) display(HTML(f\u0026#34;\u0026lt;h3\u0026gt;2-Year U.S. Bond Yield\u0026lt;/h3\u0026gt;\u0026#34;)) display(usy2.head()) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 4)) plt.plot(usy10.Close, label=\u0026#34;米国債券10年 年利回り\u0026#34;) plt.plot(usy2.Close, label=\u0026#34;米国債券2年 年利回り\u0026#34;) plt.legend() plt.tick_params(rotation=90) plt.grid() plt.show() S\u0026amp;P500と１０年債利回りの比較 # Invert Axes import mplfinance as mpf fig = mpf.figure(figsize=(16, 7), tight_layout=True, style=\u0026#34;default\u0026#34;) ax1 = fig.add_subplot(111) ax2 = ax1.twinx() mpf.plot(spx, type=\u0026#34;candle\u0026#34;, style=\u0026#34;yahoo\u0026#34;, datetime_format=\u0026#34;%Y/%m/%d\u0026#34;, ax=ax1) ax1.set_ylabel(\u0026#34;S\u0026amp;P500\u0026#34;) ax1.invert_xaxis() mpf.plot( usy10, type=\u0026#34;line\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, datetime_format=\u0026#34;%Y/%m/%d\u0026#34;, ax=ax2, ) # 左側にラベルを表示する ax2.tick_params(labelleft=True, labelright=False) ax2.set_ylabel(\u0026#34;10-Year U.S. Bond Yield\u0026#34;) ax2.yaxis.set_label_position(\u0026#34;left\u0026#34;) ax2.legend([\u0026#34;10-Year U.S. Bond Yield\u0026#34;]) \u0026lt;matplotlib.legend.Legend at 0x7fcc4de60970\u0026gt;\r"},{"id":126,"href":"/timeseries/model/005-2-ma-process/","title":"MA課程","section":"単変量","content":"import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib MA過程のデータを生成する # データを生成するための関数を用意します\ndef create_MAdata(thetas=[0.1], mu=1, N=400, init=1, c=1, sigma=0.3): \u0026#34;\u0026#34;\u0026#34;MAモデルで時系列データを生成\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;==MA({len(thetas)})過程の長さ{N}のデータを作成==\u0026#34;) epsilon = np.random.normal(loc=0, scale=sigma, size=N) data = np.zeros(N) data[0] = init for t in range(2, N): res = mu + epsilon[t] for j, theta_j in enumerate(thetas): res += theta_j * epsilon[t - j - 1] data[t] = res return data MA(1) # plt.figure(figsize=(12, 6)) thetas = [0.5] ma1_1 = create_MAdata(thetas=thetas) plt.title(f\u0026#34;MA{len(thetas)}過程\u0026#34;, fontsize=15) plt.plot(ma1_1) plt.show() ==MA(1)過程の長さ400のデータを作成==\rMA(2) # plt.figure(figsize=(12, 6)) thetas = [0.5, 0.5] ma1_2 = create_MAdata(thetas=thetas) plt.title(f\u0026#34;MA{len(thetas)}過程\u0026#34;, fontsize=15) plt.plot(ma1_1) plt.show() ==MA(2)過程の長さ400のデータを作成==\rMA(5) # plt.figure(figsize=(12, 6)) thetas = [0.5 for _ in range(10)] ma1_5 = create_MAdata(thetas=thetas) plt.title(f\u0026#34;MA{len(thetas)}過程\u0026#34;, fontsize=15) plt.plot(ma1_5) plt.show() ==MA(10)過程の長さ400のデータを作成==\r"},{"id":127,"href":"/timeseries/forecast/","title":"Prophet","section":"時系列","content":" Chapter 6 # "},{"id":128,"href":"/basic/clustering/","title":"クラスタリング","section":"機械学習","content":" Chapter 5 # クラスタリング # "},{"id":129,"href":"/visualize/","title":"可視化","section":"トップページ","content":" Section 6 # 可視化 # Wikipedia 可視化\n"},{"id":130,"href":"/basic/ensemble/gradient_boosting2/","title":"勾配ブースティングの可視化","section":"アンサンブル","content":"\r勾配ブースティングを用いた回帰について、仕組みを理解するために可視化をします。\nsklearn.ensemble.GradientBoostingRegressor scikit-learn/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.ensemble import GradientBoostingRegressor 訓練データに回帰モデルを当てはめる # # 訓練データ n_samples = 500 X = np.linspace(-10, 10, n_samples)[:, np.newaxis] noise = np.random.rand(X.shape[0]) * 10 # 目的変数 y = (np.sin(X).ravel()) * 10 + 10 + noise # 回帰モデルを作成 n_estimators = 10 learning_rate = 0.5 reg = GradientBoostingRegressor( n_estimators=n_estimators, learning_rate=learning_rate, ) reg.fit(X, y) y_pred = reg.predict(X) # 訓練データへのフィッティング具合を確認する plt.figure(figsize=(20, 10)) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;訓練データ\u0026#34;) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;最終的に作成されたモデルの予測\u0026#34;, linewidth=1) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.axhline(y=np.mean(y)) plt.title(\u0026#34;訓練データへのフィッティング具合\u0026#34;) plt.legend() plt.show() 最終的な予測結果を木ごとに分解してみる # fig, ax = plt.subplots(figsize=(20, 10)) ind = np.arange(n_samples) temp = np.zeros(n_samples) + np.mean(y) for i in range(n_estimators): res = reg.estimators_[i][0].predict(X) * learning_rate ax.bar(X.flatten(), res, bottom=temp, label=f\u0026#34;${i+1}$番目の木の出力\u0026#34;, alpha=0.05) temp += res plt.scatter(X.flatten(), y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;訓練データ\u0026#34;) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;最終的に作成されたモデルの予測\u0026#34;, linewidth=1) plt.legend() plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) Text(0, 0.5, 'y')\r少しずつ誤差を修正していく様子を確認する # for i in range(5): fig, ax = plt.subplots(figsize=(20, 10)) plt.title(f\u0026#34;{i+1}番目までの木の出力\u0026#34;) ind = np.arange(n_samples) temp = np.zeros(n_samples) + np.mean(y) for j in range(i + 1): res = reg.estimators_[j][0].predict(X) * learning_rate ax.bar(X.flatten(), res, bottom=temp, label=f\u0026#34;${j+1}$番目の木の出力\u0026#34;, alpha=0.05) temp += res plt.scatter(X.flatten(), y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;訓練データ\u0026#34;) plt.legend() plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) try: plt.show() except KeyboardInterrupt: # TODO: jupyter bookのビルドで時間がかかるプロットが中断してしまうため、一時的に例外処理を挟む break "},{"id":131,"href":"/timeseries/preprocess/005-inflation-adjustment/","title":"調整","section":"プロットと前処理","content":"貨幣価値の影響を受けるデータは、分析前に調整する必要がある時もあります。 ここでは消費者物価指数 (CPI)を用いて時系列データに対してインフレ調整をしてみます。具体的には\nFREDから消費者物価指数 (CPI)のデータを、pythonのAPIを用いて取得 アメリカの世帯収入の中央値データを取得 アメリカの世帯収入の中央値データをCPIを用いて調整する アメリカの世帯収入の中央値データ(調整済み)のデータと、CPIを用いて調整したデータを比較 してみます。\nA Python library that quickly adjusts U.S. dollars for inflation using the Consumer Price Index (CPI).\nimport os import cpi import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from full_fred.fred import Fred from datetime import date cpi.update() # 必ず実行する FREDからデータを取得する # FREDから必要なデータを取得します。データの取得方法については別動画で説明していますが、pythonからアクセスするためにはAPIキーの発行が必要です。\n# FRED_API_KEY = os.getenv(\u0026#39;FRED_API_KEY\u0026#39;) fred = Fred() print(f\u0026#34;FRED APIキーが環境変数に設定されている：{fred.env_api_key_found()}\u0026#34;) def get_fred_data(name, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;): df = fred.get_series_df(name)[[\u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;]].copy() df[\u0026#34;date\u0026#34;] = pd.to_datetime(df[\u0026#34;date\u0026#34;]) df[\u0026#34;value\u0026#34;] = pd.to_numeric(df[\u0026#34;value\u0026#34;], errors=\u0026#34;coerce\u0026#34;) df = df.set_index(\u0026#34;date\u0026#34;) if end == \u0026#34;\u0026#34;: df = df.loc[f\u0026#34;{start}\u0026#34;:] else: df = df.loc[f\u0026#34;{start}\u0026#34;:f\u0026#34;{end}\u0026#34;] return df FRED APIキーが環境変数に設定されている：True\rHousehold Income in the United States（世帯収入データ） # Median Household Income in the United States (MEHOINUSA646N)のデータを確認してみます。このデータは、データの単位が\nUnits: Current Dollars, Not Seasonally Adjusted\nとなっており、このデータはインフレ調整・季節調整がされていません。 この家計データをCPIを用いて調整してみます。\ndata = get_fred_data(\u0026#34;MEHOINUSA646N\u0026#34;, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rCPIを用いて値を調整する # data[\u0026#34;adjusted\u0026#34;] = [ cpi.inflate(dollers.value, date.year) for date, dollers in data.iterrows() ] 調整前と調整後の比較 # plt.figure(figsize=(12, 6)) sns.lineplot( data=data, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;Current Dollars, Not Seasonally Adjusted(MEHOINUSA646N)\u0026#34;, ) sns.lineplot(data=data, x=\u0026#34;date\u0026#34;, y=\u0026#34;adjusted\u0026#34;, label=\u0026#34;MEHOINUSA646N adjusted\u0026#34;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x28d341a5e80\u0026gt;\rReal Median Household Income in the United States # 調整済みのデータはReal Median Household Income in the United States (MEHOINUSA672N)にて提供されています。先ほどインフレ調整したデータ（dataの中にあるadjusted列の値）とMEHOINUSA672Nの値を比較してみます。調整前の値を消費者物価指数 (CPI)を用いて調整したものは、調整済みの値（MEHOINUSA672N）とほとんど一致するのが期待値です。\ndata_real = get_fred_data(\u0026#34;MEHOINUSA672N\u0026#34;, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;) data_real.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data_real, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;MEHOINUSA672N\u0026#34;) sns.lineplot(data=data, x=\u0026#34;date\u0026#34;, y=\u0026#34;adjusted\u0026#34;, label=\u0026#34;MEHOINUSA646N adjusted\u0026#34;) # 確認のため、プロットした値をテキストで表示しています for t, v in data_real.iterrows(): plt.text(t, v[0] - 500, f\u0026#34;{v[0]:.2f}\u0026#34;) for t, v in data.iterrows(): plt.text(t, v[1] + 500, f\u0026#34;{v[1]:.2f}\u0026#34;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x28d347968b0\u0026gt;\r"},{"id":132,"href":"/prep/special/","title":"特殊なデータ","section":"データ前処理","content":" Chapter 6 # 特殊なデータ # "},{"id":133,"href":"/finance/main/004-cci/","title":"CCI(消費者信頼感指数)","section":"時系列データ","content":"Consumer Confidence Index(CCI)は消費者信頼感指数とも呼ばれる。経済に対する消費者マインドをアンケート調査結果をもとに指数化したもの。 個人消費と関係があり、先行指標と見做される場合もある。CCIには全米産業審議委員会が公開しているものとミシガン大学が公開しているものがあり、通常はミシガン大学が先に調査結果を公開する。\nimport japanize_matplotlib import numpy as np import os import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from full_fred.fred import Fred # FRED_API_KEY = os.getenv(\u0026#39;FRED_API_KEY\u0026#39;) fred = Fred() print(f\u0026#34;FRED APIキーが環境変数に設定されている：{fred.env_api_key_found()}\u0026#34;) def get_fred_data(name, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;): df = fred.get_series_df(name)[[\u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;]].copy() df[\u0026#34;date\u0026#34;] = pd.to_datetime(df[\u0026#34;date\u0026#34;]) df[\u0026#34;value\u0026#34;] = pd.to_numeric(df[\u0026#34;value\u0026#34;], errors=\u0026#34;coerce\u0026#34;) df = df.set_index(\u0026#34;date\u0026#34;) if end == \u0026#34;\u0026#34;: df = df.loc[f\u0026#34;{start}\u0026#34;:] else: df = df.loc[f\u0026#34;{start}\u0026#34;:f\u0026#34;{end}\u0026#34;] return df FRED APIキーが環境変数に設定されている：True\rUniversity of Michigan: Consumer Sentiment # ソース： University of Michigan: Consumer Sentiment\n消費者のセンチメント（消費者マインド）をアンケートで調査して指数化した景気関連の経済指標です。CCI（Consumer Confidence Index）とも呼ばれています。一般的に個人消費やGDPとの相関性が高く、これらの先行指標として注目されています。 (出典：ＳＭＢＣ日興証券株式会社）\nソースとなるデータはSurveys of Consumers, University of Michiganにて公開されています。「Index of Consumer Sentiment」のデータを取得します。\nstart_date = \u0026#34;2017-01-01\u0026#34; df_UMCSENT = get_fred_data(\u0026#34;UMCSENT\u0026#34;, start=start_date) df_UMCSENT[\u0026#34;前年比\u0026#34;] = df_UMCSENT[\u0026#34;value\u0026#34;].diff(12) df_UMCSENT[\u0026#34;前年比変化\u0026#34;] = df_UMCSENT[\u0026#34;前年比\u0026#34;].diff() S\u0026amp;P 500 # ソース： Categories \u0026gt; Money, Banking, \u0026amp; Finance \u0026gt; Financial Indicators \u0026gt; Stock Market Indexes \u0026gt; S\u0026amp;P 500 | FRED\ndf_SP500 = get_fred_data(\u0026#34;SP500\u0026#34;, start=start_date) df_SP500[\u0026#34;value\u0026#34;] -= df_SP500[\u0026#34;value\u0026#34;].min() df_SP500[\u0026#34;value\u0026#34;] /= df_SP500[\u0026#34;value\u0026#34;].max() df_SP500[\u0026#34;value\u0026#34;] *= 100 df_SP500[\u0026#34;前月比\u0026#34;] = df_SP500[\u0026#34;value\u0026#34;].diff(30) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) japanize_matplotlib.japanize() # 折れ線グラフをプロット sns.lineplot(data=df_UMCSENT, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;消費者信頼感指数\u0026#34;, alpha=0.5) sns.lineplot(data=df_SP500, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;SP500\u0026#34;) sns.lineplot(data=df_UMCSENT, x=\u0026#34;date\u0026#34;, y=\u0026#34;前年比\u0026#34;, label=\u0026#34;消費者信頼感指数の前年比\u0026#34;) sns.lineplot(data=df_UMCSENT, x=\u0026#34;date\u0026#34;, y=\u0026#34;前年比変化\u0026#34;, label=\u0026#34;消費者信頼感指数の前年比変化\u0026#34;) # 前年比との変化が大きいタイミングに線を引く previous_down_date = df_UMCSENT.index[0] for date, row in df_UMCSENT.iterrows(): if row[\u0026#34;前年比変化\u0026#34;] \u0026lt; -2 and row[\u0026#34;前年比\u0026#34;] \u0026lt; -5: previous_down_date = date plt.axvline(x=date, alpha=0.2, linewidth=12, color=\u0026#34;k\u0026#34;) # plt.text( # date + pd.DateOffset(10), # -20, # f\u0026#39;{date.strftime(\u0026#34;%Y/%m/%d\u0026#34;)}\u0026#39;, # rotation=90, # ) plt.legend(fontsize=15, loc=\u0026#34;upper left\u0026#34;) plt.show() "},{"id":134,"href":"/timeseries/preprocess/006-moving-average/","title":"移動平均","section":"プロットと前処理","content":"import japanize_matplotlib as jm import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from full_fred.fred import Fred jm.japanize() サンプルデータ # Natural Gas Consumption (NATURALGAS)のデータを使用しています。数値の単位はBCF（10億立方フィート）です。\ndata = { \u0026#34;value\u0026#34;: { \u0026#34;2013-01-01\u0026#34;: 2878.8, \u0026#34;2013-02-01\u0026#34;: 2567.2, \u0026#34;2013-03-01\u0026#34;: 2521.1, \u0026#34;2013-04-01\u0026#34;: 1967.5, \u0026#34;2013-05-01\u0026#34;: 1752.5, \u0026#34;2013-06-01\u0026#34;: 1742.9, \u0026#34;2013-07-01\u0026#34;: 1926.3, \u0026#34;2013-08-01\u0026#34;: 1927.4, \u0026#34;2013-09-01\u0026#34;: 1767.0, \u0026#34;2013-10-01\u0026#34;: 1866.8, \u0026#34;2013-11-01\u0026#34;: 2316.9, \u0026#34;2013-12-01\u0026#34;: 2920.8, \u0026#34;2014-01-01\u0026#34;: 3204.1, \u0026#34;2014-02-01\u0026#34;: 2741.2, \u0026#34;2014-03-01\u0026#34;: 2557.9, \u0026#34;2014-04-01\u0026#34;: 1961.7, \u0026#34;2014-05-01\u0026#34;: 1810.2, \u0026#34;2014-06-01\u0026#34;: 1745.4, \u0026#34;2014-07-01\u0026#34;: 1881.0, \u0026#34;2014-08-01\u0026#34;: 1933.1, \u0026#34;2014-09-01\u0026#34;: 1809.3, \u0026#34;2014-10-01\u0026#34;: 1912.8, \u0026#34;2014-11-01\u0026#34;: 2357.5, \u0026#34;2014-12-01\u0026#34;: 2679.2, \u0026#34;2015-01-01\u0026#34;: 3115.0, \u0026#34;2015-02-01\u0026#34;: 2925.2, \u0026#34;2015-03-01\u0026#34;: 2591.3, \u0026#34;2015-04-01\u0026#34;: 2007.9, \u0026#34;2015-05-01\u0026#34;: 1858.2, \u0026#34;2015-06-01\u0026#34;: 1899.9, \u0026#34;2015-07-01\u0026#34;: 2067.7, \u0026#34;2015-08-01\u0026#34;: 2052.7, \u0026#34;2015-09-01\u0026#34;: 1901.3, \u0026#34;2015-10-01\u0026#34;: 1987.3, \u0026#34;2015-11-01\u0026#34;: 2249.1, \u0026#34;2015-12-01\u0026#34;: 2588.2, \u0026#34;2016-01-01\u0026#34;: 3091.7, \u0026#34;2016-02-01\u0026#34;: 2652.3, \u0026#34;2016-03-01\u0026#34;: 2356.3, \u0026#34;2016-04-01\u0026#34;: 2083.9, \u0026#34;2016-05-01\u0026#34;: 1965.8, \u0026#34;2016-06-01\u0026#34;: 2000.7, \u0026#34;2016-07-01\u0026#34;: 2186.6, \u0026#34;2016-08-01\u0026#34;: 2208.4, \u0026#34;2016-09-01\u0026#34;: 1947.8, \u0026#34;2016-10-01\u0026#34;: 1925.2, \u0026#34;2016-11-01\u0026#34;: 2159.5, \u0026#34;2016-12-01\u0026#34;: 2866.3, \u0026#34;2017-01-01\u0026#34;: 2913.8, \u0026#34;2017-02-01\u0026#34;: 2340.2, \u0026#34;2017-03-01\u0026#34;: 2523.3, \u0026#34;2017-04-01\u0026#34;: 1932.0, \u0026#34;2017-05-01\u0026#34;: 1892.0, \u0026#34;2017-06-01\u0026#34;: 1910.4, \u0026#34;2017-07-01\u0026#34;: 2141.6, \u0026#34;2017-08-01\u0026#34;: 2093.8, \u0026#34;2017-09-01\u0026#34;: 1920.5, \u0026#34;2017-10-01\u0026#34;: 2031.5, \u0026#34;2017-11-01\u0026#34;: 2357.3, \u0026#34;2017-12-01\u0026#34;: 3086.0, \u0026#34;2018-01-01\u0026#34;: 3340.9, \u0026#34;2018-02-01\u0026#34;: 2710.7, \u0026#34;2018-03-01\u0026#34;: 2796.7, \u0026#34;2018-04-01\u0026#34;: 2350.5, \u0026#34;2018-05-01\u0026#34;: 2055.0, \u0026#34;2018-06-01\u0026#34;: 2063.1, \u0026#34;2018-07-01\u0026#34;: 2350.7, \u0026#34;2018-08-01\u0026#34;: 2313.8, \u0026#34;2018-09-01\u0026#34;: 2156.1, \u0026#34;2018-10-01\u0026#34;: 2285.9, \u0026#34;2018-11-01\u0026#34;: 2715.9, \u0026#34;2018-12-01\u0026#34;: 2999.5, \u0026#34;2019-01-01\u0026#34;: 3424.3, \u0026#34;2019-02-01\u0026#34;: 3019.1, \u0026#34;2019-03-01\u0026#34;: 2927.8, \u0026#34;2019-04-01\u0026#34;: 2212.4, \u0026#34;2019-05-01\u0026#34;: 2134.0, \u0026#34;2019-06-01\u0026#34;: 2119.3, \u0026#34;2019-07-01\u0026#34;: 2393.9, \u0026#34;2019-08-01\u0026#34;: 2433.9, \u0026#34;2019-09-01\u0026#34;: 2206.3, \u0026#34;2019-10-01\u0026#34;: 2306.5, \u0026#34;2019-11-01\u0026#34;: 2783.8, \u0026#34;2019-12-01\u0026#34;: 3170.7, \u0026#34;2020-01-01\u0026#34;: 3320.6, \u0026#34;2020-02-01\u0026#34;: 3058.5, \u0026#34;2020-03-01\u0026#34;: 2722.0, \u0026#34;2020-04-01\u0026#34;: 2256.9, \u0026#34;2020-05-01\u0026#34;: 2072.2, \u0026#34;2020-06-01\u0026#34;: 2127.9, \u0026#34;2020-07-01\u0026#34;: 2464.1, \u0026#34;2020-08-01\u0026#34;: 2399.5, \u0026#34;2020-09-01\u0026#34;: 2151.2, \u0026#34;2020-10-01\u0026#34;: 2315.9, \u0026#34;2020-11-01\u0026#34;: 2442.0, \u0026#34;2020-12-01\u0026#34;: 3182.8, \u0026#34;2021-01-01\u0026#34;: 3343.9, \u0026#34;2021-02-01\u0026#34;: 3099.2, \u0026#34;2021-03-01\u0026#34;: 2649.4, \u0026#34;2021-04-01\u0026#34;: 2265.1, \u0026#34;2021-05-01\u0026#34;: 2117.4, \u0026#34;2021-06-01\u0026#34;: 2238.4, \u0026#34;2021-07-01\u0026#34;: 2412.2, \u0026#34;2021-08-01\u0026#34;: 2433.8, \u0026#34;2021-09-01\u0026#34;: 2142.3, \u0026#34;2021-10-01\u0026#34;: 2262.6, \u0026#34;2021-11-01\u0026#34;: 2693.3, \u0026#34;2021-12-01\u0026#34;: 3007.3, \u0026#34;2022-01-01\u0026#34;: 3612.1, \u0026#34;2022-02-01\u0026#34;: 3064.2, \u0026#34;2022-03-01\u0026#34;: 2785.4, \u0026#34;2022-04-01\u0026#34;: 2379.3, \u0026#34;2022-05-01\u0026#34;: 2247.8, \u0026#34;2022-06-01\u0026#34;: 2326.9, \u0026#34;2022-07-01\u0026#34;: 2597.9, \u0026#34;2022-08-01\u0026#34;: 2566.1, \u0026#34;2022-09-01\u0026#34;: 2263.3, } } data = pd.DataFrame(data) data.rename(columns={\u0026#34;value\u0026#34;: \u0026#34;Natural Gas Consumption(BCF)\u0026#34;}, inplace=True) data.index = pd.to_datetime(data.index) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data) 単純移動平均(SMA) # 単純移動平均線（Simple Moving Average：SMA）は最もシンプルな移動平均の計算方法です。一定期間の値の平均値を計算します。\npandas.DataFrame.rolling — pandas 1.5.2 documentation\ndata_example = data.copy() data_example[\u0026#34;直近12カ月の最大値\u0026#34;] = ( data_example[\u0026#34;Natural Gas Consumption(BCF)\u0026#34;].rolling(12).max() ) data_example[\u0026#34;SMA(12)\u0026#34;] = ( data_example[\u0026#34;Natural Gas Consumption(BCF)\u0026#34;].rolling(12).mean() ) data_example[\u0026#34;直近12カ月の最小値\u0026#34;] = ( data_example[\u0026#34;Natural Gas Consumption(BCF)\u0026#34;].rolling(12).min() ) plt.figure(figsize=(12, 6)) sns.lineplot(data=data_example) 加重移動平均 # wmaが加重移動平均を計算する関数です。w = np.array([0, 1, 2, 3, 4, 5, 5, 4, 3, 2, 1, 0])の箇所で加重移動平均の係数を指定しています。 center=Trueのパラメタが結果に大きく影響するので必ず期待したデータかどうか確認します。\ndef get_wma(s, w=np.array([0, 1, 2, 3, 4, 5, 5, 4, 3, 2, 1, 0])): \u0026#34;\u0026#34;\u0026#34;移動平均を求める Args: s (numpy.ndarray): 時系列データ w (numpy.ndarray, optional): 窓の重み Returns: 加重移動平均計算後の時系列データ (numpy.ndarray) \u0026#34;\u0026#34;\u0026#34; return np.sum(w * s) / w.sum() data_example = data.copy() data_example[\u0026#34;WMA(12, center=True)\u0026#34;] = ( data_example[\u0026#34;Natural Gas Consumption(BCF)\u0026#34;] .rolling(12, center=True) .apply(get_wma, raw=True) ) data_example[\u0026#34;WMA(12, center=False)\u0026#34;] = ( data_example[\u0026#34;Natural Gas Consumption(BCF)\u0026#34;] .rolling(12, center=False) .apply(get_wma, raw=True) ) plt.figure(figsize=(12, 6)) sns.lineplot(data=data_example) plt.show() 指数移動平均 # pandas.DataFrame.ewmを用いて計算します。\npandas.DataFrame.ewm — pandas 1.5.2 documentation\ndata_example = data.copy() data_example[\u0026#34;EMA(12)\u0026#34;] = ( data_example[\u0026#34;Natural Gas Consumption(BCF)\u0026#34;].ewm(span=12, adjust=False).mean() ) plt.figure(figsize=(12, 6)) sns.lineplot(data=data_example) plt.show() "},{"id":135,"href":"/timeseries/hts/","title":"階層化・グループ化","section":"時系列","content":" Chapter 7 # 階層化・グループ化 # "},{"id":136,"href":"/finance/","title":"経済データ","section":"トップページ","content":" Section 7 # 経済データ # Wikipedia 経済指標\n"},{"id":137,"href":"/basic/dimensionality_reduction/","title":"次元削減","section":"機械学習","content":" Chapter 6 # 次元削減 # "},{"id":138,"href":"/basic/feature_selection/","title":"特徴選択","section":"機械学習","content":" Chapter 7 # 特徴選択 # "},{"id":139,"href":"/gpt/","title":"GPTs","section":"トップページ","content":" Section 8 # GPTs # この章では自分が作成したGPTsの使い方を紹介しています。\n英会話の練習相手君 英会話の練習相手用のAI AIの入力・出力例 ビジネス英訳さん 日本語をビジネス英語で英訳してくれるAI AIの入力・出力例 複数英語スタイル同時英訳さん 日本語をカジュアル・標準・フォーマル・文章体・ビジネス場面のスタイルの英文に同時英訳してくれるAI AIの入力・出力例 "},{"id":140,"href":"/timeseries/unsupervised/","title":"教師なしの手法","section":"時系列","content":" Chapter 8 # 教師なしの手法 # "},{"id":141,"href":"/basic/timeseries/","title":"時系列","section":"機械学習","content":" Chapter 8 # 時系列 # "},{"id":142,"href":"/finance/main/005-changepoint-detection/","title":"変化点の検出","section":"時系列データ","content":"\r変化点の検出 # deepcharles/rupturesを使用して、時系列データの変化点を検出してみます。\nC. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. Signal Processing, 167:107299, 2020.\nの論文に技術的な詳細が説明されています。\nBSD 2-Clause License\rCopyright (c) 2017-2021, ENS Paris-Saclay, CNRS\rAll rights reserved.\rRedistribution and use in source and binary forms, with or without\rmodification, are permitted provided that the following conditions are met:\r* Redistributions of source code must retain the above copyright notice, this\rlist of conditions and the following disclaimer.\r* Redistributions in binary form must reproduce the above copyright notice,\rthis list of conditions and the following disclaimer in the documentation\rand/or other materials provided with the distribution.\rTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u0026#34;AS IS\u0026#34;\rAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\rIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\rDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\rFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\rDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\rSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\rCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\rOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\rOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. import matplotlib.pyplot as plt import os import pandas as pd import ruptures as rpt import seaborn as sns from full_fred.fred import Fred # FRED_API_KEY = os.getenv(\u0026#39;FRED_API_KEY\u0026#39;) fred = Fred() print(f\u0026#34;FRED APIキーが環境変数に設定されている：{fred.env_api_key_found()}\u0026#34;) def get_fred_data(name, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;): df = fred.get_series_df(name)[[\u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;]].copy() df[\u0026#34;date\u0026#34;] = pd.to_datetime(df[\u0026#34;date\u0026#34;]) df[\u0026#34;value\u0026#34;] = pd.to_numeric(df[\u0026#34;value\u0026#34;], errors=\u0026#34;coerce\u0026#34;) df = df.set_index(\u0026#34;date\u0026#34;) if end == \u0026#34;\u0026#34;: df = df.loc[f\u0026#34;{start}\u0026#34;:] else: df = df.loc[f\u0026#34;{start}\u0026#34;:f\u0026#34;{end}\u0026#34;] return df FRED APIキーが環境変数に設定されている：True\rFREDのデータを取得 # Retailers: Inventories to Sales Ratioのデータの変化点を検出してみます。このデータは棚卸資産と売上高の比率を示しており『1ヶ月の売上高に対して手元にある在庫が何ヶ月分あるか』の目安となる指標です。\ndf_RETAILIRSA = get_fred_data(\u0026#34;RETAILIRSA\u0026#34;, start=\u0026#34;2000-01-01\u0026#34;) N = len(df_RETAILIRSA[\u0026#34;value\u0026#34;]) signal = df_RETAILIRSA[\u0026#34;value\u0026#34;].values.reshape((N, 1)) algo = rpt.Pelt(model=\u0026#34;rbf\u0026#34;).fit(signal) result = algo.predict(pen=10) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 5)}) sns.lineplot(data=df_RETAILIRSA, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;) for idx in result[:-1]: plt.axvline(x=df_RETAILIRSA.index[idx - 1], color=\u0026#34;r\u0026#34;) plt.show() "},{"id":143,"href":"/webapp/","title":"アプリケーション作成","section":"トップページ","content":" Section 9 # アプリケーション作成 # 機械学習を用いたウェブアプリケーションの作成をします。\n"},{"id":144,"href":"/basic/anomaly/","title":"異常検知","section":"機械学習","content":" Chapter 9 # 異常検知 # "},{"id":145,"href":"/finance/main/006_financials/","title":"企業の財務情報をパース","section":"時系列データ","content":" このページの日英訳は以下のサイトのものを参考しつつ作成していますが、正確性に欠ける可能性があるのであくまで参考程度にお願い致します。 TOMAコンサルタンツグループ株式会社 海外決算書の科目　英語→日本語簡易対訳　損益計算書編\nimport japanize_matplotlib import matplotlib.pyplot as plt import numpy as np import os import pandas as pd import re import seaborn as sns from IPython.core.display import display # 英日辞書 en_US_ja_JP_table = { \u0026#34;Revenue\u0026#34;: \u0026#34;売上\u0026#34;, \u0026#34;Cost of revenue\u0026#34;: \u0026#34;収益コスト\u0026#34;, \u0026#34;Gross profit\u0026#34;: \u0026#34;粗利益\u0026#34;, \u0026#34;Sales, General and administrative\u0026#34;: \u0026#34;販売費及び一般管理費\u0026#34;, \u0026#34;Other operating expenses\u0026#34;: \u0026#34;その他営業費用\u0026#34;, \u0026#34;Total operating expenses\u0026#34;: \u0026#34;営業費用\u0026#34;, \u0026#34;Operating income\u0026#34;: \u0026#34;営業利益\u0026#34;, \u0026#34;Interest Expense\u0026#34;: \u0026#34;支払利息\u0026#34;, \u0026#34;Other income (expense)\u0026#34;: \u0026#34;Other income (expense)\u0026#34;, \u0026#34;Income before taxes\u0026#34;: \u0026#34;Income before taxes\u0026#34;, \u0026#34;Provision for income taxes\u0026#34;: \u0026#34;Provision for income taxes\u0026#34;, \u0026#34;Net income from continuing operations\u0026#34;: \u0026#34;Net income from continuing operations\u0026#34;, \u0026#34;Net income available to common shareholders\u0026#34;: \u0026#34;普通株式に係る当期純利益\u0026#34;, \u0026#34;Net income\u0026#34;: \u0026#34;当期純利益\u0026#34;, \u0026#34;Basic\u0026#34;: \u0026#34;Basic\u0026#34;, \u0026#34;Diluted\u0026#34;: \u0026#34;Diluted\u0026#34;, \u0026#34;EBITDA\u0026#34;: \u0026#34;EBITDA\u0026#34;, \u0026#34;Revenue \u0026#34;: \u0026#34;売上\u0026#34;, \u0026#34;Gross Margin %\u0026#34;: \u0026#34;\t売上総利益率\u0026#34;, \u0026#34;Operating Income \u0026#34;: \u0026#34;営業利益\u0026#34;, \u0026#34;Operating Margin %\u0026#34;: \u0026#34;営業利益率\u0026#34;, \u0026#34;Net Income \u0026#34;: \u0026#34;純利益\u0026#34;, \u0026#34;Earnings Per Share USD\u0026#34;: \u0026#34;EPS (USD)\u0026#34;, \u0026#34;Dividends USD\u0026#34;: \u0026#34;配当 (USD)\u0026#34;, \u0026#34;Payout Ratio % *\u0026#34;: \u0026#34;配当性向\u0026#34;, \u0026#34;Shares Mil\u0026#34;: \u0026#34;株数 (Mil)\u0026#34;, \u0026#34;Book Value Per Share * USD\u0026#34;: \u0026#34;1株あたり純資産\u0026#34;, \u0026#34;Operating Cash Flow \u0026#34;: \u0026#34;営業キャッシュフロー\u0026#34;, \u0026#34;Cap Spending \u0026#34;: \u0026#34;資本的支出\u0026#34;, \u0026#34;Free Cash Flow \u0026#34;: \u0026#34;フリーキャッシュフロー（FCF）\u0026#34;, \u0026#34;Free Cash Flow Per Share * USD\u0026#34;: \u0026#34;1株あたりFCF\u0026#34;, \u0026#34;Working Capital \u0026#34;: \u0026#34;運転資本\u0026#34;, \u0026#34;Key Ratios -\u0026gt; Profitability\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Margins % of Sales\u0026#34;: \u0026#34;Margins % of Sales\u0026#34;, \u0026#34;COGS\u0026#34;: \u0026#34;売上原価\u0026#34;, \u0026#34;Gross Margin\u0026#34;: \u0026#34;売上高総利益率\u0026#34;, \u0026#34;SG\u0026amp;A\u0026#34;: \u0026#34;販売費及び一般管理費\u0026#34;, \u0026#34;R\u0026amp;D\u0026#34;: \u0026#34;研究開発費\u0026#34;, \u0026#34;Operating Margin\u0026#34;: \u0026#34;営業利益率\u0026#34;, \u0026#34;Net Int Inc \u0026amp; Other\u0026#34;: \u0026#34;資金運用利益+その他\u0026#34;, \u0026#34;EBT Margin\u0026#34;: \u0026#34;EBTマージン\u0026#34;, \u0026#34;Debt/Equity\u0026#34;: \u0026#34;負債比率\u0026#34;, \u0026#34;Receivables Turnover\u0026#34;: \u0026#34;売上債権回転率\u0026#34;, \u0026#34;Inventory Turnover\u0026#34;: \u0026#34;棚卸資産回転率\u0026#34;, \u0026#34;Fixed Assets Turnover\u0026#34;: \u0026#34;固定資産回転率\u0026#34;, \u0026#34;Asset Turnover\u0026#34;: \u0026#34;総資産回転率\u0026#34;, \u0026#34;USD Mil\u0026#34;: \u0026#34;(USD Mil)\u0026#34;, \u0026#34;-Year Average\u0026#34;: \u0026#34;年平均\u0026#34;, } def get_preprocessed_df( filepath, is_transpose=True, drop_ttm_lastqrt=True, ttm_datetime=\u0026#34;2021-12\u0026#34;, lastqtr_datetime=\u0026#34;2021-12\u0026#34;, add_ticker_column=True, ): \u0026#34;\u0026#34;\u0026#34;Morningstar, Incのデータを整形する Args: filepath (str): ファイルパス is_transpose (bool, optional): タイムスタンプ列を縦にするかどうか. Defaults to True. drop_ttm_lastqrt (bool, optional): TTM/Last Qrtの記録は削除する. Defaults to True. ttm_datetime (str, optional): 「TTM」をどの日付に置き換えるか. Defaults to \u0026#34;2021-12\u0026#34;. lastqtr_datetime (str, optional): 「Latest Qtr」をどの日付に置き換えるか. Defaults to \u0026#34;2021-12\u0026#34;. add_ticker_column (bool, optional): ticker symbolを示した列を追加するか. Defaults to True. Returns: DataFrame: 整形済みデータフレーム \u0026#34;\u0026#34;\u0026#34; df_preprocessed = [] df_header = [] row_header = \u0026#34;\u0026#34; df = pd.read_table(filepath, header=None) if not drop_ttm_lastqrt: print(f\u0026#34;[Note] TTM は {ttm_datetime} 、Last Qtrは {lastqtr_datetime} の日付として扱われます。\u0026#34;) for idx, row in enumerate(df[0]): # 数値中の「,」を置換する row = re.sub(\u0026#39;\u0026#34;(-?[0-9]+),\u0026#39;, \u0026#39;\u0026#34;\\\\1\u0026#39;, row) row = re.sub(\u0026#39;,(-?[0-9]+)\u0026#34;,\u0026#39;, \u0026#39;\\\\1\u0026#34;,\u0026#39;, row) # 英語を対応する日本語に置き換える for str_en, str_jp in en_US_ja_JP_table.items(): if str_en in row: row = row.replace(str_en, str_jp) # TTMがある行はタイムスタンプなのでヘッダー扱いにする if \u0026#34;TTM\u0026#34; in row or \u0026#34;Latest Qtr\u0026#34; in row: if drop_ttm_lastqrt: row = row.replace(\u0026#34;TTM\u0026#34;, \u0026#34;###IGNORE###\u0026#34;) row = row.replace(\u0026#34;Latest Qtr\u0026#34;, \u0026#34;###IGNORE###\u0026#34;) else: assert ttm_datetime not in row, \u0026#34;その日付はすでに存在しています！\u0026#34; assert lastqtr_datetime not in row, \u0026#34;その日付はすでに存在しています！\u0026#34; row = row.replace(\u0026#34;TTM\u0026#34;, ttm_datetime) row = row.replace(\u0026#34;Latest Qtr\u0026#34;, lastqtr_datetime) df_header = row.split(\u0026#34;,\u0026#34;) if is_transpose: df_header[0] = \u0026#34;月\u0026#34; else: df_header = [\u0026#34;月\u0026#34;] + df_header continue # 数値に変換できるデータは数値に変換してDataFrameに追加 if len(row_splited := row.split(\u0026#34;,\u0026#34;)) \u0026gt; 1: row_data = [ float(v) if re.match(r\u0026#34;^-?\\d+(?:\\.\\d+)$\u0026#34;, v) is not None else v for v in row_splited ] if is_transpose: row_data[0] = ( f\u0026#34;{row_header}/{row_data[0]}\u0026#34; if row_header else f\u0026#34;{row_data[0]}\u0026#34; ) else: row_data = [row_header] + row_data df_preprocessed.append(row_data) else: # 先頭の行はファイルのタイトルが入っているので無視 row_header = f\u0026#34;{row}\u0026#34; if idx \u0026gt; 0 else \u0026#34;\u0026#34; # データフレーム作成 df_preprocessed = pd.DataFrame(df_preprocessed) df_preprocessed.columns = df_header if drop_ttm_lastqrt: df_preprocessed.drop(\u0026#34;###IGNORE###\u0026#34;, axis=1, inplace=True) # 不要な文字列を削除 df_preprocessed.fillna(np.nan, inplace=True) if is_transpose: df_preprocessed = df_preprocessed.T.reset_index() df_preprocessed.columns = df_preprocessed.iloc[0, :] df_preprocessed.drop(0, inplace=True) df_preprocessed[\u0026#34;月\u0026#34;] = pd.to_datetime(df_preprocessed[\u0026#34;月\u0026#34;]) for colname in df_preprocessed.columns: if colname != \u0026#34;月\u0026#34;: df_preprocessed[colname] = pd.to_numeric( df_preprocessed[colname], errors=\u0026#34;coerce\u0026#34; ) if add_ticker_column: filename = os.path.basename(filepath) ticker_symbol = filename[: filename.index(\u0026#34; \u0026#34;)] df_preprocessed[\u0026#34;ticker\u0026#34;] = [ ticker_symbol for _ in range(df_preprocessed.shape[0]) ] return df_preprocessed データを読み込む # 以下の例ではMorningstar, Inc社から提供されているデータを一部引用して使用しています。 このサイトで、指定した企業の財務情報をまとめたcsvファイルを取得します。ここの例ではGolden Ocean Group Ltdなどのデータを使用しています。\n※あくまで表示例であり正確性は保証しません。万一この情報に基づいて被ったいかなる損害についても一切責任を負い兼ねます。\ndf_is_gogl = get_preprocessed_df(\u0026#34;data/GOGL Income Statement.csv\u0026#34;) df_kr_gogl = get_preprocessed_df(\u0026#34;data/GOGL Key Ratios.csv\u0026#34;) df_is_zim = get_preprocessed_df(\u0026#34;data/ZIM Income Statement.csv\u0026#34;) df_kr_zim = get_preprocessed_df(\u0026#34;data/ZIM Key Ratios.csv\u0026#34;) df_is_sblk = get_preprocessed_df(\u0026#34;data/SBLK Income Statement.csv\u0026#34;) df_kr_sblk = get_preprocessed_df(\u0026#34;data/SBLK Key Ratios.csv\u0026#34;) df_income_statement = pd.concat([df_is_gogl, df_is_sblk, df_is_zim]) df_key_ratio = pd.concat([df_kr_gogl, df_kr_zim, df_kr_sblk]) display(df_income_statement.head()) display(df_key_ratio.head()) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r変化を可視化する # seabornを使ってグラフを作成してみます。\nseaborn.barplot\ndef show_barchart(data, x=\u0026#34;月\u0026#34;, y=\u0026#34;売上\u0026#34;, hue=\u0026#34;ticker\u0026#34;, is_log_scale=False): \u0026#34;\u0026#34;\u0026#34;バーチャートを表示する Args: data (pandas.DataFrame): データフレーム x (str, optional): 時間軸. Defaults to \u0026#34;月\u0026#34;. y (str, optional): 比較する指標. Defaults to \u0026#34;売上\u0026#34;. hue (str, optional): 何基準で比較するか. Defaults to \u0026#34;ticker\u0026#34;. is_log_scale (bool, optional): logスケールで表示するかどうか. Defaults to False. \u0026#34;\u0026#34;\u0026#34; sns.set_theme(style=\u0026#34;whitegrid\u0026#34;, rc={\u0026#34;figure.figsize\u0026#34;: (10, 4)}) japanize_matplotlib.japanize() g = sns.barplot(data=data, x=x, y=y, hue=hue) g.set_xticklabels( [xt.get_text().split(\u0026#34;-01\u0026#34;)[0] for xt in g.get_xticklabels()] ) # TODO: mdates.DateFormatterで日付を表示するとなぜか日付がずれるのでラベルを直接書き換える g.tick_params(axis=\u0026#34;x\u0026#34;, rotation=90) if is_log_scale: g.set_yscale(\u0026#34;log\u0026#34;) plt.legend(loc=\u0026#34;upper left\u0026#34;, title=\u0026#34;Ticker Name\u0026#34;) plt.title(f\u0026#34;{y}の比較\u0026#34;, fontsize=14) plt.show() show_barchart(df_income_statement, x=\u0026#34;月\u0026#34;, y=\u0026#34;売上\u0026#34;, hue=\u0026#34;ticker\u0026#34;) show_barchart(df_income_statement, x=\u0026#34;月\u0026#34;, y=\u0026#34;Operating expenses/当期純利益\u0026#34;, hue=\u0026#34;ticker\u0026#34;) "},{"id":146,"href":"/finance/nlp/","title":"テキスト処理","section":"経済データ","content":" Chapter 3 # テキスト処理 # "},{"id":147,"href":"/gpt/english-teacher/","title":"英会話の練習相手君","section":"GPTs","content":" 入力例 # プレゼンテーションの練習 # 私の上司役として、英語のプレゼンテーションの練習相手になってくれませんか？\nもちろん、喜んでお手伝いいたします。私が上司の役を演じ、あなたのプレゼンテーションを聞いた後、フィードバックを提供します。プレゼンテーションのテーマや内容について教えていただけますか？\nOf course, I\u0026rsquo;d be happy to help. I\u0026rsquo;ll play the role of your boss and listen to your presentation, then provide feedback. Could you please tell me about the theme or content of your presentation?\n{ 英語か日本語で会話を続けてください }\n"},{"id":148,"href":"/finance/main/007-%E5%AD%A3%E7%AF%80%E8%AA%BF%E6%95%B4%E6%B8%88%E3%81%BF%E3%81%AE%E7%B5%8C%E6%B8%88%E6%8C%87%E6%A8%99/","title":"季節調整","section":"時系列データ","content":"\r経済データの季節調整 # 総務省統計局の公開している「家計調査（家計収支編）　時系列データ（二人以上の世帯）」のデータを分析します。\n参考文献：経済産業省 鉱工業指数(生産・出荷・在庫、生産能力・稼働率)、製造工業生産予測指数 季節調整手法\nデータの前処理 # 配布されているデータには空白のセルや複数にまたがるセルが含まれていて、このまま処理するのは難しいです。 必要な範囲だけを切り取り数値に変換した new_df を用意します。\nimport pandas as pd df = pd.read_csv(\u0026#34;./支出金額.csv\u0026#34;, encoding=\u0026#34;cp932\u0026#34;) df.head(15) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rnew_df = pd.DataFrame( { \u0026#34;日付\u0026#34;: pd.date_range(start=\u0026#34;2000/1/1\u0026#34;, end=\u0026#34;2022/5/1\u0026#34;, freq=\u0026#34;MS\u0026#34;), \u0026#34;消費支出\u0026#34;: pd.to_numeric(df.iloc[13][6:]), } ) new_df = new_df.set_index(\u0026#34;日付\u0026#34;) new_df[\u0026#34;消費支出\u0026#34;] = (new_df[\u0026#34;消費支出\u0026#34;] / new_df[\u0026#34;消費支出\u0026#34;].mean()) * 100.0 new_df.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rデータをプロットしてみる # import japanize_matplotlib as jm import matplotlib.pyplot as plt import seaborn as sns sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) jm.japanize() sns.lineplot(data=new_df, x=\u0026#34;日付\u0026#34;, y=\u0026#34;消費支出\u0026#34;) plt.show() 傾向変動を取り除く # 人口増加などの要因による長期間にわたる変動傾向。移動平均線を用いたり、最小二乗法による直線・曲線のフィッティングなど、統計データによって手法は異なる。\n参考：numpy.polyfit — NumPy v1.23 Manual\nimport numpy as np idx = [i for i in range(new_df.shape[0])] 傾向変動の関数の係数 = np.polyfit(idx, new_df[\u0026#34;消費支出\u0026#34;], 3) 傾向変動の関数 = np.poly1d(傾向変動の関数の係数) new_df[\u0026#34;傾向変動\u0026#34;] = 傾向変動の関数(idx) new_df[\u0026#34;傾向変動調整分\u0026#34;] = new_df[\u0026#34;消費支出\u0026#34;] - 傾向変動の関数(idx) + np.mean(傾向変動の関数(idx)) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) jm.japanize() sns.lineplot(data=new_df, x=\u0026#34;日付\u0026#34;, y=\u0026#34;消費支出\u0026#34;, label=\u0026#34;消費支出\u0026#34;) sns.lineplot(data=new_df, x=\u0026#34;日付\u0026#34;, y=\u0026#34;傾向変動\u0026#34;, label=\u0026#34;傾向変動\u0026#34;) # sns.lineplot(data=new_df, x=\u0026#34;日付\u0026#34;, y=\u0026#34;傾向変動調整分\u0026#34;, label=\u0026#34;傾向変動調整分\u0026#34;) plt.show() 循環変動 # キチンサイクルなど、数年周期の変動を指すことが多い。\n季節変動 # 一年を周期とした周期的な変動を総称して「季節変動」と呼ぶ。月別平均法・連環比率法など。\n①過去の複数年の月別平均値を求める # new_df[\u0026#34;月\u0026#34;] = new_df.index.month 季節ごとの平均値 = new_df.groupby(\u0026#34;月\u0026#34;).mean()[\u0026#34;消費支出\u0026#34;] 季節ごとの平均値 月\r1 100.083036\r2 92.102619\r3 107.280556\r4 104.477264\r5 97.832924\r6 94.348753\r7 98.973205\r8 99.165835\r9 95.138160\r10 99.351828\r11 96.230671\r12 114.934402\rName: 消費支出, dtype: float64\r②月別平均値の合計を求める # 月別平均値の合計 = 季節ごとの平均値.sum() print(f\u0026#34;月別平均値の合計 = {月別平均値の合計}\u0026#34;) 月別平均値の合計 = 1199.9192545557726\r③修正係数を計算する # 修正係数 = 1200.0 / 月別平均値の合計 print(f\u0026#34;修正係数 = {修正係数}\u0026#34;) 修正係数 = 1.0000672923981517\r④月別平均に修正係数を乗じる # 修正済みの季節ごとの平均値 = 修正係数 * 季節ごとの平均値 # 各月のデータから修正済みの季節ごとの平均値を引いています new_df[\u0026#34;季節変動調整分\u0026#34;] = new_df[\u0026#34;消費支出\u0026#34;] - new_df[\u0026#34;月\u0026#34;].apply(lambda m: 修正済みの季節ごとの平均値[m]) new_df.to_csv(\u0026#34;支出金額_加工済みデータ.csv\u0026#34;, encoding=\u0026#34;utf-8-sig\u0026#34;) # グラフを作成 sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) jm.japanize() sns.lineplot(data=new_df, x=\u0026#34;日付\u0026#34;, y=\u0026#34;消費支出\u0026#34;, label=\u0026#34;消費支出\u0026#34;) sns.lineplot(data=new_df, x=\u0026#34;日付\u0026#34;, y=\u0026#34;傾向変動\u0026#34;, label=\u0026#34;傾向変動\u0026#34;) sns.lineplot(data=new_df, x=\u0026#34;日付\u0026#34;, y=\u0026#34;季節変動調整分\u0026#34;, label=\u0026#34;季節変動調整分\u0026#34;) plt.show() "},{"id":149,"href":"/finance/main/008-anomaly-detection/","title":"外れ値","section":"時系列データ","content":"\r総務省統計局の公開している「家計調査（家計収支編）　時系列データ（二人以上の世帯）」のデータを分析します。\n経済産業省 鉱工業指数(生産・出荷・在庫、生産能力・稼働率)、製造工業生産予測指数 季節調整手法\nimport japanize_matplotlib as jm import matplotlib.pyplot as plt import pandas as pd import seaborn as sns data = pd.read_csv(\u0026#34;./支出金額_加工済みデータ.csv\u0026#34;, index_col=\u0026#34;日付\u0026#34;) data.index = pd.to_datetime(data.index) data.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rsns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) jm.japanize() sns.lineplot(data=data, x=\u0026#34;日付\u0026#34;, y=\u0026#34;季節変動調整分\u0026#34;, label=\u0026#34;季節変動調整分\u0026#34;) plt.show() 異常値を検出してみる # DHI/tsodを用いて異常値を検出してみます。\nimport tsod magd = tsod.DiffDetector(direction=\u0026#34;positive\u0026#34;) series = data[\u0026#34;季節変動調整分\u0026#34;] magd.fit(series[0:10]) res = magd.detect(series) plt.figure(figsize=(16, 4)) plt.plot(series) plt.plot(series[res], \u0026#34;ro\u0026#34;, label=\u0026#34;Anomaly\u0026#34;) for d in series[res].index: plt.axvline(d, alpha=0.5, linewidth=0.75, color=\u0026#34;k\u0026#34;) plt.text( d + pd.DateOffset(10), 10, f\u0026#39;{d.strftime(\u0026#34;%Y/%m/%d\u0026#34;)}\u0026#39;, rotation=90, ) plt.legend() plt.show() ピークの点付近にあった出来事 # 2000/12頃 # 日本経済は、2000年10月に(1)景気の山を越え、景気後退局面に入った。その後、2001年を通じて、生産は大幅に減少するとともに、失業率も既往最高水準を更新し、景気は悪化を続けた。（経済財政白書／経済白書 \u0026gt; 平成14年度 年次経済財政報告）\n2014/03頃 # 2014年の消費増税時を振り返る データから見えた消費への影響と小売店の対策とは？\n2019/10頃 # 2019年10月に消費税率は10％になりました。 | 財務省\n2020/06頃 # 都道府県をまたぐ移動、6月19日から全国的に緩和\n"},{"id":150,"href":"/finance/main/010-quantstats/","title":"Quantstats","section":"時系列データ","content":"\rranaroussi/quantstats: Portfolio analytics for quants\nimport numpy as np import pandas as pd import quantstats as qs import yfinance as yf qs.extend_pandas() VTIと特定の銘柄の比較レポート # GOOG_returns = qs.utils.download_returns(\u0026#34;GOOG\u0026#34;, period=\u0026#34;2y\u0026#34;) GOOG_returns.head(10) qs.stats.sharpe(GOOG_returns) qs.plots.snapshot(GOOG_returns, title=\u0026#34;GOOG Performance\u0026#34;) qs.reports.html(GOOG_returns, \u0026#34;VTI\u0026#34;, download_filename=\u0026#34;GOOGとVTIの比較.html\u0026#34;) 複数銘柄のパフォーマンス比較レポート # すべての銘柄を等しい割合で保有していた場合のリターンを、SP500と比較して見ます。 銘柄のオアフォーマンスを計算するには株価の変化率の系列が必要なので、pandas.DataFrame.pct_change を用いて変化率の系列を求めています。\npandas.DataFrame.pct_change\n# ポートフォリオ内での各銘柄の比重 stock_dict = { \u0026#34;MSFT\u0026#34;: 0.3, \u0026#34;AAPL\u0026#34;: 0.3, \u0026#34;AMZN\u0026#34;: 0.2, \u0026#34;GOOG\u0026#34;: 0.1, \u0026#34;TSLA\u0026#34;: 0.1, } # 株価の系列 stock_prices_df = yf.download( list(stock_dict.keys()), start=\u0026#34;2021-01-01\u0026#34;, end=\u0026#34;2022-01-01\u0026#34;, adjusted=True ).dropna() stock_prices_df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead tr th {\rtext-align: left;\r}\r.dataframe thead tr:last-of-type th {\rtext-align: right;\r}\rreturns_df = stock_prices_df[\u0026#34;Adj Close\u0026#34;].pct_change().dropna() returns_df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rpf_returns = np.dot(list(stock_dict.values()), returns_df.T) print(pf_returns[:10]) [-0.01530147 0.00900607 -0.01796814 0.02928912 0.0152212 -0.02670266\r0.00147395 0.01133958 -0.01243583 -0.00920841]\rpf_returns_series = pd.Series(pf_returns, index=returns_df.index) pf_returns_series.head(10) Date\r2021-01-04 -0.015301\r2021-01-05 0.009006\r2021-01-06 -0.017968\r2021-01-07 0.029289\r2021-01-08 0.015221\r2021-01-11 -0.026703\r2021-01-12 0.001474\r2021-01-13 0.011340\r2021-01-14 -0.012436\r2021-01-15 -0.009208\rdtype: float64\rqs.reports.html(pf_returns_series, \u0026#34;VTI\u0026#34;, download_filename=\u0026#34;ポートフォリオのパフォーマンス.html\u0026#34;) "},{"id":151,"href":"/gpt/business-english/","title":"ビジネス英訳さん","section":"GPTs","content":" 入力例 # プレゼンテーションの練習 # 発表は以上です\n● \u0026ldquo;That concludes my presentation.\u0026rdquo; ● \u0026ldquo;This marks the end of my presentation.\u0026rdquo; ● \u0026ldquo;I have finished my presentation.\u0026rdquo;\n"},{"id":152,"href":"/finance/misc/pyportfolioopt/","title":"投資信託","section":"その他","content":" 投資信託で効率的フロンティアを計算してみる # 2014091602: AB･米国成長株投信Dコース(H無) 予想分配金 https://www.wealthadvisor.co.jp/FundData/SnapShot.do?fnc=2014091602 2018103105:eMAXIS Slim全世界株式(オール･カントリー) https://www.wealthadvisor.co.jp/FundData/SnapShot.do?fnc=2018103105 2015042708: ニッセイ TOPIXインデックスファンド https://www.wealthadvisor.co.jp/FundData/SnapShot.do?fnc=2015042708 2019092601: SBI・V・S\u0026amp;P500インデックス・ファンド https://www.wealthadvisor.co.jp/FundData/SnapShot.do?fnc=2019092601 2018070301: eMAXIS Slim米国株式(S\u0026amp;P500) https://www.wealthadvisor.co.jp/FundData/SnapShot.do?fnc=2018070301 2011020701: 三菱UFJ 純金ファンド（←保有していないですが金価格の参考として） https://www.wealthadvisor.co.jp/FundData/SnapShot.do?fnc=2011020701 データの読み込み # WEALTH ADVISERから手動で取得した週次のcsvデータを使用して、効率的フロンティアを計算してみます。\nimport os import pandas as pd データ保存先ディレクトリ = \u0026#34;C://Users//nanja-win-ms//Dropbox//PC//Downloads//\u0026#34; 投資信託一覧 = { \u0026#34;2018103105\u0026#34;: \u0026#34;eMAXIS Slim全世界株式(オール･カントリー)\u0026#34;, \u0026#34;2015042708\u0026#34;: \u0026#34;ニッセイ TOPIXインデックスファンド\u0026#34;, \u0026#34;2019092601\u0026#34;: \u0026#34;SBI・V・S\u0026amp;P500インデックス・ファンド\u0026#34;, \u0026#34;2018070301\u0026#34;: \u0026#34;eMAXIS Slim米国株式(S\u0026amp;P500)\u0026#34;, \u0026#34;2011020701\u0026#34;: \u0026#34;三菱UFJ 純金ファンド\u0026#34;, } 投資信託リターン = {} for 投資信託ID, 投資信託名 in 投資信託一覧.items(): 月次リターンファイル名 = [ c for c in os.listdir(os.path.join(データ保存先ディレクトリ, 投資信託ID)) if c.startswith(\u0026#34;基準価額\u0026#34;) ][0] 投資信託リターン[投資信託ID] = pd.read_csv( os.path.join(データ保存先ディレクトリ, 投資信託ID, 月次リターンファイル名), encoding=\u0026#34;cp932\u0026#34; ) 投資信託リターン[投資信託ID].columns = [\u0026#34;日付\u0026#34;, 投資信託名] データの整形 # ｘ＝日付、ｙ＝銘柄名のデータを作成します。投資信託でなく株式でも可能です。\nポートフォリオ = None for 投資信託ID in 投資信託一覧.keys(): if ポートフォリオ is None: ポートフォリオ = 投資信託リターン[投資信託ID] else: ポートフォリオ = pd.merge(ポートフォリオ, 投資信託リターン[投資信託ID], on=\u0026#34;日付\u0026#34;) ポートフォリオ.index = pd.to_datetime(ポートフォリオ[\u0026#34;日付\u0026#34;], format=\u0026#34;%Y%m%d\u0026#34;) ポートフォリオ.drop(\u0026#34;日付\u0026#34;, axis=1, inplace=True) ポートフォリオ .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r可視化 # import japanize_matplotlib import matplotlib.pyplot as plt import seaborn as sns plt.figure(figsize=(12, 6)) sns.lineplot(data=ポートフォリオ) \u0026lt;Axes: xlabel='日付'\u0026gt;\rfrom pypfopt import expected_returns, risk_models from pypfopt.efficient_frontier import EfficientFrontier mu = expected_returns.mean_historical_return(ポートフォリオ) S = risk_models.sample_cov(ポートフォリオ) ef = EfficientFrontier(mu, S) weights = ef.max_sharpe() ef.portfolio_performance(verbose=True) Expected annual return: 115.5%\rAnnual volatility: 34.2%\rSharpe Ratio: 3.32\r(1.1546507705298557, 0.34188826020611157, 3.318776637272708)\rfrom pypfopt import CLA, plotting cla = CLA(mu, S) cla.max_sharpe() cla.portfolio_performance(verbose=True) plt.figure(figsize=(10, 5)) plotting.plot_efficient_frontier(cla, show_assets=True, points=50, show_tickers=True) Expected annual return: 115.1%\rAnnual volatility: 34.1%\rSharpe Ratio: 3.32\r\u0026lt;Axes: xlabel='Volatility', ylabel='Return'\u0026gt;\r値動きの相関 # 値動きの変化で相関を計算してみます。\nsns.heatmap(ポートフォリオ.diff().dropna().corr(), annot=True, fmt=\u0026#34;1.4f\u0026#34;) \u0026lt;Axes: \u0026gt;\r"},{"id":153,"href":"/finance/misc/edinet/","title":"EDINET","section":"その他","content":"\rEDINETのAPIを使用してみる # import os import requests import pandas as pd API_ENDPOINT = \u0026#34;https://disclosure.edinet-fsa.go.jp/api/v2\u0026#34; # v2を使用する 書類一覧APIのリクエストURL # 11_EDINET_API仕様書に従って書類一覧を取得してみます。\nrequest_params = { \u0026#34;date\u0026#34;: \u0026#34;2024-02-09\u0026#34;, \u0026#34;type\u0026#34;: 2, # 1=メタデータのみ、2=提出書類一覧及びメタデータ \u0026#34;Subscription-Key\u0026#34;: os.environ.get( \u0026#34;EDINET_API_KEY\u0026#34; ), # v1を使用する場合は不要, 2024年３月29日（金）まで利用可能 } docs_submitted_json = requests.get( f\u0026#34;{API_ENDPOINT}/documents.json\u0026#34;, request_params ).json() 取得データの確認 # 今は四半期報告書を取得したいので、『四半期報告書』が文書の説明（docDescription）に含まれている行のみを抽出して確認します。\nsd_df = pd.DataFrame(docs_submitted_json[\u0026#34;results\u0026#34;]) sd_df = sd_df[sd_df[\u0026#34;docDescription\u0026#34;].str.contains(\u0026#34;四半期報告書\u0026#34;, na=False)] sd_df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rDocIDの確認 # 商船三井の四半期報告書のDocIDを確認してみます。filerNameに商船三井が含まれる行のみを抽出します。\nsd_df[sd_df.filerName.str.contains(\u0026#34;商船三井\u0026#34;)] .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r四半期報告書の取得 # docID をもとに四半期報告書の取得に必要なデータを取得します。pdfを取得したい場合はマニュアルに従い、type=2を指定します。\ndocID = \u0026#34;S100STH6\u0026#34; pdf_response = requests.get( f\u0026#34;{API_ENDPOINT}/documents/{docID}\u0026#34;, { \u0026#34;type\u0026#34;: 2, \u0026#34;Subscription-Key\u0026#34;: os.environ.get(\u0026#34;EDINET_API_KEY\u0026#34;), }, ) with open(\u0026#34;sample.pdf\u0026#34;, \u0026#34;wb\u0026#34;) as f: f.write(pdf_response.content) ダウンロードしたpdfを表示する # from IPython.display import display_pdf with open(\u0026#34;sample.pdf\u0026#34;, \u0026#34;rb\u0026#34;) as f: display_pdf(f.read(), raw=True) （pdfの表示は省略します）\n"},{"id":154,"href":"/gpt/multi-style-english/","title":"複数英語スタイル同時英訳さん","section":"GPTs","content":" 入力例 # プレゼンテーションの練習 # スライドの２ページ目を見てください\n● カジュアル: Please look at slide number two. ● 標準: Kindly turn to page two of the slide presentation. ● フォーマル: I request your attention on the second slide. ● 文章: May I direct your focus to the second page of the slide deck. ● ビジネス: Please refer to the second slide in the presentation.\n"},{"id":155,"href":"/finance/misc/edinet2/","title":"EDINET②","section":"その他","content":"\r本書のゴール＝pythonで売上高や配当金のグラフを作成する\n以下のコンテンツではEDINET閲覧サイト（url）から取得できる情報をもとにデータを加工して作成しています。本ページに記載されているコードの実行や取得したデータの利用についてはEDINETの利用規約を確認し理解した上で利用してください。また、Version2のAPIを使用する際はログイン認証＋APIキーの発行が必要になります。\n※注意：使用時には必ず利用規約（pdf・web)を確認した上で常識の範囲内での使用にとどめてください。\nimport os import time import zipfile import pandas as pd import requests API_ENDPOINT = \u0026#34;https://disclosure.edinet-fsa.go.jp/api/v2\u0026#34; # v2を使用する def save_csv(docID, type=5): \u0026#34;\u0026#34;\u0026#34;EDINETからデータを取得してフォルダに保存する Args: docID (str): DocID \u0026#34;\u0026#34;\u0026#34; assert type in [1, 2, 3, 4, 5], \u0026#34;typeの指定が間違っている\u0026#34; if type == 1: print(f\u0026#34;{docID}のXBRLデータを取得中\u0026#34;) elif type == 2: print(f\u0026#34;{docID}のpdfデータを取得中\u0026#34;) elif type in {3, 4}: print(f\u0026#34;{docID}のデータを取得中\u0026#34;) elif type == 5: print(f\u0026#34;{docID}のcsvデータを取得中\u0026#34;) time.sleep(5) r = requests.get( f\u0026#34;{API_ENDPOINT}/documents/{docID}\u0026#34;, { \u0026#34;type\u0026#34;: type, \u0026#34;Subscription-Key\u0026#34;: os.environ.get(\u0026#34;EDINET_API_KEY\u0026#34;), }, ) if r is None: print(\u0026#34;データの取得に失敗しました。csvFlag==1かどうか確認してください。\u0026#34;) else: os.makedirs(f\u0026#34;{docID}\u0026#34;, exist_ok=True) temp_zip = \u0026#34;uuid_89FD71B5_CD7B_4833-B30D‗5AA5006097E2.zip\u0026#34; with open(temp_zip, \u0026#34;wb\u0026#34;) as f: for chunk in r.iter_content(chunk_size=1024): f.write(chunk) with zipfile.ZipFile(temp_zip) as z: z.extractall(f\u0026#34;{docID}\u0026#34;) os.remove(temp_zip) 書類取得API # csvデータをAPI経由で取得してzipファイルに保存します。\ndocID_dict = { \u0026#34;商船三井\u0026#34;: \u0026#34;S100STH6\u0026#34;, \u0026#34;日本郵船\u0026#34;: \u0026#34;S100SS7P\u0026#34;, \u0026#34;玉井商船株式会社\u0026#34;: \u0026#34;S100STLS\u0026#34;, \u0026#34;川崎汽船\u0026#34;: \u0026#34;S100SRTI\u0026#34;, \u0026#34;飯野海運\u0026#34;: \u0026#34;S100SP9O\u0026#34;, } for docID in docID_dict.values(): save_csv(docID, type=5) S100STH6のcsvデータを取得中\rS100SS7Pのcsvデータを取得中\rS100STLSのcsvデータを取得中\rS100SRTIのcsvデータを取得中\rS100SP9Oのcsvデータを取得中\r取得したデータの確認 # dfs = [] for companyName, docID in docID_dict.items(): csv_savedir = os.path.join(docID, \u0026#34;XBRL_TO_CSV\u0026#34;) filelist = [f for f in os.listdir(csv_savedir) if f.startswith(\u0026#34;jpcrp\u0026#34;)] if len(filelist) \u0026gt; 0: df = pd.read_csv( os.path.join(csv_savedir, filelist[0]), encoding=\u0026#34;utf-16\u0026#34;, sep=\u0026#34;\\t\u0026#34; ) df[\u0026#34;会社名\u0026#34;] = [companyName for _ in range(df.shape[0])] dfs.append(df) all_data = pd.concat(dfs) all_data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r取得したデータを確認してみる # EDINET タクソノミ の概要説明を見ながらデータのある項目を抽出してみます。\nprint( all_data.query(f\u0026#34;要素ID==\u0026#39;jpcrp_cor:NotesRegardingDividendTextBlock\u0026#39;\u0026#34;)[[\u0026#34;会社名\u0026#34;, \u0026#34;値\u0026#34;]] ) 会社名 値\r462 商船三井 １．配当金支払額（決議）株式の種類配当金の総額（百万円）１株当たり配当額（円）基準日効力発生...\r463 商船三井 １．配当金支払額（決議）株式の種類配当金の総額（百万円）１株当たり配当額（円）基準日効力発生...\r431 日本郵船 配当金支払額（決議）株式の種類配当金の総額（百万円）配当の原資１株当たり配当額（円）基準日効...\r432 日本郵船 配当金支払額（決議）株式の種類配当金の総額（百万円）配当の原資１株当たり配当額（円）基準日効...\r334 玉井商船株式会社 前第３四半期連結累計期間（自　2022年４月１日　至　2022年12月31日）１．配当金支払...\r335 玉井商船株式会社 当第３四半期連結累計期間（自　2023年４月１日　至　2023年12月31日）１．配当金支払...\r380 川崎汽船 １．配当金支払額（決議）株式の種類配当金の総額（百万円）１株当たり配当額（円）基準日効力発生...\r382 川崎汽船 １．配当金支払額（決議）株式の種類配当金の総額（百万円）１株当たり配当額（円）基準日効力発生...\r385 飯野海運 １　配当金支払額（決議）株式の種類配当金の総額（百万円）１株当たり配当額（円）基準日効力発生...\r386 飯野海運 １　配当金支払額（決議）株式の種類配当金の総額（百万円）１株当たり配当額（円）基準日効力発生...\rグラフを作る # １株当たり当期純利益又は当期純損失(いわゆるEPS)の変化を可視化して比較したいと思います。\nimport japanize_matplotlib import matplotlib.pyplot as plt import seaborn as sns def compare_company_IR(data, contextId, elementId, elementJpName): plot_data = data.query(f\u0026#34;要素ID==\u0026#39;{elementId}\u0026#39; and コンテキストID==\u0026#39;{contextId}\u0026#39;\u0026#34;).copy() plot_data[elementJpName] = pd.to_numeric(plot_data[\u0026#34;値\u0026#34;]) sns.barplot(data=plot_data, x=\u0026#34;会社名\u0026#34;, y=elementJpName) plt.ylabel(elementJpName) plt.show() compare_company_IR( all_data, \u0026#34;CurrentQuarterDuration\u0026#34;, \u0026#34;jpcrp_cor:BasicEarningsLossPerShareSummaryOfBusinessResults\u0026#34;, \u0026#34;EPS\u0026#34;, ) compare_company_IR(all_data, \u0026#34;CurrentYTDDuration\u0026#34;, \u0026#34;jppfs_cor:GrossProfit\u0026#34;, \u0026#34;粗利益\u0026#34;) XBRLファイルからデータを取得する # pip install edinet-xbrlにてライブラリをインストールし使用します。 参考文献：EDINETのXBRL用のPythonライブラリを作った - Parser編\nコンテキストIDを指定することで指定した時期のデータを取得できます。以下の__NUM__には0~9の値が入ります（ドキュメント）。\nPrior__NUM__YearDuration\rCurrentYearDuration\rPrior__NUM__YearInstant\rCurrentYearInstant\rCurrentQuarterDuration\rCurrentQuarterInstant\rPrior__NUM__QuarterDuration\rPrior__NUM__QuarterInstant\rCurrentYTDDuration\rPrior__NUM__YTDDuration\rInterimDuration\rInterimInstant\rPrior__NUM__InterimDuration\rPrior__NUM__InterimInstant for docID in docID_dict.values(): save_csv(docID, type=1) S100STH6のXBRLデータを取得中\rS100SS7PのXBRLデータを取得中\rS100STLSのXBRLデータを取得中\rS100SRTIのXBRLデータを取得中\rS100SP9OのXBRLデータを取得中\rfrom edinet_xbrl.edinet_xbrl_parser import EdinetXbrlParser parser = EdinetXbrlParser() edinet_xbrl_object = parser.parse_file( \u0026#34;S100QZLI/XBRL/PublicDoc/jpcrp030000-asr-001_E04236-000_2023-03-31_01_2023-06-20.xbrl\u0026#34; ) key = \u0026#34;jppfs_cor:Assets\u0026#34; context_ref = \u0026#34;Prior1YearInstant\u0026#34; current_year_assets = edinet_xbrl_object.get_data_by_context_ref( key, context_ref ).get_value() current_year_assets c:\\Users\\nanja-win-ms\\miniconda3\\envs\\py39\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\u0026quot;xml\u0026quot;` into the BeautifulSoup constructor.\rwarnings.warn(\r'2686701000000'\r配当の推移 # １株当たり配当額（DividendPaidPerShareSummaryOfBusinessResults）をプロットしてみます。\nkey = \u0026#34;jpcrp_cor:DividendPaidPerShareSummaryOfBusinessResults\u0026#34; edinet_xbrl_object = parser.parse_file( r\u0026#34;S100QZLI\\XBRL\\PublicDoc\\jpcrp030000-asr-001_E04236-000_2023-03-31_01_2023-06-20.xbrl\u0026#34; ) interimDividendPaidPerShares = [] context_refs = [ \u0026#34;Prior4YearDuration_NonConsolidatedMember\u0026#34;, \u0026#34;Prior3YearDuration_NonConsolidatedMember\u0026#34;, \u0026#34;Prior2YearDuration_NonConsolidatedMember\u0026#34;, \u0026#34;Prior1YearDuration_NonConsolidatedMember\u0026#34;, \u0026#34;CurrentYearDuration_NonConsolidatedMember\u0026#34;, ] for context_ref in context_refs: current_year_assets = edinet_xbrl_object.get_data_by_context_ref( key, context_ref ).get_value() interimDividendPaidPerShares.append(pd.to_numeric(current_year_assets)) plt.bar( [i for i in range(len(interimDividendPaidPerShares))], interimDividendPaidPerShares ) \u0026lt;BarContainer object of 5 artists\u0026gt;\r"},{"id":156,"href":"/finance/misc/jquants/","title":"J-Quants API","section":"その他","content":"\rJ-Quants APIを使ってみる # J-Quants jquants-api-client API仕様書 import os import time from datetime import datetime import jquantsapi import pandas as pd import requests from dateutil import tz REFRESH_TOKEN: str = os.environ.get(\u0026#34;JQ_REFRESH_TOKEN\u0026#34;) my_mail_address: str = os.environ.get(\u0026#34;JQ_MAIL_ADDRESS\u0026#34;) my_password: str = os.environ.get(\u0026#34;JQ_PASSWORD\u0026#34;) cli = jquantsapi.Client(mail_address=my_mail_address, password=my_password) df = cli.get_price_range( start_dt=datetime(2022, 7, 25, tzinfo=tz.gettz(\u0026#34;Asia/Tokyo\u0026#34;)), end_dt=datetime(2022, 7, 26, tzinfo=tz.gettz(\u0026#34;Asia/Tokyo\u0026#34;)), ) print(df) Date Code Open High Low Close UpperLimit LowerLimit \\\r0 2022-07-25 13010 3615.0 3660.0 3615.0 3630.0 0 0 0 2022-07-26 13010 3615.0 3640.0 3610.0 3635.0 0 0 1 2022-07-25 13050 2026.5 2037.0 2022.0 2023.0 0 0 1 2022-07-26 13050 2026.0 2029.5 2022.0 2023.5 0 0 2 2022-07-25 13060 2002.5 2015.0 2000.0 2001.0 0 0 ... ... ... ... ... ... ... ... ... 4191 2022-07-26 99950 403.0 404.0 402.0 404.0 0 0 4192 2022-07-25 99960 1274.0 1274.0 1263.0 1267.0 0 0 4192 2022-07-26 99960 1254.0 1266.0 1254.0 1255.0 0 0 4193 2022-07-25 99970 829.0 831.0 816.0 826.0 0 0 4193 2022-07-26 99970 826.0 827.0 816.0 825.0 0 0 Volume TurnoverValue AdjustmentFactor AdjustmentOpen \\\r0 8100.0 2.942050e+07 1.0 3615.0 0 8500.0 3.083550e+07 1.0 3615.0 1 54410.0 1.103787e+08 1.0 2026.5 1 22950.0 4.646586e+07 1.0 2026.0 2 943830.0 1.891360e+09 1.0 2002.5 ... ... ... ... ... 4191 13000.0 5.240900e+06 1.0 403.0 4192 1500.0 1.902700e+06 1.0 1274.0 4192 4000.0 5.021300e+06 1.0 1254.0 4193 151200.0 1.245601e+08 1.0 829.0 4193 133600.0 1.099946e+08 1.0 826.0 AdjustmentHigh AdjustmentLow AdjustmentClose AdjustmentVolume 0 3660.0 3615.0 3630.0 8100.0 0 3640.0 3610.0 3635.0 8500.0 1 2037.0 2022.0 2023.0 54410.0 1 2029.5 2022.0 2023.5 22950.0 2 2015.0 2000.0 2001.0 943830.0 ... ... ... ... ... 4191 404.0 402.0 404.0 13000.0 4192 1274.0 1263.0 1267.0 1500.0 4192 1266.0 1254.0 1255.0 4000.0 4193 831.0 816.0 826.0 151200.0 4193 827.0 816.0 825.0 133600.0 [8388 rows x 16 columns]\r上場銘柄一覧(/listed/info) # J-Quants API - listed info def get_listed_companies(idToken: str): \u0026#34;\u0026#34;\u0026#34;上場銘柄一覧を習得する Args: idToken (str): idToken Returns: listed_companies (pd.DataFrame): 上場銘柄が記録されたデータフレーム \u0026#34;\u0026#34;\u0026#34; r = requests.get( \u0026#34;https://api.jquants.com/v1/listed/info\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer {}\u0026#34;.format(idToken)}, ) if r.status_code == requests.codes.ok: listed_companies = pd.DataFrame(r.json()[\u0026#34;info\u0026#34;]).set_index(\u0026#34;Code\u0026#34;) return listed_companies else: return None r_post = requests.post( f\u0026#34;https://api.jquants.com/v1/token/auth_refresh?refreshtoken={REFRESH_TOKEN}\u0026#34; ) idToken = r_post.json()[\u0026#34;idToken\u0026#34;] listed_companies = get_listed_companies(idToken) listed_companies.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rpaint_companies = listed_companies[ listed_companies[\u0026#34;CompanyName\u0026#34;].str.contains(\u0026#34;塗料|ペイント\u0026#34;) ] paint_companies .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rセクターの分布 # 東証業種別株価指数・TOPIX-17シリーズ matplotlib.pyplot.pie — Matplotlib 3.8.3 documentation import japanize_matplotlib import matplotlib.pyplot as plt import seaborn as sns sector17_distribution = listed_companies[\u0026#34;Sector17CodeName\u0026#34;].value_counts() colors = sns.color_palette(\u0026#34;Set2\u0026#34;) plt.figure(figsize=(10, 10)) plt.pie( sector17_distribution, labels=sector17_distribution.index, colors=colors, autopct=\u0026#34;%.0f%%\u0026#34;, ) plt.show() 規模コード # 東証規模別株価指数 上場銘柄一覧(/listed/info) import plotly.express as px midcap_categories = [\u0026#34;TOPIX Mid400\u0026#34;, \u0026#34;TOPIX Large70\u0026#34;, \u0026#34;TOPIX Core30\u0026#34;] df = px.data.tips() fig = px.treemap( listed_companies[listed_companies[\u0026#34;ScaleCategory\u0026#34;].isin(midcap_categories)], path=[\u0026#34;Sector17CodeName\u0026#34;, \u0026#34;Sector33CodeName\u0026#34;], ) fig.show() df = px.data.tips() fig = px.treemap( listed_companies[listed_companies[\u0026#34;ScaleCategory\u0026#34;].isin(midcap_categories)], path=[\u0026#34;Sector17CodeName\u0026#34;, \u0026#34;Sector33CodeName\u0026#34;, \u0026#34;ScaleCategory\u0026#34;], ) fig.show() 財務情報の取得 # 財務情報(/fins/statements) Ⅰ．インターフェイス仕様 def get_statements(idToken, code): time.sleep(0.5) r = requests.get( f\u0026#34;https://api.jquants.com/v1/fins/statements?code={code}\u0026#34;, headers={\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer {}\u0026#34;.format(idToken)}, ) if r.status_code == requests.codes.ok: return r.json()[\u0026#34;statements\u0026#34;] else: return None res = get_statements(idToken, 86970) res [{'DisclosedDate': '2022-01-27',\r'DisclosedTime': '12:00:00',\r'LocalCode': '86970',\r'DisclosureNumber': '20220126573026',\r'TypeOfDocument': '3QFinancialStatements_Consolidated_IFRS',\r'TypeOfCurrentPeriod': '3Q',\r'CurrentPeriodStartDate': '2021-04-01',\r'CurrentPeriodEndDate': '2021-12-31',\r'CurrentFiscalYearStartDate': '2021-04-01',\r'CurrentFiscalYearEndDate': '2022-03-31',\r'NextFiscalYearStartDate': '',\r'NextFiscalYearEndDate': '',\r'NetSales': '100586000000',\r'OperatingProfit': '55967000000',\r'OrdinaryProfit': '',\r'Profit': '38013000000',\r'EarningsPerShare': '71.71',\r'DilutedEarningsPerShare': '',\r'TotalAssets': '62076519000000',\r'Equity': '311381000000',\r...\r'NonConsolidatedProfit': '',\r'NonConsolidatedEarningsPerShare': '',\r'NonConsolidatedTotalAssets': '',\r'NonConsolidatedEquity': '',\r'NonConsolidatedEquityToAssetRatio': '',\r'NonConsolidatedBookValuePerShare': '',\r'ForecastNonConsolidatedNetSales2ndQuarter': '',\r'ForecastNonConsolidatedOperatingProfit2ndQuarter': '',\r'ForecastNonConsolidatedOrdinaryProfit2ndQuarter': '',\r'ForecastNonConsolidatedProfit2ndQuarter': '',\r'ForecastNonConsolidatedEarningsPerShare2ndQuarter': '',\r'NextYearForecastNonConsolidatedNetSales2ndQuarter': '',\r'NextYearForecastNonConsolidatedOperatingProfit2ndQuarter': '',\r'NextYearForecastNonConsolidatedOrdinaryProfit2ndQuarter': '',\r'NextYearForecastNonConsolidatedProfit2ndQuarter': '',\r'NextYearForecastNonConsolidatedEarningsPerShare2ndQuarter': '',\r'ForecastNonConsolidatedNetSales': '',\r'ForecastNonConsolidatedOperatingProfit': '',\r'ForecastNonConsolidatedOrdinaryProfit': '',\r'ForecastNonConsolidatedProfit': '',\r'ForecastNonConsolidatedEarningsPerShare': '',\r'NextYearForecastNonConsolidatedNetSales': '',\r'NextYearForecastNonConsolidatedOperatingProfit': '',\r'NextYearForecastNonConsolidatedOrdinaryProfit': '',\r'NextYearForecastNonConsolidatedProfit': '',\r'NextYearForecastNonConsolidatedEarningsPerShare': ''}]\rpaint_companies_statements = pd.concat( [pd.DataFrame(get_statements(idToken, code)) for code in paint_companies.index] ) for c in paint_companies_statements.filter( regex=\u0026#34;Sales|Assets|CashFlows|Profit|Equity|EarningsPerShare\u0026#34; ).columns: paint_companies_statements[c] = pd.to_numeric(paint_companies_statements[c]) for c in paint_companies_statements.filter(regex=\u0026#34;Date\u0026#34;).columns: paint_companies_statements[c] = pd.to_datetime(paint_companies_statements[c]) sorted_data = paint_companies_statements.groupby([\u0026#34;LocalCode\u0026#34;]).apply( lambda x: x.sort_values([\u0026#34;DisclosureNumber\u0026#34;], ascending=False) ) sorted_data[\u0026#34;決算期\u0026#34;] = sorted_data.apply( lambda row: f\u0026#34;{row[\u0026#39;DisclosedDate\u0026#39;].year}-{row[\u0026#39;TypeOfCurrentPeriod\u0026#39;]}\u0026#34;, axis=1 ) sorted_data[\u0026#34;会社名\u0026#34;] = [ paint_companies.at[code, \u0026#34;CompanyName\u0026#34;] for code in sorted_data[\u0026#34;LocalCode\u0026#34;] ] sorted_data .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(20, 5)) hue_order = [ \u0026#34;2021-FY\u0026#34;, \u0026#34;2021-1Q\u0026#34;, \u0026#34;2021-2Q\u0026#34;, \u0026#34;2022-3Q\u0026#34;, \u0026#34;2022-FY\u0026#34;, \u0026#34;2022-1Q\u0026#34;, \u0026#34;2022-2Q\u0026#34;, \u0026#34;2023-3Q\u0026#34;, \u0026#34;2023-FY\u0026#34;, \u0026#34;2023-1Q\u0026#34;, \u0026#34;2023-2Q\u0026#34;, ] ax = sns.barplot( data=sorted_data, x=\u0026#34;会社名\u0026#34;, y=\u0026#34;EarningsPerShare\u0026#34;, hue=\u0026#34;決算期\u0026#34;, hue_order=hue_order, ) for c in ax.containers: ax.bar_label(c, rotation=90, fontsize=10) plt.xticks(rotation=90) plt.legend(loc=\u0026#34;upper left\u0026#34;, bbox_to_anchor=(1, 1)) plt.show() "},{"id":157,"href":"/finance/misc/edinet_chatgpt/","title":"ChatGPT","section":"その他","content":"\rChagGPTで決算書についての質問をしてみる # ChatGPTを使用する # import os from openai import OpenAI client = OpenAI( api_key=os.environ.get(\u0026#34;OPENAI_API_KEY\u0026#34;), ) response = client.chat.completions.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;あなたは日本企業の決算を分析するアナリストです。\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;日本郵船とはどのような会社ですか？\u0026#34;}, ], ) print(response.choices[0].message.content) 日本郵船株式会社（Nippon Yusen Kabushiki Kaisha、NYK Line）は、日本を代表する大手海運会社です。同社は、海上コンテナ輸送、液化天然ガス（LNG）船、タンカー、バルク船、自動車輸送船などの幅広い海運サービスを提供しています。また、同社は国際的な物流サービスも展開しており、陸上輸送や倉庫管理、カスタムズクリアランスなども提供しています。日本郵船は、日本の国際貿易や産業に不可欠な存在であり、世界的にも高い評価を受けている海運会社の一つです。\rEDINETを使用する # import os import time import zipfile import pandas as pd import requests API_ENDPOINT = \u0026#34;https://disclosure.edinet-fsa.go.jp/api/v2\u0026#34; # v2を使用する def save_csv(docID, type=5): \u0026#34;\u0026#34;\u0026#34;EDINETからデータを取得してフォルダに保存する Args: docID (str): DocID \u0026#34;\u0026#34;\u0026#34; assert type in [1, 2, 3, 4, 5], \u0026#34;typeの指定が間違っている\u0026#34; if type == 1: print(f\u0026#34;{docID}のXBRLデータを取得中\u0026#34;) elif type == 2: print(f\u0026#34;{docID}のpdfデータを取得中\u0026#34;) elif type in {3, 4}: print(f\u0026#34;{docID}のデータを取得中\u0026#34;) elif type == 5: print(f\u0026#34;{docID}のcsvデータを取得中\u0026#34;) time.sleep(5) r = requests.get( f\u0026#34;{API_ENDPOINT}/documents/{docID}\u0026#34;, { \u0026#34;type\u0026#34;: type, \u0026#34;Subscription-Key\u0026#34;: os.environ.get(\u0026#34;EDINET_API_KEY\u0026#34;), }, ) if r is None: print(\u0026#34;データの取得に失敗しました。csvFlag==1かどうか確認してください。\u0026#34;) else: os.makedirs(f\u0026#34;{docID}\u0026#34;, exist_ok=True) temp_zip = \u0026#34;uuid_89FD71B5_CD7B_4833-B30D‗5AA5006097E2.zip\u0026#34; with open(temp_zip, \u0026#34;wb\u0026#34;) as f: for chunk in r.iter_content(chunk_size=1024): f.write(chunk) with zipfile.ZipFile(temp_zip) as z: z.extractall(f\u0026#34;{docID}\u0026#34;) os.remove(temp_zip) 関西ペイントの『四半期報告書－第160期第3四半期(2023/10/01－2023/12/31) 』の書類を取得します。\nsave_csv(\u0026#34;S100SRKD\u0026#34;, type=1) S100SRKDのXBRLデータを取得中\rfrom edinet_xbrl.edinet_xbrl_parser import EdinetXbrlParser from IPython.display import HTML parser = EdinetXbrlParser() edinet_xbrl_object = parser.parse_file( r\u0026#34;S100SRKD\\XBRL\\PublicDoc\\jpcrp040300-q3r-001_E00893-000_2023-12-31_01_2024-02-09.xbrl\u0026#34; ) key = \u0026#34;jpcrp_cor:ManagementAnalysisOfFinancialPositionOperatingResultsAndCashFlowsTextBlock\u0026#34; context_ref = \u0026#34;FilingDateInstant\u0026#34; management_analysis_description = edinet_xbrl_object.get_data_by_context_ref( key, context_ref ).get_value() HTML(management_analysis_description.text) C:\\Users\\nanja-win-ms\\miniconda3\\envs\\py39\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\u0026quot;xml\u0026quot;` into the BeautifulSoup constructor.\rwarnings.warn(\rimport re def remove_tags(t): return re.compile(r\u0026#34;\u0026lt;.*?\u0026gt;|\\n|\\t\u0026#34;).sub(\u0026#34;\u0026#34;, t) text = management_analysis_description.get_text() remove_tags(text) '２【経営者による財政状態、経営成績及びキャッシュ・フローの状況の分析】\\u3000文中の将来に関する事項は、当四半期連結会計期間の末日現在において判断したものであります。(1）経営成績の状況\\u3000当期における世界経済は、供給制約が概ね解消されインフレ率も鈍化の傾向が見られるものの、地政学リスクは依然高まったままで基調的な物価上昇圧力は根強く、欧米を中心に金融引き締めが継続しており、その回復ペースは鈍化しております。そのような状況下、中国においては不動産市況の停滞の影響もありゼロコロナ政策解除後の景気回復は緩慢なペースにとどまっています。欧州においては物価高や利上げによる金融引き締めが景気を下押しする状況が継続しております。その他の地域においては、堅調な内需に支えられ景気は回復基調もしくは持ち直しの動きが見られました。わが国経済は、物価上昇や海外経済の回復ペースの鈍化などの影響を受けつつも、経済活動の正常化を背景に内需を中心に緩やかに持ち直しております。\\u3000当社グループの当第３四半期連結累計期間における売上高は4,222億94百万円（前年同期比10.3％増）となりました。営業利益は、人件費等の固定費の増加があったものの、原価低減や販売価格の改善などに取り組んだ結果、413億90百万円（前年同期比71.3％増）となりました。経常利益は為替差損や超インフレ会計による正味貨幣持高に係る損失の計上があったものの、持分法投資利益の増加などにより、441億89百万円（前年同期比53.4％増）となりました。親会社株主に帰属する四半期純利益は、政策保有株式縮減に伴う投資有価証券売却益やインドの土地売却に伴う固定資産売却益を計上したことなどにより、539億43百万円（前年同期比221.3％増）となりました。\\xa0\\u3000各セグメントの状況は以下のとおりであります。\\u3000なお、第１四半期連結会計期間より、当社グループの経営成績の評価等の検討に使用している主要な経営管理指標を、経常利益から営業利益及び持分法投資損益に変更したことに伴い、セグメント利益も経常利益から営業利益及び持分法投資損益に変更しております。この変更に伴い、前年第３四半期累計期間のセグメント利益も営業利益及び持分法投資損益に変更したうえで比較しております。\\xa0≪日本≫\\u3000自動車分野では自動車生産台数が前年を上回り、売上は前年を上回りました。工業分野、建築分野、自動車分野（補修用）及び防食分野では、市況は低調に推移するものの販売価格の改善に取り組んだことなどからトータルで売上は前年を上回りました。船舶分野では、外航船修繕向けの数量増加などにより売上は前年を上回りました。利益は一部の原材料価格が低下してきたことに加え、販売価格の改善に取り組んだことなどから前年を上回りました。\\u3000これらの結果、当セグメントの売上高は1,230億６百万円（前年同期8.7％増）、セグメント利益は164億７百万円（前年同期比99.4％増）となりました。\\xa0≪インド≫\\u3000建築分野では販売促進活動を推進するものの、モンスーンの影響による市況の低迷や競争の激化等の影響を受け、売上は前年並みとなりました。一方、自動車生産は安定しており販売価格の改善も寄与し、インド全体の売上は前年を上回りました。利益は、一部の原材料価格が低下してきたことに加え、販売価格の改善に継続して取り組んだことなどから前年を上回りました。\\u3000これらの結果、当セグメントの売上高は1,052億16百万円（前年同期比5.6％増）、セグメント利益は124億69百万円（前年同期比37.6％増）となりました。\\xa0≪欧州≫\\u3000トルコでは、自動車生産台数が前年を上回り、販売価格の改善に取り組んだこともあり、売上は前年を上回りました。その他欧州各国においては、主力の工業分野の売上が堅調に推移したことに加え、販売価格の改善などに取り組んだことにより、売上は前年を上回り、欧州全体としても前年を上回りました。利益はインフレの影響による人件費等のコストの増加があったものの、販売価格の改善に加え一部の原材料価格が低下してきたことなどにより、前年を上回りました。\\u3000これらの結果、当セグメントの売上高は1,028億25百万円（前年同期比22.2％増）、セグメント利益は39億53百万円（前年同期比123.6％増）となりました。\\xa0≪アジア≫\\u3000中国においては、自動車生産台数は前年を上回ったものの主要顧客の需要は伸び悩み、売上は前年を下回りました。タイ、マレーシア及びインドネシアにおいては、自動車生産の回復に加え、販売価格の改善の取り組みにより売上は前年を上回りました。利益は一部の原材料価格が低下してきたことに加え、持分法投資利益が増加したことで前年を上回りました。\\u3000これらの結果、当セグメントの売上高は534億16百万円（前年同期比6.0％増）、セグメント利益は87億96百万円（前年同期比65.4％増）となりました。\\xa0≪アフリカ≫\\u3000南アフリカ及び近隣諸国の経済は慢性的な電力不足や物価高などの影響で回復が遅れており需要が低迷するなか、工業分野の需要の取り込みや販売価格の改善などに取り組んだことにより、売上は前年を上回りました。東アフリカ地域においても、建築分野において拡販に注力して売上は堅調に推移し、アフリカ全体の売上は前年を上回りました。利益は安価品原材料への置換などコスト削減に取り組んだことにより、前年を上回りました。\\u3000これらの結果、当セグメントの売上高は310億49百万円（前年同期比1.9％増）、セグメント利益は28億62百万円（前年同期比32.4％増）となりました。\\xa0≪その他≫\\u3000北米では、自動車生産台数が前年を上回り、売上は前年を上回りました。利益については、売上の増加に伴い営業利益が改善したほか、持分法投資利益も増加したことなどにより、前年を上回りました。\\u3000これらの結果、当セグメントの売上高は67億80百万円（前年同期比31.5％増）、セグメント利益は21億66百万円（前年同期比137.9％増）となりました。\\xa0(2）優先的に対処すべき事業上及び財務上の課題\\u3000当第３四半期連結累計期間において、当社グループが優先的に対処すべき事業上及び財務上の課題について重要な変更はありません。\\xa0(3）研究開発活動\\u3000当第３四半期連結累計期間におけるグループ全体の研究開発活動の総額は、68億59百万円であります。\\u3000なお、当第３四半期連結累計期間において、当社グループの研究開発活動の状況に重要な変更はありません。\\xa0(4）経営成績に重要な影響を与える要因\\u3000当第３四半期連結累計期間において、経営成績に重要な影響を与える要因について、重要な変更はありません。\\xa0(5）資本の財源及び資金の流動性についての分析（財政状態の状況）① 流動資産\\u3000当第３四半期連結会計期間末における流動資産合計は、3,372億65百万円（前連結会計年度末比174億33百万円増）となりました。流動資産の増加は、現金及び預金などが減少したものの、主に受取手形、売掛金及び契約資産や有価証券などが増加したことによるものであります。② 固定資産\\u3000当第３四半期連結会計期間末における固定資産合計は、3,460億67百万円（前連結会計年度末比60億54百万円減）となりました。固定資産の減少は、有形固定資産や無形固定資産などが増加したものの、投資有価証券などが減少したことによるものであります。③ 流動負債\\u3000当第３四半期連結会計期間末における流動負債合計は、2,342億14百万円（前連結会計年度末比311億17百万円減）となりました。流動負債の減少は、主に短期借入金などが減少したことによるものであります。④ 固定負債\\u3000当第３四半期連結会計期間末における固定負債合計は、524億円（前連結会計年度末比12億２百万円減）となりました。⑤ 純資産\\u3000当第３四半期連結会計期間末における純資産合計は、3,967億18百万円（前連結会計年度末比436億97百万円増）となりました。\\xa0(6）経営方針・経営戦略、経営上の目標の達成状況を判断するための客観的な指標等\\u3000当社グループは、第３四半期連結累計期間において、経営上の目標の達成状況を判断するための客観的な指標等の見直しを行いました。\\u3000当社グループは、成長性と収益性の両立を図りながら、企業価値の向上を目指しております。第17次中期経営計画の最終年度である2024年度の目標として、連結売上高5,500億円、連結EBITDA850億円、調整後ROE13％超を設定しております。\\xa0\\xa0'\rresponse = client.chat.completions.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;あなたは日本企業の決算を分析するアナリストです。\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;以下の決算のコメントを要約してください。 {remove_tags(text)}\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;先ほどの決算においてインド市場についてコメントはしていましたか？\u0026#34;}, ], ) HTML(response.choices[0].message.content) はい、先ほどの決算においては、インド市場についてもコメントがありました。インド市場では、建築部門においては、モンスーンの影響や競争の激化などにより市況が低迷しているものの、自動車部門では安定した生産と販売価格の改善が売上を前年を上回る水準に押し上げたと述べられています。利益も一部の原材料価格の低下や販売価格の改善により前年を上回っており、インド市場においても一定の成長が見られたことが報告されています。\nfrom IPython.display import Markdown response = client.chat.completions.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;あなたは日本企業の決算を分析するアナリストです。\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;以下の決算のコメントを要約してください。 {remove_tags(text)}\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;インドでの業績が好調だった理由を箇条書きで３つ示してください。\u0026#34;}, ], ) display(Markdown(response.choices[0].message.content)) 自動車生産が安定しており、販売価格の改善が寄与した 一部の原材料価格の低下と販売価格の改善により利益が増加した 建築分野では販売促進活動の推進により売上が堅調に推移した key = \u0026#34;jpcrp_cor:QuarterlyConsolidatedBalanceSheetTextBlock\u0026#34; context_ref = \u0026#34;CurrentYTDDuration\u0026#34; balance_sheet = edinet_xbrl_object.get_data_by_context_ref(key, context_ref).get_value() HTML(balance_sheet) def remove_css(t): t = re.compile(r\u0026#39;style=\u0026#34;.*?\u0026#34;|\\n|\\t\u0026#39;).sub(\u0026#34;\u0026#34;, t) return re.compile(r\u0026#34;\u0026lt;p \u0026gt;|\u0026lt;/p\u0026gt;|\u0026lt;span \u0026gt;|\u0026lt;/span\u0026gt;\u0026#34;).sub(\u0026#34;\u0026#34;, t) response = client.chat.completions.create( model=\u0026#34;gpt-3.5-turbo\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;あなたは日本企業の決算を分析するアナリストです。\u0026#34;}, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;以下の貸借対照表について、2023年12月31日の純資産合計はいくらとなっていますか。 {remove_css(balance_sheet)}\u0026#34;, }, ], ) display(Markdown(response.choices[0].message.content)) 2023年12月31日の純資産合計は396,718百万円です。\n"},{"id":158,"href":"/finance/misc/","title":"その他","section":"経済データ","content":" おまけ # やってみた系のコード # "},{"id":159,"href":"/misc/","title":"その他","section":"トップページ","content":" Section 10 # その他 # ときどき使用するテクニックなどをメモする章\n"},{"id":160,"href":"/visualize/tips/","title":"Appendix","section":"可視化","content":" Tips # "},{"id":161,"href":"/others/issues/","title":"Issues","section":"Others","content":"誤表記、ミス、問題点を見つけた場合はお手数ですがこのページのコメント欄にご連絡お願いいたします。 またメールでのやり取りをご希望の方はプライバシーポリシーに記載のメールアドレスよりご連絡お願いいたします。\n"},{"id":162,"href":"/others/privacy-policy/","title":"Privacy Policy","section":"Others","content":"以下の「当Webサイト」「当サイト」などの文言は以下のページ全てを指しています。\nhttps://www.youtube.com/@K_DMにアップロードされた動画・文章 https://k-dm.work/のコンテンツ 当サイトで発信・投稿されている内容はあくまで個人的な見解であり、所属する組織・団体とは一切関係ございません。\nお問い合わせ先 # k▲datamining■gmail▲com\n※■を@、▲を.に置き換えてください。\n免責事項 # 当Webサイトに掲載されている情報の正確さには万全を期していますが、利用者が当Webサイトの情報を用いて行う一切の行為に関して、一切の責任を負わないものとします。 当Webサイトは、利用者が当社Webサイトを利用したことにより生じた利用者の損害及び利用者が第三者に与えた損害に関して、一切の責任を負わないものとします。 当Webサイト内の文章や画像、すべてのコンテンツは著作権・肖像権等により保護されています。 広告の配信・ログの利用について # Google Adsense # 当サイトは一部で第三者配信の広告サービスGoogle Adsense を利用しています。Googleを含む広告配信事業者は、Cookie を使用して、ユーザーがそのウェブサイトや他のウェブサイトに過去にアクセスした際の情報に基づいて広告を配信しています。\nこの目的は、ユーザーがそのサイトや他のサイトにアクセスした際の情報に基づいて、Google やそのパートナーが適切な広告をユーザーに表示するためです。\nCookieを無効にする設定およびGoogleアドセンスに関する詳細は「広告 – ポリシーと規約 – Google」をご覧ください。また、パーソナライズ広告を無効にする手続きは「広告設定ーGoogle」からお手続きください。なお、パーソナライズ広告に使われる第三者配信事業者の Cookie を無効にする手続きは「www.aboutads.info」から行うことができます。\nAmazonアソシエイトについて # 当サイトは、Amazon.co.jpを宣伝しリンクすることによってサイトが紹介料を獲得できる手段を提供することを目的に設定されたアフィリエイトプログラムである、Amazonアソシエイト・プログラムの参加者です。第三者がコンテンツおよび宣伝を提供し、訪問者から直接情報を収集し、訪問者のブラウザにCookieを設定したりこれを認識したりする場合があります。\nGoogleアナリティクス # 当サイトでは、Googleによるアクセス解析ツール「Googleアナリティクス」を利用しています。 このGoogleアナリティクスはトラフィックデータの収集のためにCookieを使用しています。このトラフィックデータは匿名で収集されており、個人を特定するものではありません。この機能はCookieを無効にすることで収集を拒否することが出来ますので、お使いのブラウザの設定をご確認ください。\n"}]