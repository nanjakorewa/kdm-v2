[{"id":0,"href":"/en/basic/anomaly/adtk1/","title":"Anomaly Detection ①","section":"Anomaly detection","content":"We will perform anomaly detection using the Anomaly Detection Toolkit (ADTK).\nThe original data is sourced from the Numenta Anomaly Benchmark.\nimport pandas as pd s_train = pd.read_csv( \u0026#34;./training.csv\u0026#34;, index_col=\u0026#34;timestamp\u0026#34;, parse_dates=True, squeeze=True ) from adtk.data import validate_series s_train = validate_series(s_train) print(s_train) timestamp\r2014-04-01 00:00:00 18.090486\r2014-04-01 00:05:00 20.359843\r2014-04-01 00:10:00 21.105470\r2014-04-01 00:15:00 21.151585\r2014-04-01 00:20:00 18.137141\r... 2014-04-14 23:35:00 18.269290\r2014-04-14 23:40:00 19.087351\r2014-04-14 23:45:00 19.594689\r2014-04-14 23:50:00 19.767817\r2014-04-14 23:55:00 20.479156\rFreq: 5T, Name: value, Length: 4032, dtype: float64\rs_train = pd.read_csv(\u0026quot;./training.csv\u0026quot;, index_col=\u0026quot;timestamp\u0026quot;, parse_dates=True, squeeze=True)\rfrom adtk.visualization import plot plot(s_train) Comparison of Anomaly Detection Methods # We will perform anomaly detection using SeasonalAD.\nFor other methods, refer to Detector.\nimport matplotlib.pyplot as plt from adtk.detector import ( AutoregressionAD, InterQuartileRangeAD, LevelShiftAD, PersistAD, SeasonalAD, ) model_dict = { \u0026#34;LevelShiftAD\u0026#34;: LevelShiftAD(window=5), \u0026#34;SeasonalAD\u0026#34;: SeasonalAD(), \u0026#34;PersistAD\u0026#34;: PersistAD(c=3.0, side=\u0026#34;positive\u0026#34;), \u0026#34;InterQuartileRangeAD\u0026#34;: InterQuartileRangeAD(c=1.5), \u0026#34;AutoregressionAD\u0026#34;: AutoregressionAD(n_steps=7 * 2, step_size=24, c=3.0), } for model_name, model in model_dict.items(): anomalies = model.fit_detect(s_train) plot(s_train, anomaly=anomalies, anomaly_color=\u0026#34;red\u0026#34;, anomaly_tag=\u0026#34;marker\u0026#34;) plt.title(model_name) plt.show() "},{"id":1,"href":"/en/prep/numerical/binning/","title":"Binning","section":"Numerical Data","content":"import numpy as np import pandas as pd df = pd.read_csv(\u0026#34;../data/sample.csv\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rBinning data at each quantile # pandas.qcut\nBinning based on how much of the data is X% of the total when sorted.\n0.0 46.0\r0.1 18002.9 \u0026lt;- 10%の値\r0.2 20476.8\r0.3 22755.0\r0.4 26204.8\r0.5 30824.0\r0.6 45622.6\r0.7 89873.9\r0.8 245544.0\r0.9 290714.1 \u0026lt;- 90%の値\r1.0 765403.0 df[\u0026#34;人口総数_ビン化\u0026#34;] = pd.qcut(df[\u0026#34;人口総数\u0026#34;], q=11) df[[\u0026#34;人口総数\u0026#34;, \u0026#34;人口総数_ビン化\u0026#34;]].head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":2,"href":"/en/timeseries/preprocess/001-check-data/","title":"Check Dataset","section":"Plotting and Preprocessing","content":" See what\u0026rsquo;s in the data # import datetime import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns from scipy import stats from statsmodels.tsa import stattools Reading a dataset from a csv file # data = pd.read_csv(\u0026#34;sample.csv\u0026#34;) data.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rSet timestamp to datetime # The Date column is currently read as an Object type, i.e., a string. To treat it as a timestamp, use the following datetime \u0026mdash; Basic Date and Time Types to convert it to a datetime type.\ndata[\u0026#34;Date\u0026#34;] = data[\u0026#34;Date\u0026#34;].apply( lambda x: datetime.datetime.strptime(str(x), \u0026#34;%Y-%m-%d\u0026#34;) ) print(f\u0026#34;Date column dtype: {data[\u0026#39;Date\u0026#39;].dtype}\u0026#34;) Date column dtype: datetime64[ns]\nGet an overview of a time series # pandas.DataFrame.describe # To begin, we briefly review what the data looks like. We will use pandas.DataFrame.describe to check some simple statistics for the Temp column.\ndata.describe() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rLine graph # Use seaborn.lineplot to see what the cycle looks like.\nplt.figure(figsize=(12, 6)) sns.lineplot(x=data[\u0026#34;Date\u0026#34;], y=data[\u0026#34;Temp\u0026#34;]) plt.ylabel(\u0026#34;Temp\u0026#34;) plt.grid(axis=\u0026#34;x\u0026#34;) plt.grid(axis=\u0026#34;y\u0026#34;, color=\u0026#34;r\u0026#34;, alpha=0.3) plt.show() Histogram # plt.figure(figsize=(12, 6)) plt.hist(x=data[\u0026#34;Temp\u0026#34;], rwidth=0.8) plt.xlabel(\u0026#34;Temp\u0026#34;) plt.ylabel(\u0026#34;日数\u0026#34;) plt.grid(axis=\u0026#34;y\u0026#34;) plt.show() Autocorrelation and Cholerograms # Using pandas.plotting.autocorrelation_plot Check autocorrelation to check the periodicity of time series data. Roughly speaking, autocorrelation is a measure of how well a signal matches a time-shifted signal of itself, expressed as a function of the magnitude of the time shift.\nplt.figure(figsize=(12, 6)) pd.plotting.autocorrelation_plot(data[\u0026#34;Temp\u0026#34;]) plt.grid() plt.axvline(x=365) plt.xlabel(\u0026#34;lag\u0026#34;) plt.ylabel(\u0026#34;autocorrelation\u0026#34;) plt.show() Unit Root Test # We check to see if the data are a unit root process. The Augmented Dickey-Fuller test is used to test the null hypothesis of a unit root process.\nstatsmodels.tsa.stattools.adfuller\nstattools.adfuller(data[\u0026#34;Temp\u0026#34;], autolag=\u0026#34;AIC\u0026#34;) (-4.444804924611697,\r0.00024708263003610177,\r20,\r3629,\r{'1%': -3.4321532327220154,\r'5%': -2.862336767636517,\r'10%': -2.56719413172842},\r16642.822304301197)\rChecking the trend # The trend line is drawn by fitting a one-dimensional polynomial to the time series. Since the data in this case is almost trend-stationary, there is almost no trend.\nnumpy.poly1d — NumPy v1.22 Manual\ndef get_trend(timeseries, deg=3): \u0026#34;\u0026#34;\u0026#34;Create a trend line for time-series data Args: timeseries(pd.Series) : time-series data Returns: pd.Series: trend line \u0026#34;\u0026#34;\u0026#34; x = list(range(len(timeseries))) y = timeseries.values coef = np.polyfit(x, y, deg) trend = np.poly1d(coef)(x) return pd.Series(data=trend, index=timeseries.index) data[\u0026#34;Trend\u0026#34;] = get_trend(data[\u0026#34;Temp\u0026#34;]) # グラフをプロット plt.figure(figsize=(12, 6)) sns.lineplot(x=data[\u0026#34;Date\u0026#34;], y=data[\u0026#34;Temp\u0026#34;], alpha=0.5, label=\u0026#34;Temp\u0026#34;) sns.lineplot(x=data[\u0026#34;Date\u0026#34;], y=data[\u0026#34;Trend\u0026#34;], label=\u0026#34;トレンド\u0026#34;) plt.grid(axis=\u0026#34;x\u0026#34;) plt.legend() plt.show() Supplement: If there is a clear trend # The green line is the trend line.\ndata_sub = data.copy() data_sub[\u0026#34;Temp\u0026#34;] = ( data_sub[\u0026#34;Temp\u0026#34;] + np.log(data_sub[\u0026#34;Date\u0026#34;].dt.year - 1980) * 10 ) # Dummy Trends data_sub[\u0026#34;Trend\u0026#34;] = get_trend(data_sub[\u0026#34;Temp\u0026#34;]) plt.figure(figsize=(12, 6)) sns.lineplot(x=data_sub[\u0026#34;Date\u0026#34;], y=data_sub[\u0026#34;Temp\u0026#34;], alpha=0.5, label=\u0026#34;Temp\u0026#34;) sns.lineplot(x=data_sub[\u0026#34;Date\u0026#34;], y=data_sub[\u0026#34;Trend\u0026#34;], label=\u0026#34;トレンド\u0026#34;) plt.grid(axis=\u0026#34;x\u0026#34;) plt.legend() plt.show() "},{"id":3,"href":"/en/eval/regression/correlation_coefficient/","title":"Correlation coefficient","section":"Regression","content":"Correlation coefficient measures the strength of a linear relationship between two data or random variables. It is an indicator that allows us to check whether there is a trend change of linear form between two variables, which can be expressed in the following equation.\n$ \\frac{\\Sigma_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\Sigma_{i=1}^N(x_i - \\bar{x})^2 \\Sigma_{i=1}^N(y_i - \\bar{y})^2 }} $\nIt has the following properties\n1 to less than 1 If correlation coefficient is close to 1, $x$ increases → $y$ also increases The value of correlation coefficient does not change when $x, y$ are multiplied by a low number Calculate the correlation coefficient between two numerical columns # import numpy as np np.random.seed(777) # to fix random numbers import matplotlib.pyplot as plt import numpy as np x = [xi + np.random.rand() for xi in np.linspace(0, 100, 40)] y = [yi + np.random.rand() for yi in np.linspace(1, 50, 40)] plt.figure(figsize=(5, 5)) plt.scatter(x, y) plt.show() coef = np.corrcoef(x, y) print(coef) [[1. 0.99979848]\r[0.99979848 1. ]]\rCollectively compute the correlation coefficient between multiple variables # pandas.io.formats.style.Styler.background_gradient\nimport seaborn as sns df = sns.load_dataset(\u0026#34;iris\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rCheck the CORRELATION COEFFICIENCES between all variables # Using the iris dataset, let\u0026rsquo;s look at the correlation between variables.\nThe Iris Dataset pandas.io.formats.style.Styler.background_gradient df.corr().style.background_gradient(cmap=\u0026#34;YlOrRd\u0026#34;) In the heatmap, it is hard to see where the correlation is highest. Check the bar chart to see which variables have the highest correlation with sepal_length.\npandas.DataFrame.plot.bar\ndf.corr()[\u0026#34;sepal_length\u0026#34;].plot.bar(grid=True, ylabel=\u0026#34;corr\u0026#34;) When correlation coefficient is low # Check the data distribution when the correlation coefficient is low and confirm that the correlation coefficient may be low even when there is a relationship between variables.\nnumpy.random.multivariate_normal — NumPy v1.22 Manual\nn_samples = 1000 plt.figure(figsize=(12, 12)) for i, ci in enumerate(np.linspace(-1, 1, 16)): ci = np.round(ci, 4) mean = np.array([0, 0]) cov = np.array([[1, ci], [ci, 1]]) v1, v2 = np.random.multivariate_normal(mean, cov, size=n_samples).T plt.subplot(4, 4, i + 1) plt.plot(v1, v2, \u0026#34;x\u0026#34;) plt.title(f\u0026#34;r={ci}\u0026#34;) plt.tight_layout() plt.show() In some cases, there is a relationship between variables even if the correlation coefficient is low. We will try to create such an example, albeit a simple one.\nimport japanize_matplotlib from sklearn import datasets japanize_matplotlib.japanize() n_samples = 1000 circle, _ = datasets.make_circles(n_samples=n_samples, factor=0.1, noise=0.05) moon, _ = datasets.make_moons(n_samples=n_samples, noise=0.05) corr_circle = np.round(np.corrcoef(circle[:, 0], circle[:, 1])[1, 0], 4) plt.title(f\u0026#34;correlation coefficient={corr_circle}\u0026#34;, fontsize=23) plt.scatter(circle[:, 0], circle[:, 1]) plt.show() corr_moon = np.round(np.corrcoef(moon[:, 0], moon[:, 1])[1, 0], 4) plt.title(f\u0026#34;correlation coefficient={corr_moon}\u0026#34;, fontsize=23) plt.scatter(moon[:, 0], moon[:, 1]) plt.show() "},{"id":4,"href":"/en/finance/visualize/country_risk/","title":"Country risk premium","section":"Visualize","content":"\rThe original data was imported from Country Default Spreads and Risk Premiums (Last updated: January 5, 2022).\n"},{"id":5,"href":"/en/basic/tree/decision_tree_classifier/","title":"Decision Tree (Classification)","section":"Decision Tree","content":" import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier, plot_tree Generate sample data # Generate sample data for 2-class classification.\nn_classes = 2 X, y = make_classification( n_samples=100, n_features=2, n_redundant=0, n_informative=2, random_state=2, n_classes=n_classes, n_clusters_per_class=1, ) Create a decision tree # Train the model with DecisionTreeClassifier(criterion=\u0026quot;gini\u0026quot;).fit(X, y) to visualize the decision boundaries of the created tree. The criterion=\u0026quot;gini\u0026quot; is an option to specify an indicator to determine the branching.\nsklearn.tree.DecisionTreeClassifier\n# Train Decision Tree Classifier clf = DecisionTreeClassifier(criterion=\u0026#34;gini\u0026#34;).fit(X, y) # Dataset for color map of decision boundary x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # Visualize decision boundaries plt.figure(figsize=(8, 8)) plt.tight_layout() cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1) plt.xlabel(\u0026#34;x1\u0026#34;) plt.ylabel(\u0026#34;x2\u0026#34;) # Separate color plots for each label for i, color, label_name in zip(range(n_classes), [\u0026#34;r\u0026#34;, \u0026#34;b\u0026#34;], [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;]): idx = np.where(y == i) plt.scatter(X[idx, 0], X[idx, 1], c=color, label=label_name, cmap=plt.cm.Pastel1) plt.legend() plt.show() Outputs decision tree structure as an image # sklearn.tree.plot_tree — scikit-learn 1.0.2 documentation\nplt.figure() clf = DecisionTreeClassifier(criterion=\u0026#34;gini\u0026#34;).fit(X, y) plt.figure(figsize=(12, 12)) plot_tree(clf, filled=True) plt.show() "},{"id":6,"href":"/en/timeseries/shape/001_dtw/","title":"Dynamic Time Warping","section":"Shape \u0026 Similarity","content":" import numpy as np import matplotlib.pyplot as plt from fastdtw import fastdtw Plot the two waveforms used in the experiment # data1 = [91.0 * np.sin(i / 2.1) for i in range(30)] data2 = [100.0 * np.sin(i / 2.0) + np.random.rand() for i in range(30)] data3 = [50.0 * np.cos(i / 2.0) + np.random.rand() for i in range(30)] plt.figure(figsize=(12, 4)) # Plotting Waveforms plt.plot(data1, label=\u0026#34;data1\u0026#34;, color=\u0026#34;k\u0026#34;) plt.plot(data2, label=\u0026#34;data2\u0026#34;, color=\u0026#34;r\u0026#34;) plt.plot(data3, label=\u0026#34;data3\u0026#34;, color=\u0026#34;b\u0026#34;) plt.legend() plt.show() Calculate DTW and plot the correspondence # We can determine that the DTW between the black and red waveforms shows smaller DTW and is more similar (on the measure of DTW) than the blue waveform.\n# Calculate DTW distance_12, path_12 = fastdtw(data1, data2) distance_13, path_13 = fastdtw(data1, data3) # Connect corresponding points with a line plt.figure(figsize=(12, 4)) for x_12, x_13 in zip(path_12, path_13): plt.plot(x_12, [data1[x_12[0]], data2[x_12[1]]], color=\u0026#34;r\u0026#34;, linestyle=\u0026#34;dotted\u0026#34;) plt.plot(x_13, [data1[x_13[0]], data3[x_13[1]]], color=\u0026#34;b\u0026#34;, linestyle=\u0026#34;dotted\u0026#34;) # Plotting Waveforms plt.plot(data1, label=\u0026#34;data1\u0026#34;, color=\u0026#34;k\u0026#34;) plt.plot(data2, label=\u0026#34;data2\u0026#34;, color=\u0026#34;r\u0026#34;) plt.plot(data3, label=\u0026#34;data2\u0026#34;, color=\u0026#34;b\u0026#34;) plt.legend() plt.title( f\u0026#34;DTW(data1, data2) {np.round(distance_12, 3)} \u0026lt; {np.round(distance_13, 3)} DTW(data1, data3)\u0026#34;, fontsize=14, ) plt.show() "},{"id":7,"href":"/en/finance/main/001-fred/","title":"FRED","section":"Time Series","content":"\rimport os import pandas as pd import seaborn as sns from full_fred.fred import Fred # FRED_API_KEY = os.getenv(\u0026#39;FRED_API_KEY\u0026#39;) fred = Fred() print(f\u0026#34;FRED API key is set to an environment variable: {fred.env_api_key_found()}\u0026#34;) def get_fred_data(name, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;): df = fred.get_series_df(name)[[\u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;]].copy() df[\u0026#34;date\u0026#34;] = pd.to_datetime(df[\u0026#34;date\u0026#34;]) df[\u0026#34;value\u0026#34;] = pd.to_numeric(df[\u0026#34;value\u0026#34;], errors=\u0026#34;coerce\u0026#34;) df = df.set_index(\u0026#34;date\u0026#34;) if end == \u0026#34;\u0026#34;: df = df.loc[f\u0026#34;{start}\u0026#34;:] else: df = df.loc[f\u0026#34;{start}\u0026#34;:f\u0026#34;{end}\u0026#34;] return df FRED API key is set to an environment variable:True\rUnemployment Rate # Original source：https://fred.stlouisfed.org/series/UNRATE\ndf_UNRATE = get_fred_data(\u0026#34;UNRATE\u0026#34;, start=\u0026#34;2010-01-01\u0026#34;) df_UNRATE.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rsns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) sns.lineplot(data=df_UNRATE, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;) ICE BofA US High Yield Index Option-Adjusted Spread # Source of data:https://fred.stlouisfed.org/series/BAMLH0A0HYM2\ndf_BAMLH0A0HYM2 = get_fred_data(\u0026#34;BAMLH0A0HYM2\u0026#34;, start=\u0026#34;2010-01-01\u0026#34;) df_BAMLH0A0HYM2.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rWilshire US Large-Cap Growth Total Market Index # Source of data:https://fred.stlouisfed.org/series/WILLLRGCAPGR\ndf_WILLLRGCAPGR = get_fred_data(\u0026#34;WILLLRGCAPGR\u0026#34;, start=\u0026#34;2010-01-01\u0026#34;) df_WILLLRGCAPGR.head(5) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rsns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) sns.lineplot(data=df_WILLLRGCAPGR, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;) sns.lineplot(data=df_BAMLH0A0HYM2 * 10000, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;) "},{"id":8,"href":"/en/visualize/distribution/histogram/","title":"Histogram","section":"Category \u0026 Number","content":"Similar to a density plot, it visualizes how numerical data is distributed.\nsns.histplot\nimport matplotlib.pyplot as plt import seaborn as sns df = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (12, 8)}) sns.histplot(data=df[\u0026#34;sepal_length\u0026#34;], binwidth=0.5) "},{"id":9,"href":"/en/install/","title":"Introduction","section":"Home","content":" Section 1 # What is this page? # This site is a page where _k (site creator) leaves a vlog and jupyter notebook of what he studied.\nHow to run code # Google Colaboratory(Colab)\rAnaconda\rPoetry\rGoogle Colaboratory(Colab) is the name of the Python runtime environment provided by Google that runs in the browser. Colab allows you to easily execute and share code without building any virtual environment. A Google account is required to use this service.\nClick on theColab button to run the code on that page on Google Colaboratory.\nAfter creating a virtual environment with Anaconda, install the necessary library packages for each notebook and code. Please note, however, this site does not use Anaconda at all and has not been tested.\nconda create -n py38_env python=3.8\rconda install *library_name* After installing POETRY in the manner described Poetry | Installation, run poetry install command.\n"},{"id":10,"href":"/en/basic/clustering/k-means1/","title":"k-means","section":"Clustering","content":" import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.cluster import KMeans from sklearn.datasets import make_blobs k-means clustering example # create sample data # Create sample data with make_blobs and apply clustering to it.\nn_samples = 1000 random_state = 117117 n_clusters = 4 X, y = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=1.5, centers=8 ) # Initial value of the centroid of each cluster of k-means init = X[np.where(X[:, 1] \u0026lt; -8)][:n_clusters] plt.figure(figsize=(8, 8)) plt.scatter(X[:, 0], X[:, 1], c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;training dataset\u0026#34;) plt.legend() plt.show() Centroid Calculation # We can re-compute the centroid four times and see that the data are cleanly separated at the fourth time.\nplt.figure(figsize=(12, 12)) for i in range(4): plt.subplot(2, 2, i + 1) max_iter = 1 + i km = KMeans( n_clusters=n_clusters, init=init, max_iter=max_iter, n_init=1, random_state=1 ).fit(X) cluster_centers = km.cluster_centers_ y_pred = km.predict(X) plt.title(f\u0026#34;max_iter={max_iter}\u0026#34;) plt.scatter(X[:, 0], X[:, 1], c=y_pred, marker=\u0026#34;x\u0026#34;) plt.scatter( cluster_centers[:, 0], cluster_centers[:, 1], c=\u0026#34;r\u0026#34;, marker=\u0026#34;o\u0026#34;, label=\u0026#34;Cluster centroid\u0026#34;, ) plt.tight_layout() plt.show() When clusters overlap # Check the clustering results when cluster data overlap.\nn_samples = 1000 random_state = 117117 plt.figure(figsize=(8, 8)) for i in range(4): cluster_std = (i + 1) * 1.5 X, y = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=cluster_std ) y_pred = KMeans(n_clusters=2, random_state=random_state, init=\u0026#34;random\u0026#34;).fit_predict( X ) plt.subplot(2, 2, i + 1) plt.title(f\u0026#34;cluster_std={cluster_std}\u0026#34;) plt.scatter(X[:, 0], X[:, 1], c=y_pred, marker=\u0026#34;x\u0026#34;) plt.tight_layout() plt.show() Influence of the number of k # n_samples = 1000 random_state = 117117 plt.figure(figsize=(8, 8)) for i in range(4): n_clusters = (i + 1) * 2 X, y = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=1, centers=5 ) y_pred = KMeans( n_clusters=n_clusters, random_state=random_state, init=\u0026#34;random\u0026#34; ).fit_predict(X) plt.subplot(2, 2, i + 1) plt.title(f\u0026#34;#cluster={n_clusters}\u0026#34;) plt.scatter(X[:, 0], X[:, 1], c=y_pred, marker=\u0026#34;x\u0026#34;) plt.tight_layout() plt.show() "},{"id":11,"href":"/en/basic/regression/linear_regression/","title":"Least-squares method","section":"Linear Regression","content":" import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler japanize_matplotlib is imported to display Japanese in the graph.\nCreate regression data for experiments # Use np.linspace to create data. It creates a list of values equally spaced between the values you specify. The following code creates 500 sample data for linear regression.\n# Training data n_samples = 500 X = np.linspace(-10, 10, n_samples)[:, np.newaxis] epsolon = np.random.normal(size=n_samples) y = np.linspace(-2, 2, n_samples) + epsolon # Visualize straight lines plt.figure(figsize=(10, 5)) plt.scatter(X, y, marker=\u0026#34;x\u0026#34;, label=\u0026#34;Target\u0026#34;, c=\u0026#34;orange\u0026#34;) plt.xlabel(\u0026#34;$x_1$\u0026#34;) plt.xlabel(\u0026#34;$y$\u0026#34;) plt.legend() plt.show() Check noise on y # About y = np.linspace(-2, 2, n_samples) + epsolon, I plot a histogram for epsolon. Confirm that noise with a distribution close to the normal distribution is on the target variable.\nplt.figure(figsize=(10, 5)) plt.hist(epsolon) plt.xlabel(\u0026#34;$\\epsilon$\u0026#34;) plt.ylabel(\u0026#34;#data\u0026#34;) plt.show() Fit a straight line by the least-squares method # sklearn.linear_model.LinearRegression\n# fit your model lin_r = make_pipeline(StandardScaler(with_mean=False), LinearRegression()).fit(X, y) y_pred = lin_r.predict(X) # Visualize straight lines plt.figure(figsize=(10, 5)) plt.scatter(X, y, marker=\u0026#34;x\u0026#34;, label=\u0026#34;target\u0026#34;, c=\u0026#34;orange\u0026#34;) plt.plot(X, y_pred, label=\u0026#34;Straight line fitted by linear regression\u0026#34;) plt.xlabel(\u0026#34;$x_1$\u0026#34;) plt.xlabel(\u0026#34;$y$\u0026#34;) plt.legend() plt.show() "},{"id":12,"href":"/en/basic/classification/logistic_regression/","title":"Logistic Regression","section":"Linear Classification","content":" import matplotlib.pyplot as plt import numpy as np Visualization of Logit Functions # fig = plt.figure(figsize=(4, 8)) p = np.linspace(0.01, 0.999, num=100) y = np.log(p / (1 - p)) plt.plot(p, y) plt.xlabel(\u0026#34;p\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.axhline(y=0, color=\u0026#34;k\u0026#34;) plt.ylim(-3, 3) plt.grid() plt.show() Logistic Regression # In this section, I try to train Logistic regression on artificially generated data.\nsklearn.linear_model.LogisticRegression sklearn.datasets.make_classification from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification ## Dataset X, Y = make_classification( n_samples=300, n_features=2, n_redundant=0, n_informative=1, random_state=2, n_clusters_per_class=1, ) # Training LogisticRegression clf = LogisticRegression() clf.fit(X, Y) # Find the decision boundary b = clf.intercept_[0] w1, w2 = clf.coef_.T b_1 = -w1 / w2 b_0 = -b / w2 xmin, xmax = np.min(X[:, 0]), np.max(X[:, 0]) ymin, ymax = np.min(X[:, 1]), np.max(X[:, 1]) xd = np.array([xmin, xmax]) yd = b_1 * xd + b_0 # Plot Graphs plt.figure(figsize=(10, 10)) plt.plot(xd, yd, \u0026#34;k\u0026#34;, lw=1, ls=\u0026#34;-\u0026#34;) plt.scatter(*X[Y == 0].T, marker=\u0026#34;o\u0026#34;) plt.scatter(*X[Y == 1].T, marker=\u0026#34;x\u0026#34;) plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.show() "},{"id":13,"href":"/en/visualize/category-groupby/japanmap/","title":"Map of Japan","section":"Numeric Value Distribution","content":" japanmap SaitoTsutomu/japanmap Visualization of data by prefecture (page with library author\u0026rsquo;s explanation of usage) This package is useful for creating heat maps, etc. with Japanese maps. It should run on python 3.9 or higher version.\nDisplay map of Japan # import matplotlib.pyplot as plt import numpy as np from japanmap import picture np.random.seed(77) plt.figure(figsize=(10, 10)) plt.xticks([]) plt.yticks([]) plt.imshow(picture()) Colorize japan map # For example, you can color a map of Japan by preparing the following dictionary with prefecture names as key and color names as value.\n{\u0026#39;北海道\u0026#39;: \u0026#39;#a9e5bb\u0026#39;, \u0026#39;青森\u0026#39;: \u0026#39;#fcf6b1\u0026#39;, \u0026#39;沖縄\u0026#39;: \u0026#39;#fcf6b1\u0026#39;} colors = [ \u0026#34;#e3170a\u0026#34;, \u0026#34;#a9e5bb\u0026#34;, \u0026#34;#fcf6b1\u0026#34;, \u0026#34;#f7b32b\u0026#34;, \u0026#34;#2d1e2f\u0026#34;, ] prefectures = [ \u0026#34;北海道\u0026#34;, \u0026#34;青森\u0026#34;, \u0026#34;岩手\u0026#34;, \u0026#34;宮城\u0026#34;, \u0026#34;秋田\u0026#34;, \u0026#34;山形\u0026#34;, \u0026#34;福島\u0026#34;, \u0026#34;茨城\u0026#34;, \u0026#34;栃木\u0026#34;, \u0026#34;群馬\u0026#34;, \u0026#34;埼玉\u0026#34;, \u0026#34;千葉\u0026#34;, \u0026#34;東京\u0026#34;, \u0026#34;神奈川\u0026#34;, \u0026#34;新潟\u0026#34;, \u0026#34;富山\u0026#34;, \u0026#34;石川\u0026#34;, \u0026#34;福井\u0026#34;, \u0026#34;山梨\u0026#34;, \u0026#34;長野\u0026#34;, \u0026#34;岐阜\u0026#34;, \u0026#34;静岡\u0026#34;, \u0026#34;愛知\u0026#34;, \u0026#34;三重\u0026#34;, \u0026#34;滋賀\u0026#34;, \u0026#34;京都\u0026#34;, \u0026#34;大阪\u0026#34;, \u0026#34;兵庫\u0026#34;, \u0026#34;奈良\u0026#34;, \u0026#34;和歌山\u0026#34;, \u0026#34;鳥取\u0026#34;, \u0026#34;島根\u0026#34;, \u0026#34;岡山\u0026#34;, \u0026#34;広島\u0026#34;, \u0026#34;山口\u0026#34;, \u0026#34;徳島\u0026#34;, \u0026#34;香川\u0026#34;, \u0026#34;愛媛\u0026#34;, \u0026#34;高知\u0026#34;, \u0026#34;福岡\u0026#34;, \u0026#34;佐賀\u0026#34;, \u0026#34;長崎\u0026#34;, \u0026#34;熊本\u0026#34;, \u0026#34;大分\u0026#34;, \u0026#34;宮崎\u0026#34;, \u0026#34;鹿児島\u0026#34;, \u0026#34;沖縄\u0026#34;, ] pref_color_dict = {prefecture: np.random.choice(colors) for prefecture in prefectures} plt.figure(figsize=(10, 10)) plt.xticks([]) plt.yticks([]) plt.imshow(picture(pref_color_dict)) print(f\u0026#34;入力：{pref_color_dict}\u0026#34;) 入力：{'北海道': '#2d1e2f', '青森': '#2d1e2f', '岩手': '#f7b32b', '宮城': '#e3170a', '秋田': '#e3170a', '山形': '#a9e5bb', '福島': '#2d1e2f', '茨城': '#f7b32b', '栃木': '#e3170a', '群馬': '#f7b32b', '埼玉': '#2d1e2f', '千葉': '#2d1e2f', '東京': '#fcf6b1', '神奈川': '#f7b32b', '新潟': '#f7b32b', '富山': '#a9e5bb', '石川': '#f7b32b', '福井': '#a9e5bb', '山梨': '#a9e5bb', '長野': '#2d1e2f', '岐阜': '#e3170a', '静岡': '#2d1e2f', '愛知': '#a9e5bb', '三重': '#e3170a', '滋賀': '#a9e5bb', '京都': '#fcf6b1', '大阪': '#f7b32b', '兵庫': '#2d1e2f', '奈良': '#f7b32b', '和歌山': '#a9e5bb', '鳥取': '#2d1e2f', '島根': '#e3170a', '岡山': '#2d1e2f', '広島': '#e3170a', '山口': '#a9e5bb', '徳島': '#f7b32b', '香川': '#2d1e2f', '愛媛': '#f7b32b', '高知': '#fcf6b1', '福岡': '#2d1e2f', '佐賀': '#2d1e2f', '長崎': '#2d1e2f', '熊本': '#fcf6b1', '大分': '#a9e5bb', '宮崎': '#fcf6b1', '鹿児島': '#a9e5bb', '沖縄': '#2d1e2f'}\rZoom in on a specific region only # You can also zoom in on a specific region of Japan (e.g. Kanto, Kansai, etc.).\nfrom japanmap import get_data, groups, pref_map print(f\u0026#34;groups: {groups}\u0026#34;) pref_map( groups[\u0026#34;近畿\u0026#34;], cols=[np.random.choice(colors) for _ in groups[\u0026#34;近畿\u0026#34;]], qpqo=get_data() ) groups: {'北海道': [1], '東北': [2, 3, 4, 5, 6, 7], '関東': [8, 9, 10, 11, 12, 13, 14], '中部': [15, 16, 17, 18, 19, 20, 21, 22, 23], '近畿': [24, 25, 26, 27, 28, 29, 30], '中国': [31, 32, 33, 34, 35], '四国': [36, 37, 38, 39], '九州': [40, 41, 42, 43, 44, 45, 46, 47]}\r"},{"id":14,"href":"/en/eval/model-selection/","title":"Model Selection","section":"Metrics","content":" Chapter 1 # Model Selection # "},{"id":15,"href":"/en/visualize/category-groupby/","title":"Numeric Value Distribution","section":"Visualization","content":" Chapter1 # Numeric Value Distribution # "},{"id":16,"href":"/en/basic/dimensionality_reduction/pca/","title":"PCA","section":"Dimensionality Reduction","content":" import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_blobs Dataset for experiments # sklearn.datasets.make_blobs\nX, y = make_blobs(n_samples=600, n_features=3, random_state=117117) fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\u0026#34;o\u0026#34;, c=y) ax.set_xlabel(\u0026#34;$x_1$\u0026#34;) ax.set_ylabel(\u0026#34;$x_2$\u0026#34;) ax.set_zlabel(\u0026#34;$x_3$\u0026#34;) Dimension reduction to two dimensions with PCA # sklearn.decomposition.PCA\nfrom sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler pca = PCA(n_components=2, whiten=True) X_pca = pca.fit_transform(StandardScaler().fit_transform(X)) fig = plt.figure(figsize=(8, 8)) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y) See the effect of normalization # Number of clusters 3, overlapping clusters # X, y = make_blobs( n_samples=200, n_features=3, random_state=11711, centers=3, cluster_std=2.0 ) X[:, 1] = X[:, 1] * 1000 X[:, 2] = X[:, 2] * 0.01 X_ss = StandardScaler().fit_transform(X) # Scatter plots of data before dimensionality reduction fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\u0026#34;o\u0026#34;, c=y) ax.set_xlabel(\u0026#34;$x_1$\u0026#34;) ax.set_ylabel(\u0026#34;$x_2$\u0026#34;) ax.set_zlabel(\u0026#34;$x_3$\u0026#34;) plt.title(\u0026#34;Scatter plots of data before PCA\u0026#34;) plt.show() # PCA without normalization pca = PCA(n_components=2).fit(X) X_pca = pca.transform(X) # PCA with normalization pca_ss = PCA(n_components=2).fit(X_ss) X_pca_ss = pca_ss.transform(X_ss) fig = plt.figure(figsize=(10, 5)) plt.subplot(121) plt.title(\u0026#34;without normalization\u0026#34;) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, marker=\u0026#34;x\u0026#34;, alpha=0.6) plt.subplot(122) plt.title(\u0026#34;with normalization\u0026#34;) plt.scatter(X_pca_ss[:, 0], X_pca_ss[:, 1], c=y, marker=\u0026#34;x\u0026#34;, alpha=0.6) "},{"id":17,"href":"/en/timeseries/preprocess/","title":"Plotting and Preprocessing","section":"TimeSeries","content":" Chapter 1 # Plotting and Preprocessing # "},{"id":18,"href":"/en/timeseries/forecast/001-prophet/","title":"Prophet","section":"timeseries forecast","content":""},{"id":19,"href":"/en/basic/ensemble/randomforest/","title":"Random Forests","section":"Ensemble","content":" sklearn.ensemble.RandomForestClassifier\nimport numpy as np import matplotlib.pyplot as plt Train Random Forests # For ROC-AUC, see ROC-AUC for an explanation of how to plot.\nsklearn.metrics.roc_auc_score sklearn.model_selection.train_test_split from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score n_features = 20 X, y = make_classification( n_samples=2500, n_features=n_features, n_informative=10, n_classes=2, n_redundant=0, n_clusters_per_class=4, random_state=777, ) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=777 ) model = RandomForestClassifier( n_estimators=50, max_depth=3, random_state=777, bootstrap=True, oob_score=True ) model.fit(X_train, y_train) y_pred = model.predict(X_test) rf_score = roc_auc_score(y_test, y_pred) print(f\u0026#34;ROC-AUC @ test dataset = {rf_score}\u0026#34;) ROC-AUC @ test dataset = 0.814573097628059\rCheck the performance of each tree in the random forest # import japanize_matplotlib estimator_scores = [] for i in range(10): estimator = model.estimators_[i] estimator_pred = estimator.predict(X_test) estimator_scores.append(roc_auc_score(y_test, estimator_pred)) plt.figure(figsize=(10, 4)) bar_index = [i for i in range(len(estimator_scores))] plt.bar(bar_index, estimator_scores) plt.bar([10], rf_score) plt.xticks(bar_index + [10], bar_index + [\u0026#34;RF\u0026#34;]) plt.xlabel(\u0026#34;tree index\u0026#34;) plt.ylabel(\u0026#34;ROC-AUC\u0026#34;) plt.show() Feature Importance # Importance based on impurity # plt.figure(figsize=(10, 4)) feature_index = [i for i in range(n_features)] plt.bar(feature_index, model.feature_importances_) plt.xlabel(\u0026#34;Feature Index\u0026#34;) plt.ylabel(\u0026#34;Feature Importance\u0026#34;) plt.show() permutation importance # permutation_importance\nfrom sklearn.inspection import permutation_importance p_imp = permutation_importance( model, X_train, y_train, n_repeats=10, random_state=77 ).importances_mean plt.figure(figsize=(10, 4)) plt.bar(feature_index, p_imp) plt.xlabel(\u0026#34;Feature Index\u0026#34;) plt.ylabel(\u0026#34;Feature Importance\u0026#34;) plt.show() Output each tree contained in the random forest # from sklearn.tree import export_graphviz from subprocess import call from IPython.display import Image from IPython.display import display for i in range(10): try: estimator = model.estimators_[i] export_graphviz( estimator, out_file=f\u0026#34;tree{i}.dot\u0026#34;, feature_names=[f\u0026#34;x{i}\u0026#34; for i in range(n_features)], class_names=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;], proportion=True, filled=True, ) call([\u0026#34;dot\u0026#34;, \u0026#34;-Tpng\u0026#34;, f\u0026#34;tree{i}.dot\u0026#34;, \u0026#34;-o\u0026#34;, f\u0026#34;tree{i}.png\u0026#34;, \u0026#34;-Gdpi=500\u0026#34;]) display(Image(filename=f\u0026#34;tree{i}.png\u0026#34;)) except KeyboardInterrupt: # TODO pass OOB (out-of-bag) Score # We can confirm that the OOB and test data results are close to each other. Compare the OOB accuracies with the test data while changing the random numbers and tree depth.\nfrom sklearn.metrics import accuracy_score for i in range(10): model_i = RandomForestClassifier( n_estimators=50, max_depth=3 + i % 2, random_state=i, bootstrap=True, oob_score=True, ) model_i.fit(X_train, y_train) y_pred = model_i.predict(X_test) oob_score = model_i.oob_score_ test_score = accuracy_score(y_test, y_pred) print(f\u0026#34;OOB＝{oob_score} test＝{test_score}\u0026#34;) OOB＝0.7868656716417910 test＝0.8121212121212121\rOOB＝0.8101492537313433 test＝0.8363636363636363\rOOB＝0.7886567164179105 test＝0.8024242424242424\rOOB＝0.8161194029850747 test＝0.8315151515151515\rOOB＝0.7910447761194029 test＝0.8072727272727273\rOOB＝0.8101492537313433 test＝0.833939393939394\rOOB＝0.7814925373134328 test＝0.8133333333333334\rOOB＝0.8059701492537313 test＝0.833939393939394\rOOB＝0.7832835820895523 test＝0.7951515151515152\rOOB＝0.8083582089552239 test＝0.8387878787878787\r"},{"id":20,"href":"/en/eval/classification/rocauc/roc-auc/","title":"ROC-AUC","section":"Classification","content":"The area under the ROC curve is called AUC (Area Under the Curve) and is used as an evaluation index for classification models; the best is when the AUC is 1, and 0.5 for random and totally invalid models.\nROC-AUC is a typical example of binary classification evaluation index 1 is the best, 0.5 is close to a completely random prediction Below 0.5 can be when the prediction is the opposite of the correct answer Plotting the ROC curve can help determine what the classification threshold should be import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_curve Plot ROC Curve # sklearn.metrics.roc_curve\nFunction to plot ROC Curve # def plot_roc_curve(test_y, pred_y): \u0026#34;\u0026#34;\u0026#34;Plot ROC Curve from correct answers and predictions Args: test_y (ndarray of shape (n_samples,)): y pred_y (ndarray of shape (n_samples,)): Predicted value for y \u0026#34;\u0026#34;\u0026#34; # False Positive Rate, True Positive Rate fprs, tprs, thresholds = roc_curve(test_y, pred_y) # plot ROC plt.figure(figsize=(8, 8)) plt.plot([0, 1], [0, 1], linestyle=\u0026#34;-\u0026#34;, c=\u0026#34;k\u0026#34;, alpha=0.2, label=\u0026#34;ROC-AUC=0.5\u0026#34;) plt.plot(fprs, tprs, color=\u0026#34;orange\u0026#34;, label=\u0026#34;ROC Curve\u0026#34;) plt.xlabel(\u0026#34;False Positive Rate\u0026#34;) plt.ylabel(\u0026#34;True Positive Rate\u0026#34;) # Fill in the area corresponding to the ROC-AUC score y_zeros = [0 for _ in tprs] plt.fill_between(fprs, y_zeros, tprs, color=\u0026#34;orange\u0026#34;, alpha=0.3, label=\u0026#34;ROC-AUC\u0026#34;) plt.legend() plt.show() Create a model and plot ROC Curve against sample data # X, y = make_classification( n_samples=1000, n_classes=2, n_informative=4, n_clusters_per_class=3, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) model = RandomForestClassifier(max_depth=5) model.fit(train_X, train_y) pred_y = model.predict_proba(test_X)[:, 1] plot_roc_curve(test_y, pred_y) Calculate ROC-AUC # sklearn.metrics.roc_auc_score\nfrom sklearn.metrics import roc_auc_score roc_auc_score(test_y, pred_y) 0.89069793083171\r"},{"id":21,"href":"/en/finance/nlp/001-sentiment-analysis/","title":"Sentiment analysis of text","section":"NLP","content":"We will use the model in『SiEBERT - English-Language Sentiment Classification』 to classify the sentiment of English sentences. I would like to classify the sentiment of each English sentence by positive and negative.\nHartmann, Jochen and Heitmann, Mark and Siebert, Christian and Schamp, Christina, \u0026ldquo;More than a feeling: Accuracy and Application of Sentiment Analysis\u0026rdquo;, International Journal of Research in Marketing(2022)\nHere we use the siebert/sentiment-roberta-large-english model on huggingface, if you want to use transformers on Google Colab you need to install transformers beforehand.\nimport numpy as np from transformers import pipeline from IPython.display import HTML sentiment_pipeline = pipeline( \u0026#34;sentiment-analysis\u0026#34;, model=\u0026#34;siebert/sentiment-roberta-large-english\u0026#34; ) Analyze the sentiment of each sentence in the text # The entire text is separated by \u0026ldquo;.\u0026rdquo; to separate each sentence.\nHere, Petrobras Webcast – 3rd Quarter Results 2022 November 5, 2022 transcription data is used.\ntranscript = \u0026#34;\u0026#34;\u0026#34;Hello!Hello!Hello!Hello!Hello!\u0026#34;\u0026#34;\u0026#34; ts_list = [ts for ts in transcript.split(\u0026#34;.\u0026#34;) if len(ts) \u0026gt; 20] scores = sentiment_pipeline(ts_list) Visualize the results # Visualize the results using positive and negative labels and their scores.\nfor t, s in zip(ts_list, scores): score = np.round(float(s[\u0026#34;score\u0026#34;]), 4) # sentiment score font_weight = \u0026#34;bold\u0026#34; if score \u0026gt; 0.995 else \u0026#34;normal\u0026#34; # thickness of the text # color display for each sentiment if s[\u0026#34;label\u0026#34;] == \u0026#34;NEGATIVE\u0026#34;: r = 255 - 10 * int(1000 - score * 1000) display( HTML( f\u0026#34;[score={score}] \u0026lt;span style=\u0026#39;color:rgb({r},100,100);font-weight:{font_weight};\u0026#39;\u0026gt;{t}\u0026lt;/span\u0026gt;\u0026#34; ) ) elif s[\u0026#34;label\u0026#34;] == \u0026#34;POSITIVE\u0026#34;: g = 255 - 10 * int(1000 - score * 1000) display( HTML( f\u0026#34;[score={score}] \u0026lt;span style=\u0026#39;color:rgb(100,{g},100);font-weight:{font_weight};\u0026#39;\u0026gt;{t}\u0026lt;/span\u0026gt;\u0026#34; ) ) [score=0.9976] Hello!Hello!Hello!Hello!Hello!\n"},{"id":22,"href":"/en/finance/main/","title":"Time Series","section":"Economic Data","content":" Chapter 1 # Time Series # "},{"id":23,"href":"/en/basic/timeseries/prophet/","title":"Using Prophet","section":"Time Series","content":"Please refer to \u0026ldquo;prophet Installation\u0026rdquo; for prophet installation instructions. Also, please refer to Quick Start (prophet | Quick Start) in the official documentation.\nK_DM - Timeseries \u0026gt; forecast \u0026gt; Prophet\nCreate time series data # Create dummy time series data.\nimport numpy as np import pandas as pd import seaborn as sns from prophet import Prophet sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (15, 8)}) date = pd.date_range(\u0026#34;2020-01-01\u0026#34;, periods=365, freq=\u0026#34;D\u0026#34;) y = [np.cos(di.weekday()) + di.month % 2 + np.log(i + 1) for i, di in enumerate(date)] df = pd.DataFrame({\u0026#34;ds\u0026#34;: date, \u0026#34;y\u0026#34;: y}) df.index = date sns.lineplot(data=df) Train Prophet # m = Prophet(yearly_seasonality=False, daily_seasonality=True) m.fit(df) Initial log joint probability = -24.5101\r\u0026lt;prophet.forecaster.Prophet at 0x7fe3b5d93d60\u0026gt;\rIter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 99 798.528 0.00821602 204.832 1 1 136 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 136 799.486 0.00040141 83.1101 5.02e-06 0.001 225 LS failed, Hessian reset 158 799.529 0.00027729 48.4168 3.528e-06 0.001 291 LS failed, Hessian reset 199 799.55 3.15651e-05 54.5691 1 1 345 Iter log prob ||dx|| ||grad|| alpha alpha0 # evals Notes 204 799.553 3.54297e-05 56.7445 5.36e-07 0.001 397 LS failed, Hessian reset 267 799.556 6.19351e-08 44.7029 0.2081 1 490 Optimization terminated normally: Convergence detected: relative gradient magnitude is below tolerance\rCreate data for forecasts and run forecasts # future = m.make_future_dataframe(periods=90) forecast = m.predict(future) fig1 = m.plot(forecast) "},{"id":24,"href":"/en/timeseries/exponential_smoothing/001-simple-es/","title":"単純指数平滑化","section":"Exponential smoothing","content":"import japanize_matplotlib as jm import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from statsmodels.tsa.holtwinters import SimpleExpSmoothing jm.japanize() サンプルデータ # Natural Gas Consumption (NATURALGAS)のデータを使用しています。数値の単位はBCF（10億立方フィート）です。\ndata = { \u0026#34;value\u0026#34;: { \u0026#34;2013-01-01\u0026#34;: 2878.8, \u0026#34;2013-02-01\u0026#34;: 2567.2, \u0026#34;2013-03-01\u0026#34;: 2521.1, \u0026#34;2013-04-01\u0026#34;: 1967.5, \u0026#34;2013-05-01\u0026#34;: 1752.5, \u0026#34;2013-06-01\u0026#34;: 1742.9, \u0026#34;2013-07-01\u0026#34;: 1926.3, \u0026#34;2013-08-01\u0026#34;: 1927.4, \u0026#34;2013-09-01\u0026#34;: 1767.0, \u0026#34;2013-10-01\u0026#34;: 1866.8, \u0026#34;2013-11-01\u0026#34;: 2316.9, \u0026#34;2013-12-01\u0026#34;: 2920.8, \u0026#34;2014-01-01\u0026#34;: 3204.1, \u0026#34;2014-02-01\u0026#34;: 2741.2, \u0026#34;2014-03-01\u0026#34;: 2557.9, \u0026#34;2014-04-01\u0026#34;: 1961.7, \u0026#34;2014-05-01\u0026#34;: 1810.2, \u0026#34;2014-06-01\u0026#34;: 1745.4, \u0026#34;2014-07-01\u0026#34;: 1881.0, \u0026#34;2014-08-01\u0026#34;: 1933.1, \u0026#34;2014-09-01\u0026#34;: 1809.3, \u0026#34;2014-10-01\u0026#34;: 1912.8, \u0026#34;2014-11-01\u0026#34;: 2357.5, \u0026#34;2014-12-01\u0026#34;: 2679.2, \u0026#34;2015-01-01\u0026#34;: 3115.0, \u0026#34;2015-02-01\u0026#34;: 2925.2, \u0026#34;2015-03-01\u0026#34;: 2591.3, \u0026#34;2015-04-01\u0026#34;: 2007.9, \u0026#34;2015-05-01\u0026#34;: 1858.2, \u0026#34;2015-06-01\u0026#34;: 1899.9, \u0026#34;2015-07-01\u0026#34;: 2067.7, \u0026#34;2015-08-01\u0026#34;: 2052.7, \u0026#34;2015-09-01\u0026#34;: 1901.3, \u0026#34;2015-10-01\u0026#34;: 1987.3, \u0026#34;2015-11-01\u0026#34;: 2249.1, \u0026#34;2015-12-01\u0026#34;: 2588.2, \u0026#34;2016-01-01\u0026#34;: 3091.7, \u0026#34;2016-02-01\u0026#34;: 2652.3, \u0026#34;2016-03-01\u0026#34;: 2356.3, \u0026#34;2016-04-01\u0026#34;: 2083.9, \u0026#34;2016-05-01\u0026#34;: 1965.8, \u0026#34;2016-06-01\u0026#34;: 2000.7, \u0026#34;2016-07-01\u0026#34;: 2186.6, \u0026#34;2016-08-01\u0026#34;: 2208.4, \u0026#34;2016-09-01\u0026#34;: 1947.8, \u0026#34;2016-10-01\u0026#34;: 1925.2, \u0026#34;2016-11-01\u0026#34;: 2159.5, \u0026#34;2016-12-01\u0026#34;: 2866.3, \u0026#34;2017-01-01\u0026#34;: 2913.8, \u0026#34;2017-02-01\u0026#34;: 2340.2, \u0026#34;2017-03-01\u0026#34;: 2523.3, \u0026#34;2017-04-01\u0026#34;: 1932.0, \u0026#34;2017-05-01\u0026#34;: 1892.0, \u0026#34;2017-06-01\u0026#34;: 1910.4, \u0026#34;2017-07-01\u0026#34;: 2141.6, \u0026#34;2017-08-01\u0026#34;: 2093.8, \u0026#34;2017-09-01\u0026#34;: 1920.5, \u0026#34;2017-10-01\u0026#34;: 2031.5, \u0026#34;2017-11-01\u0026#34;: 2357.3, \u0026#34;2017-12-01\u0026#34;: 3086.0, \u0026#34;2018-01-01\u0026#34;: 3340.9, \u0026#34;2018-02-01\u0026#34;: 2710.7, \u0026#34;2018-03-01\u0026#34;: 2796.7, \u0026#34;2018-04-01\u0026#34;: 2350.5, \u0026#34;2018-05-01\u0026#34;: 2055.0, \u0026#34;2018-06-01\u0026#34;: 2063.1, \u0026#34;2018-07-01\u0026#34;: 2350.7, \u0026#34;2018-08-01\u0026#34;: 2313.8, \u0026#34;2018-09-01\u0026#34;: 2156.1, \u0026#34;2018-10-01\u0026#34;: 2285.9, \u0026#34;2018-11-01\u0026#34;: 2715.9, \u0026#34;2018-12-01\u0026#34;: 2999.5, \u0026#34;2019-01-01\u0026#34;: 3424.3, \u0026#34;2019-02-01\u0026#34;: 3019.1, \u0026#34;2019-03-01\u0026#34;: 2927.8, \u0026#34;2019-04-01\u0026#34;: 2212.4, \u0026#34;2019-05-01\u0026#34;: 2134.0, \u0026#34;2019-06-01\u0026#34;: 2119.3, \u0026#34;2019-07-01\u0026#34;: 2393.9, \u0026#34;2019-08-01\u0026#34;: 2433.9, \u0026#34;2019-09-01\u0026#34;: 2206.3, \u0026#34;2019-10-01\u0026#34;: 2306.5, \u0026#34;2019-11-01\u0026#34;: 2783.8, \u0026#34;2019-12-01\u0026#34;: 3170.7, \u0026#34;2020-01-01\u0026#34;: 3320.6, \u0026#34;2020-02-01\u0026#34;: 3058.5, \u0026#34;2020-03-01\u0026#34;: 2722.0, \u0026#34;2020-04-01\u0026#34;: 2256.9, \u0026#34;2020-05-01\u0026#34;: 2072.2, \u0026#34;2020-06-01\u0026#34;: 2127.9, \u0026#34;2020-07-01\u0026#34;: 2464.1, \u0026#34;2020-08-01\u0026#34;: 2399.5, \u0026#34;2020-09-01\u0026#34;: 2151.2, \u0026#34;2020-10-01\u0026#34;: 2315.9, \u0026#34;2020-11-01\u0026#34;: 2442.0, \u0026#34;2020-12-01\u0026#34;: 3182.8, \u0026#34;2021-01-01\u0026#34;: 3343.9, \u0026#34;2021-02-01\u0026#34;: 3099.2, \u0026#34;2021-03-01\u0026#34;: 2649.4, \u0026#34;2021-04-01\u0026#34;: 2265.1, \u0026#34;2021-05-01\u0026#34;: 2117.4, \u0026#34;2021-06-01\u0026#34;: 2238.4, \u0026#34;2021-07-01\u0026#34;: 2412.2, \u0026#34;2021-08-01\u0026#34;: 2433.8, \u0026#34;2021-09-01\u0026#34;: 2142.3, \u0026#34;2021-10-01\u0026#34;: 2262.6, \u0026#34;2021-11-01\u0026#34;: 2693.3, \u0026#34;2021-12-01\u0026#34;: 3007.3, \u0026#34;2022-01-01\u0026#34;: 3612.1, \u0026#34;2022-02-01\u0026#34;: 3064.2, \u0026#34;2022-03-01\u0026#34;: 2785.4, \u0026#34;2022-04-01\u0026#34;: 2379.3, \u0026#34;2022-05-01\u0026#34;: 2247.8, \u0026#34;2022-06-01\u0026#34;: 2326.9, \u0026#34;2022-07-01\u0026#34;: 2597.9, \u0026#34;2022-08-01\u0026#34;: 2566.1, \u0026#34;2022-09-01\u0026#34;: 2263.3, } } data = pd.DataFrame(data) data.rename(columns={\u0026#34;value\u0026#34;: \u0026#34;Natural Gas Consumption(BCF)\u0026#34;}, inplace=True) data.index = pd.to_datetime(data.index) data = data.asfreq(\u0026#34;MS\u0026#34;) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data) plt.grid() plt.show() データの分割 # データの末尾（2020年以降のデータ）を予測性能の検証に使用するためにデータを分割します。\ndata_train = data[data.index \u0026lt; \u0026#34;2020-1-1\u0026#34;] data_test = data[data.index \u0026gt;= \u0026#34;2020-1-1\u0026#34;] plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.legend() plt.grid() モデルの学習と予測 # ses = SimpleExpSmoothing(data_train) ses = ses.fit(smoothing_level=0.1) pred = ses.forecast(33) plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.plot(pred.index, pred.values, \u0026#34;-x\u0026#34;, label=\u0026#34;pred\u0026#34;) plt.legend() plt.grid() "},{"id":25,"href":"/en/basic/anomaly/adtk2/","title":"Anomaly Detection②","section":"Anomaly detection","content":"Let\u0026rsquo;s try anomaly detection using the Anomaly Detection Toolkit (ADTK).\nWe will apply anomaly detection to multidimensional synthetic data. This time, we will work with data across multiple dimensions.\nimport numpy as np import pandas as pd from adtk.data import validate_series s_train = pd.read_csv(\u0026#34;./training.csv\u0026#34;, index_col=\u0026#34;timestamp\u0026#34;, parse_dates=True) s_train = validate_series(s_train) s_train[\u0026#34;value2\u0026#34;] = s_train[\u0026#34;value\u0026#34;].apply(lambda v: np.sin(v) + np.cos(v)) s_train .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rfrom adtk.visualization import plot plot(s_train) Comparison of Anomaly Detection Methods # We will perform anomaly detection using SeasonalAD.\nFor other methods, refer to Detector.\nimport matplotlib.pyplot as plt from adtk.detector import OutlierDetector, PcaAD, RegressionAD from sklearn.linear_model import LinearRegression from sklearn.neighbors import LocalOutlierFactor model_dict = { \u0026#34;OutlierDetector\u0026#34;: OutlierDetector(LocalOutlierFactor(contamination=0.05)), \u0026#34;RegressionAD\u0026#34;: RegressionAD(regressor=LinearRegression(), target=\u0026#34;value2\u0026#34;, c=3.0), \u0026#34;PcaAD\u0026#34;: PcaAD(k=2), } for model_name, model in model_dict.items(): anomalies = model.fit_detect(s_train) plot( s_train, anomaly=anomalies, ts_linewidth=1, ts_markersize=3, anomaly_color=\u0026#34;red\u0026#34;, anomaly_alpha=0.3, curve_group=\u0026#34;all\u0026#34;, ) plt.title(model_name) plt.show() "},{"id":26,"href":"/en/prep/numerical/boxcox/","title":"BoxCox transformation","section":"Numerical Data","content":" scipy.stats.boxcox\nThe following transformation is applied to the numbers to make the shape of the distribution closer to a normal distribution.\n$ y = \\begin{cases} \\displaystyle \\frac{x^\\lambda - 1}{\\lambda} \u0026amp; \\lambda \\neq 0\\ \\log x \u0026amp; \\lambda = 0\\end{cases} $\nFrom the form of the equation, $x$ must always take on non-negative values to apply this transformation to numerical data. If negative values are contained, one might add a constant or use the YeoJonson transformation to make everything greater than 0.\nfrom scipy import stats import matplotlib.pyplot as plt x = stats.loggamma.rvs(1, size=1000) + 10 plt.hist(x) plt.show() import numpy as np from scipy.stats import boxcox plt.hist(boxcox(x)) plt.show() "},{"id":27,"href":"/en/visualize/distribution/","title":"Category \u0026 Number","section":"Visualization","content":" Chapter2 # Category \u0026amp; Number # "},{"id":28,"href":"/en/eval/regression/r2/","title":"Coefficient of determination","section":"Regression","content":"The coefficient of determination is a value in statistics that expresses how much of the dependent variable (objective variable) is explained by the independent variable (explanatory variable).\nGenerally, the higher the better the rating indicator The best case is 1. However, the more features you add, the higher the score tends to be. Therefore, it is not possible to judge \u0026ldquo;high accuracy of the model\u0026rdquo; by looking at this indicator alone import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_regression from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split Create models and calculate coefficients of determination for sample data # First, let\u0026rsquo;s create data that makes predictions more likely to be correct.\nX, y = make_regression( n_samples=1000, n_informative=3, n_features=20, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) model = RandomForestRegressor(max_depth=5) model.fit(train_X, train_y) pred_y = model.predict(test_X) Calculate coefficient of determination # sklearn.metrics.roc_auc_score\nfrom sklearn.metrics import r2_score r2 = r2_score(test_y, pred_y) y_min, y_max = np.min(test_y), np.max(test_y) plt.figure(figsize=(6, 6)) plt.title(f\u0026#34;$R^2 =${r2}\u0026#34;) plt.plot([y_min, y_max], [y_min, y_max], linestyle=\u0026#34;-\u0026#34;, c=\u0026#34;k\u0026#34;, alpha=0.2) plt.scatter(test_y, pred_y, marker=\u0026#34;x\u0026#34;) plt.xlabel(\u0026#34;target\u0026#34;) plt.ylabel(\u0026#34;prediction\u0026#34;) Next, create data for which the predictions are less likely to be correct and check to see that the coefficient of determination falls.\nX, y = make_regression( n_samples=1000, n_informative=3, n_features=20, effective_rank=4, noise=1.5, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) model = RandomForestRegressor(max_depth=5) model.fit(train_X, train_y) pred_y = model.predict(test_X) r2 = r2_score(test_y, pred_y) y_min, y_max = np.min(test_y), np.max(test_y) plt.figure(figsize=(6, 6)) plt.title(f\u0026#34;$R^2 =${r2}\u0026#34;) plt.plot([y_min, y_max], [y_min, y_max], linestyle=\u0026#34;-\u0026#34;, c=\u0026#34;k\u0026#34;, alpha=0.2) plt.scatter(test_y, pred_y, marker=\u0026#34;x\u0026#34;) plt.xlabel(\u0026#34;target\u0026#34;) plt.ylabel(\u0026#34;prediction\u0026#34;) When the prediction is almost random # When the accuracy is even worse than simply predicting the average, the coefficient of determination is negative.\nX, y = make_regression( n_samples=1000, n_informative=3, n_features=20, effective_rank=4, noise=1.5, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) # Randomly reorder train_y and convert values train_y = np.random.permutation(train_y) train_y = np.sin(train_y) * 10 + 1 model = RandomForestRegressor(max_depth=1) model.fit(train_X, train_y) pred_y = model.predict(test_X) r2 = r2_score(test_y, pred_y) y_min, y_max = np.min(test_y), np.max(test_y) plt.figure(figsize=(6, 6)) plt.title(f\u0026#34;$R^2 =${r2}\u0026#34;) plt.plot([y_min, y_max], [y_min, y_max], linestyle=\u0026#34;-\u0026#34;, c=\u0026#34;k\u0026#34;, alpha=0.2) plt.scatter(test_y, pred_y, marker=\u0026#34;x\u0026#34;) plt.xlabel(\u0026#34;target\u0026#34;) plt.ylabel(\u0026#34;prediction\u0026#34;) Coefficient of determination when using the least squares method # In the case of a regression line for a single regression using the least squares method, the range of the coefficient of determination is $ 0 \\le R^2 \\le 1$. Let us try to find the coefficient of determination by running a 100-line regression with random noise on the data.\nfrom sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler r2_scores = [] for i in range(100): X, y = make_regression( n_samples=500, n_informative=1, n_features=1, effective_rank=4, noise=i * 0.1, random_state=RND, ) train_X, test_X, train_y, test_y = train_test_split( X, y, test_size=0.33, random_state=RND ) # linear regression model = make_pipeline( StandardScaler(with_mean=False), LinearRegression(positive=True) ).fit(train_X, train_y) # Calculate coefficient of determination pred_y = model.predict(test_X) r2 = r2_score(test_y, pred_y) r2_scores.append(r2) plt.figure(figsize=(8, 4)) plt.hist(r2_scores, bins=20) plt.show() "},{"id":29,"href":"/en/basic/tree/decision_tree_regressor/","title":"Decision tree (regression)","section":"Decision Tree","content":" from sklearn.tree import DecisionTreeRegressor from sklearn.datasets import make_regression from dtreeviz.trees import * import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from dtreeviz.trees import dtreeviz dtreeviz : Decision Tree Visualization\nGenerate sample data for decision trees # X, y = make_regression(n_samples=100, n_features=2, random_state=777) plt.figure(figsize=(10, 10)) plt.scatter(X[:, 0], X[:, 1], c=y) plt.show() Check how the regression tree branches. # tree = DecisionTreeRegressor(max_depth=3, random_state=117117) model = tree.fit(X, y) viz = dtreeviz(tree, X, y, target_name=\u0026#34;y\u0026#34;) viz.save(\u0026#34;./regression_tree.svg\u0026#34;) Visualize the branching of a regression tree # from IPython.display import SVG SVG(filename=\u0026#34;regression_tree.svg\u0026#34;) "},{"id":30,"href":"/en/visualize/distribution/densityplot/","title":"Density plot","section":"Category \u0026 Number","content":"Visualize how numerical data is distributed.\nseaborn.kdeplot\nimport matplotlib.pyplot as plt import seaborn as sns df = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (12, 8)}) sns.kdeplot(df[\u0026#34;sepal_length\u0026#34;]) sns.kdeplot(df[\u0026#34;sepal_width\u0026#34;]) sns.kdeplot(df[\u0026#34;petal_length\u0026#34;]) sns.kdeplot(df[\u0026#34;petal_width\u0026#34;]) plt.legend(labels=[\u0026#34;sepal_length\u0026#34;, \u0026#34;sepal_width\u0026#34;, \u0026#34;petal_length\u0026#34;, \u0026#34;petal_width\u0026#34;]) "},{"id":31,"href":"/en/timeseries/shape/002_dtwvsddtw/","title":"DTW and DDTW","section":"Shape \u0026 Similarity","content":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns from dtaidistance import dtw from dtaidistance import dtw_visualisation as dtwvis from utils import DDTW np.random.seed(777) Plotting Two Waveforms for the Experiment # data1 = np.array([12.0 * np.sin(i / 2.1) + 20 for i in range(30)]) data2 = np.array([10.0 * np.sin(i / 2.0) + np.random.rand() for i in range(30)]) plt.figure(figsize=(12, 4)) # Plotting the waveforms plt.plot(data1, label=\u0026#34;data1\u0026#34;, color=\u0026#34;k\u0026#34;) plt.plot(data2, label=\u0026#34;data2\u0026#34;, color=\u0026#34;r\u0026#34;) plt.legend() plt.show() DTW # d, paths = dtw.warping_paths( data1, data2, window=25, ) best_path = dtw.best_path(paths) dtwvis.plot_warpingpaths(data1, data2, paths, best_path) DDTW # γ_mat, arrows, ddtw = DDTW(np.array(data1), np.array(data2)) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (18, 15)}) sns.set(font=\u0026#34;IPAexGothic\u0026#34;) ax = sns.heatmap(-1 * γ_mat, cmap=\u0026#34;YlGnBu\u0026#34;) ax.set_title(f\u0026#34;DDTW = {ddtw}\u0026#34;) ax.invert_xaxis() ax.invert_yaxis() ax.set_xlabel(\u0026#34;w2\u0026#34;) ax.set_ylabel(\u0026#34;w2\u0026#34;) plt.show() "},{"id":32,"href":"/en/timeseries/exponential_smoothing/","title":"Exponential smoothing","section":"TimeSeries","content":" Chapter 2 # Exponential smoothing # "},{"id":33,"href":"/en/timeseries/exponential_smoothing/002-hw-es/","title":"Holt's Linear Exponential Smoothing Method","section":"Exponential smoothing","content":"import japanize_matplotlib as jm import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from statsmodels.tsa.holtwinters import Holt from statsmodels.tsa.holtwinters import SimpleExpSmoothing jm.japanize() Sample Data # The data used is from Natural Gas Consumption (NATURALGAS). The unit of measurement is BCF (billion cubic feet).\ndata = { \u0026#34;value\u0026#34;: { \u0026#34;2013-01-01\u0026#34;: 2878.8, \u0026#34;2013-02-01\u0026#34;: 2567.2, \u0026#34;2013-03-01\u0026#34;: 2521.1, \u0026#34;2013-04-01\u0026#34;: 1967.5, \u0026#34;2013-05-01\u0026#34;: 1752.5, \u0026#34;2013-06-01\u0026#34;: 1742.9, \u0026#34;2013-07-01\u0026#34;: 1926.3, \u0026#34;2013-08-01\u0026#34;: 1927.4, \u0026#34;2013-09-01\u0026#34;: 1767.0, \u0026#34;2013-10-01\u0026#34;: 1866.8, \u0026#34;2013-11-01\u0026#34;: 2316.9, \u0026#34;2013-12-01\u0026#34;: 2920.8, \u0026#34;2014-01-01\u0026#34;: 3204.1, \u0026#34;2014-02-01\u0026#34;: 2741.2, \u0026#34;2014-03-01\u0026#34;: 2557.9, \u0026#34;2014-04-01\u0026#34;: 1961.7, \u0026#34;2014-05-01\u0026#34;: 1810.2, \u0026#34;2014-06-01\u0026#34;: 1745.4, \u0026#34;2014-07-01\u0026#34;: 1881.0, \u0026#34;2014-08-01\u0026#34;: 1933.1, \u0026#34;2014-09-01\u0026#34;: 1809.3, \u0026#34;2014-10-01\u0026#34;: 1912.8, \u0026#34;2014-11-01\u0026#34;: 2357.5, \u0026#34;2014-12-01\u0026#34;: 2679.2, \u0026#34;2015-01-01\u0026#34;: 3115.0, \u0026#34;2015-02-01\u0026#34;: 2925.2, \u0026#34;2015-03-01\u0026#34;: 2591.3, \u0026#34;2015-04-01\u0026#34;: 2007.9, \u0026#34;2015-05-01\u0026#34;: 1858.2, \u0026#34;2015-06-01\u0026#34;: 1899.9, \u0026#34;2015-07-01\u0026#34;: 2067.7, \u0026#34;2015-08-01\u0026#34;: 2052.7, \u0026#34;2015-09-01\u0026#34;: 1901.3, \u0026#34;2015-10-01\u0026#34;: 1987.3, \u0026#34;2015-11-01\u0026#34;: 2249.1, \u0026#34;2015-12-01\u0026#34;: 2588.2, \u0026#34;2016-01-01\u0026#34;: 3091.7, \u0026#34;2016-02-01\u0026#34;: 2652.3, \u0026#34;2016-03-01\u0026#34;: 2356.3, \u0026#34;2016-04-01\u0026#34;: 2083.9, \u0026#34;2016-05-01\u0026#34;: 1965.8, \u0026#34;2016-06-01\u0026#34;: 2000.7, \u0026#34;2016-07-01\u0026#34;: 2186.6, \u0026#34;2016-08-01\u0026#34;: 2208.4, \u0026#34;2016-09-01\u0026#34;: 1947.8, \u0026#34;2016-10-01\u0026#34;: 1925.2, \u0026#34;2016-11-01\u0026#34;: 2159.5, \u0026#34;2016-12-01\u0026#34;: 2866.3, \u0026#34;2017-01-01\u0026#34;: 2913.8, \u0026#34;2017-02-01\u0026#34;: 2340.2, \u0026#34;2017-03-01\u0026#34;: 2523.3, \u0026#34;2017-04-01\u0026#34;: 1932.0, \u0026#34;2017-05-01\u0026#34;: 1892.0, \u0026#34;2017-06-01\u0026#34;: 1910.4, \u0026#34;2017-07-01\u0026#34;: 2141.6, \u0026#34;2017-08-01\u0026#34;: 2093.8, \u0026#34;2017-09-01\u0026#34;: 1920.5, \u0026#34;2017-10-01\u0026#34;: 2031.5, \u0026#34;2017-11-01\u0026#34;: 2357.3, \u0026#34;2017-12-01\u0026#34;: 3086.0, \u0026#34;2018-01-01\u0026#34;: 3340.9, \u0026#34;2018-02-01\u0026#34;: 2710.7, \u0026#34;2018-03-01\u0026#34;: 2796.7, \u0026#34;2018-04-01\u0026#34;: 2350.5, \u0026#34;2018-05-01\u0026#34;: 2055.0, \u0026#34;2018-06-01\u0026#34;: 2063.1, \u0026#34;2018-07-01\u0026#34;: 2350.7, \u0026#34;2018-08-01\u0026#34;: 2313.8, \u0026#34;2018-09-01\u0026#34;: 2156.1, \u0026#34;2018-10-01\u0026#34;: 2285.9, \u0026#34;2018-11-01\u0026#34;: 2715.9, \u0026#34;2018-12-01\u0026#34;: 2999.5, \u0026#34;2019-01-01\u0026#34;: 3424.3, \u0026#34;2019-02-01\u0026#34;: 3019.1, \u0026#34;2019-03-01\u0026#34;: 2927.8, \u0026#34;2019-04-01\u0026#34;: 2212.4, \u0026#34;2019-05-01\u0026#34;: 2134.0, \u0026#34;2019-06-01\u0026#34;: 2119.3, \u0026#34;2019-07-01\u0026#34;: 2393.9, \u0026#34;2019-08-01\u0026#34;: 2433.9, \u0026#34;2019-09-01\u0026#34;: 2206.3, \u0026#34;2019-10-01\u0026#34;: 2306.5, \u0026#34;2019-11-01\u0026#34;: 2783.8, \u0026#34;2019-12-01\u0026#34;: 3170.7, \u0026#34;2020-01-01\u0026#34;: 3320.6, \u0026#34;2020-02-01\u0026#34;: 3058.5, \u0026#34;2020-03-01\u0026#34;: 2722.0, \u0026#34;2020-04-01\u0026#34;: 2256.9, \u0026#34;2020-05-01\u0026#34;: 2072.2, \u0026#34;2020-06-01\u0026#34;: 2127.9, \u0026#34;2020-07-01\u0026#34;: 2464.1, \u0026#34;2020-08-01\u0026#34;: 2399.5, \u0026#34;2020-09-01\u0026#34;: 2151.2, \u0026#34;2020-10-01\u0026#34;: 2315.9, \u0026#34;2020-11-01\u0026#34;: 2442.0, \u0026#34;2020-12-01\u0026#34;: 3182.8, \u0026#34;2021-01-01\u0026#34;: 3343.9, \u0026#34;2021-02-01\u0026#34;: 3099.2, \u0026#34;2021-03-01\u0026#34;: 2649.4, \u0026#34;2021-04-01\u0026#34;: 2265.1, \u0026#34;2021-05-01\u0026#34;: 2117.4, \u0026#34;2021-06-01\u0026#34;: 2238.4, \u0026#34;2021-07-01\u0026#34;: 2412.2, \u0026#34;2021-08-01\u0026#34;: 2433.8, \u0026#34;2021-09-01\u0026#34;: 2142.3, \u0026#34;2021-10-01\u0026#34;: 2262.6, \u0026#34;2021-11-01\u0026#34;: 2693.3, \u0026#34;2021-12-01\u0026#34;: 3007.3, \u0026#34;2022-01-01\u0026#34;: 3612.1, \u0026#34;2022-02-01\u0026#34;: 3064.2, \u0026#34;2022-03-01\u0026#34;: 2785.4, \u0026#34;2022-04-01\u0026#34;: 2379.3, \u0026#34;2022-05-01\u0026#34;: 2247.8, \u0026#34;2022-06-01\u0026#34;: 2326.9, \u0026#34;2022-07-01\u0026#34;: 2597.9, \u0026#34;2022-08-01\u0026#34;: 2566.1, \u0026#34;2022-09-01\u0026#34;: 2263.3, } } data = pd.DataFrame(data) data.rename(columns={\u0026#34;value\u0026#34;: \u0026#34;Natural Gas Consumption(BCF)\u0026#34;}, inplace=True) data.index = pd.to_datetime(data.index) data = data.asfreq(\u0026#34;MS\u0026#34;) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data) plt.grid() plt.show() Data Splitting # The dataset is split so that the data from the end (data from 2020 onwards) is used for validating the forecasting performance.\ndata_train = data[data.index \u0026lt; \u0026#34;2020-1-1\u0026#34;] data_test = data[data.index \u0026gt;= \u0026#34;2020-1-1\u0026#34;] plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.legend() plt.grid() Model Training and Forecasting # ses = SimpleExpSmoothing(data_train) ses = ses.fit(smoothing_level=0.1) ses_pred = ses.forecast(33) holt = Holt(data_train) holt = holt.fit(smoothing_level=0.1, smoothing_trend=0.2) holt_pred = holt.forecast(33) plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.plot(ses_pred.index, ses_pred.values, \u0026#34;-x\u0026#34;, label=\u0026#34;prediction(ses)\u0026#34;) plt.plot(holt_pred.index, holt_pred.values, \u0026#34;-x\u0026#34;, label=\u0026#34;prediction(Holt)\u0026#34;) plt.legend() plt.grid() "},{"id":34,"href":"/en/timeseries/preprocess/002-preprocess-trend/","title":"Impact of Trends","section":"Plotting and Preprocessing","content":" import japanize_matplotlib import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns Generate sample data # date_list = pd.date_range(\u0026#34;2021-01-01\u0026#34;, periods=720, freq=\u0026#34;D\u0026#34;) value_list = [ 10 + np.cos(np.pi * i / 28.0) * (i % 3 \u0026gt; 0) + np.cos(np.pi * i / 14.0) * (i % 5 \u0026gt; 0) + np.cos(np.pi * i / 7.0) + (i / 10) ** 1.1 / 20 for i, di in enumerate(date_list) ] df = pd.DataFrame( { \u0026#34;日付\u0026#34;: date_list, \u0026#34;y\u0026#34;: value_list, } ) df.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;y\u0026#34;]) Forecast Time Series Data with XGBoost # df[\u0026#34;曜日\u0026#34;] = df[\u0026#34;日付\u0026#34;].dt.weekday df[\u0026#34;年初からの日数%14\u0026#34;] = df[\u0026#34;日付\u0026#34;].dt.dayofyear % 14 df[\u0026#34;年初からの日数%28\u0026#34;] = df[\u0026#34;日付\u0026#34;].dt.dayofyear % 28 def get_trend(timeseries, deg=3, trainN=0): \u0026#34;\u0026#34;\u0026#34;Create a trend line for time-series data Args: timeseries(pd.Series) : Time series data deg(int) : Degree of polynomial trainN(int): Number of data used to estimate the coefficients of the polynomial Returns: pd.Series: Time series data corresponding to trends \u0026#34;\u0026#34;\u0026#34; if trainN == 0: trainN = len(timeseries) x = list(range(len(timeseries))) y = timeseries.values coef = np.polyfit(x[:trainN], y[:trainN], deg) trend = np.poly1d(coef)(x) return pd.Series(data=trend, index=timeseries.index) trainN = 500 df[\u0026#34;Trend\u0026#34;] = get_trend(df[\u0026#34;y\u0026#34;], trainN=trainN, deg=2) plt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;y\u0026#34;]) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;Trend\u0026#34;]) X = df[[\u0026#34;曜日\u0026#34;, \u0026#34;年初からの日数%14\u0026#34;, \u0026#34;年初からの日数%28\u0026#34;]] y = df[\u0026#34;y\u0026#34;] trainX, trainy = X[:trainN], y[:trainN] testX, testy = X[trainN:], y[trainN:] trend_train, trend_test = df[\u0026#34;Trend\u0026#34;][:trainN], df[\u0026#34;Trend\u0026#34;][trainN:] Forecasting without considering trends # XGBoost does not know that data changes slowly between training and test data. Therefore, the more you predict the future, the more your predictions will be off down the road. For XGBoost to forecast well, the y distribution of the training and test data must be close.\nimport xgboost as xgb from sklearn.metrics import mean_squared_error regressor = xgb.XGBRegressor(max_depth=5).fit(trainX, trainy) prediction = regressor.predict(testX) plt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;][trainN:], y=prediction) sns.lineplot(x=df[\u0026#34;日付\u0026#34;][trainN:], y=testy) plt.legend([\u0026#34;モデルの出力\u0026#34;, \u0026#34;正解\u0026#34;], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102)) print(f\u0026#34;MSE = {mean_squared_error(testy, prediction)}\u0026#34;) MSE = 2.815118389938834\rForecasting with the trend taken into account # We first remove the portion corresponding to the trend from the observed values and then predict the values without the trend. The XGBoost prediction is then added to the XGBoost prediction to obtain the final prediction.\nregressor = xgb.XGBRegressor(max_depth=5).fit(trainX, trainy - trend_train) prediction = regressor.predict(testX) prediction = [pred_i + trend_i for pred_i, trend_i in zip(prediction, trend_test)] plt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;][trainN:], y=prediction) sns.lineplot(x=df[\u0026#34;日付\u0026#34;][trainN:], y=testy) plt.legend([\u0026#34;モデルの出力＋Trend\u0026#34;, \u0026#34;正解\u0026#34;], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102)) print(f\u0026#34;MSE = {mean_squared_error(testy, prediction)}\u0026#34;) MSE = 0.46014173311011325\r"},{"id":35,"href":"/en/basic/clustering/k-means2/","title":"k-means++","section":"Clustering","content":" import numpy as np import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.datasets import make_blobs Comparing clustering results between k-means++ and k-means(random initial value selection) # Compare the clustering results between random and kmeans++. You can see that k-means++ may split the upper left cluster into two clusters. *In this example, we dare to specify max_iter=1.\nn_samples = 3000 random_state = 11711 X, y = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=1.5, centers=8 ) plt.figure(figsize=(8, 8)) for i in range(10): # Compare k-means and k-means++ rand_index = np.random.permutation(1000) X_rand = X[rand_index] y_pred_rnd = KMeans( n_clusters=5, random_state=random_state, init=\u0026#34;random\u0026#34;, max_iter=1, n_init=1 ).fit_predict(X_rand) y_pred_kpp = KMeans( n_clusters=5, random_state=random_state, init=\u0026#34;k-means++\u0026#34;, max_iter=1, n_init=1 ).fit_predict(X_rand) plt.figure(figsize=(10, 2)) plt.subplot(1, 2, 1) plt.title(f\u0026#34;random\u0026#34;) plt.scatter(X_rand[:, 0], X_rand[:, 1], c=y_pred_rnd, marker=\u0026#34;x\u0026#34;) plt.subplot(1, 2, 2) plt.title(f\u0026#34;k-means++\u0026#34;) plt.scatter(X_rand[:, 0], X_rand[:, 1], c=y_pred_kpp, marker=\u0026#34;x\u0026#34;) plt.show() plt.tight_layout() plt.show() "},{"id":36,"href":"/en/basic/classification/linear_discriminant_analysis/","title":"Linear Discriminant Analysis","section":"Linear Classification","content":" import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_blobs from sklearn.discriminant_analysis import LinearDiscriminantAnalysis Generate dataset # n_samples = 200 X, y = make_blobs(n_samples=200, centers=2, n_features=2, cluster_std=2) X[:, 0] -= np.mean(X[:, 0]) X[:, 1] -= np.mean(X[:, 1]) fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;Scatter plots of data\u0026#34;, fontsize=20) plt.scatter(X[:, 0], X[:, 1], c=y) plt.show() Find decision boundaries by Linear discriminant analysis # sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n# Find the decision boundary clf = LinearDiscriminantAnalysis(store_covariance=True) clf.fit(X, y) # Check the decision boundary w = clf.coef_[0] wt = -1 / (w[1] / w[0]) ## Find the slope perpendicular to w xs = np.linspace(-10, 10, 100) ys_w = [(w[1] / w[0]) * xi for xi in xs] ys_wt = [wt * xi for xi in xs] fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;Visualize the slope of the decision boundary\u0026#34;, fontsize=20) plt.scatter(X[:, 0], X[:, 1], c=y) # sample dataset plt.plot(xs, ys_w, \u0026#34;-.\u0026#34;, color=\u0026#34;k\u0026#34;, alpha=0.5) # orientation of W plt.plot(xs, ys_wt, \u0026#34;--\u0026#34;, color=\u0026#34;k\u0026#34;) # Orientation perpendicular to w plt.xlim(-10, 10) plt.ylim(-10, 10) plt.show() # The result of transferring data to one dimension based on the vector w obtained X_1d = clf.transform(X).reshape(1, -1)[0] fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;Location of data when data is projected in one dimension\u0026#34;, fontsize=15) plt.scatter(X_1d, [0 for _ in range(n_samples)], c=y) plt.show() Example with more than 2-dimensional data # X_3d, y_3d = make_blobs(n_samples=200, centers=3, n_features=3, cluster_std=3) # Distribution of sample data fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;Scatter plots of data\u0026#34;, fontsize=20) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=y_3d) plt.show() # Apply LDA clf_3d = LinearDiscriminantAnalysis() clf_3d.fit(X_3d, y_3d) X_2d = clf_3d.transform(X_3d) # Location of data when data is projected in two dimensions fig = plt.figure(figsize=(7, 7)) plt.title(\u0026#34;Location of data when data is projected in two dimensions\u0026#34;, fontsize=15) plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_3d) plt.show() "},{"id":37,"href":"/en/basic/regression/","title":"Linear Regression","section":"Machine Learning","content":" Chapter 1 # Linear Regression # "},{"id":38,"href":"/en/basic/","title":"Machine Learning","section":"Home","content":" Section 2 # Machine Learning # Wikipedia | Machine Learning\n"},{"id":39,"href":"/en/finance/main/001_mplfinance/","title":"mplfinance","section":"Time Series","content":" mplfinance is an extension to matplotlib for visualizing finance data. In this article, we will try to create a graph from financial data using mplfinance. The basic usage is explained in the \u0026ldquo;Tutorial\u0026rdquo; in the mplfinance repository.\nLoading Data # Pandas-datareader | Remote Data Access\nimport os import time import pandas as pd import pandas_datareader.data as web from pandas_datareader._utils import RemoteDataError Yahoo Terms of Service\ndef get_finance_data( ticker_symbol: str, start=\u0026#34;2021-01-01\u0026#34;, end=\u0026#34;2021-06-30\u0026#34;, savedir=\u0026#34;data\u0026#34; ) -\u0026gt; pd.DataFrame: \u0026#34;\u0026#34;\u0026#34;Retrieves data recording stock prices Args: ticker_symbol (str): Description of param1 start (str): Date of beginning of period, optional. end (str): Date of end of period, optional. Returns: res: Stock Price Data \u0026#34;\u0026#34;\u0026#34; res = None filepath = os.path.join(savedir, f\u0026#34;{ticker_symbol}_{start}_{end}_historical.csv\u0026#34;) os.makedirs(savedir, exist_ok=True) if not os.path.exists(filepath): try: time.sleep(5.0) res = web.DataReader(ticker_symbol, \u0026#34;yahoo\u0026#34;, start=start, end=end) res.to_csv(filepath, encoding=\u0026#34;utf-8-sig\u0026#34;) except (RemoteDataError, KeyError): print(f\u0026#34;ticker_symbol ${ticker_symbol} が正しいか確認してください。\u0026#34;) else: res = pd.read_csv(filepath, index_col=\u0026#34;Date\u0026#34;) res.index = pd.to_datetime(res.index) assert res is not None, \u0026#34;データ取得に失敗しました\u0026#34; return res # ticker symbol, period, and destination file ticker_symbol = \u0026#34;NVDA\u0026#34; start = \u0026#34;2021-01-01\u0026#34; end = \u0026#34;2021-06-30\u0026#34; df = get_finance_data(ticker_symbol, start=start, end=end, savedir=\u0026#34;../data\u0026#34;) df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rPlotting OHLC # OHLC is the opening price, high, low, and close, and is one of the graphs we usually look at most often. Let\u0026rsquo;s plot it as a candlestick and as a line.\nmplfinance | matplotlib utilities for the visualization, and visual analysis, of financial data\nimport mplfinance as mpf mpf.plot(df, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4)) mpf.plot(df, type=\u0026#34;line\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4)) Moving Average # One of the indicators used in technical analysis of stock prices and foreign exchange is the moving average. In current technical analysis, there are many examples where three moving averages (short, medium, and long term) are displayed simultaneously. In daily charts, the 5-day, 25-day, and 75-day moving averages are often used.\nSpecify the mav option to plot the 5/25/75 day moving average.\nmpf.plot(df, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4), mav=[5, 25, 75]) Show legend # Add a legend because it is difficult to tell which line represents a moving average over what number of days.\nmatplotlib.pyplot.legend\nimport japanize_matplotlib import matplotlib.patches as mpatches fig, axes = mpf.plot( df, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4), mav=[5, 25, 75], returnfig=True, ) fig.legend( [f\u0026#34;{days} days\u0026#34; for days in [5, 25, 75]], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102) ) \u0026lt;matplotlib.legend.Legend at 0x7f8be96e5b80\u0026gt;\rDisplaying Volume # fig, axes = mpf.plot( df, title=\u0026#34;NVDA 2021/1/B~2021/6/E\u0026#34;, type=\u0026#34;candle\u0026#34;, style=\u0026#34;starsandstripes\u0026#34;, figsize=(12, 4), mav=[5, 25, 75], volume=True, datetime_format=\u0026#34;%Y/%m/%d\u0026#34;, returnfig=True, ) fig.legend( [f\u0026#34;{days} days\u0026#34; for days in [5, 25, 75]], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102) ) fig, axes = mpf.plot( df, title=\u0026#34;NVDA 2021/1/B~2021/6/E\u0026#34;, type=\u0026#34;candle\u0026#34;, style=\u0026#34;yahoo\u0026#34;, figsize=(12, 4), mav=[5, 25, 75], volume=True, datetime_format=\u0026#34;%Y/%m/%d\u0026#34;, returnfig=True, ) fig.legend( [f\u0026#34;{days} days\u0026#34; for days in [5, 25, 75]], bbox_to_anchor=(0.0, 0.78, 0.28, 0.102) ) "},{"id":40,"href":"/en/finance/visualize/002-visuazlize-by-circle/","title":"positive and negative changes","section":"Visualize","content":"import matplotlib.pyplot as plt import numpy as np import pandas as pd from matplotlib.colors import LinearSegmentedColormap cmap = LinearSegmentedColormap.from_list(\u0026#34;rg\u0026#34;, [\u0026#34;r\u0026#34;, \u0026#34;w\u0026#34;, \u0026#34;g\u0026#34;], N=256) data = { \u0026#34;$AAAA\u0026#34;: { \u0026#34;EPS\u0026#34;: [0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3, 0.4], \u0026#34;Revenue\u0026#34;: [-0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3, 0.4], }, \u0026#34;$BBBB\u0026#34;: { \u0026#34;EPS\u0026#34;: [0.3, 0.1, -0.3, 0.1, 0.1, -0.2, 0.3, 0.4], \u0026#34;Revenue\u0026#34;: [0.1, -0.2, 0.3, 0.4, 0.3, 0.1, -0.3, 0.1], }, \u0026#34;$CCCC\u0026#34;: { \u0026#34;EPS\u0026#34;: [0.1, 0.4, 0.5, 0.2, 0.1, 0.4, 0.5, 0.2], \u0026#34;Revenue\u0026#34;: [0.1, -0.2, 0.3, 0.4, 0.5, 0.2, 0.1, 0.2], }, } n_companies = 3 n_times = 8 Plot # y_index = 0 y_label = [] fig = plt.figure(figsize=(12, 6)) ax = plt.gca() ax.set_facecolor(\u0026#34;#fefefe\u0026#34;) for company_name, eps_rev in data.items(): d = -0.1 for name, v in eps_rev.items(): x = np.arange(n_times) y = [y_index + d for _ in range(n_times)] plt.scatter(x, y, c=v, s=500, cmap=cmap) for xi, vi in zip(x, v): plt.text(xi + 0.15, y_index + d, f\u0026#34;{vi}%\u0026#34;, fontsize=15) d += 0.2 y_label.append(company_name) y_index += 1 plt.xticks( np.arange(n_times), labels=[f\u0026#34;2022/{m+1}\u0026#34; for m in np.arange(n_times)], fontsize=14 ) plt.yticks(np.arange(n_companies), labels=y_label, fontsize=20) plt.grid(axis=\u0026#34;x\u0026#34;, color=\u0026#34;#ddd\u0026#34;) "},{"id":41,"href":"/en/eval/regression/","title":"Regression","section":"Metrics","content":" Chapter 2 # Content # "},{"id":42,"href":"/en/basic/regression/ridge_and_lasso/","title":"Ridge and Lasso regression","section":"Linear Regression","content":" Import required modules # import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_regression from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import cross_val_score from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler Generate regression data for experiments # sklearn.datasets.make_regression\nArtificially generate regression data using sklearn.datasets.make_regression. Specify two features (n_informative = 2) as necessary for prediction. Otherwise, they are redundant features that are not useful for prediction.\nn_features = 5 n_informative = 2 X, y = make_regression( n_samples=500, n_features=n_features, n_informative=n_informative, noise=0.5, random_state=777, ) X = np.concatenate([X, np.log(X + 100)], 1) # Add redundant features y_mean = y.mean(keepdims=True) y_std = np.std(y, keepdims=True) y = (y - y_mean) / y_std Compare least squares, ridge regression, and lasso regression models # Let\u0026rsquo;s train each model with the same data and see the differences. Normally, the coefficient alpha of the regularization term should be changed, but in this case, it is fixed.\nsklearn.pipeline.make_pipeline sklearn.linear_model.LinearRegression sklearn.linear_model.Ridge sklearn.linear_model.Lasso lin_r = make_pipeline(StandardScaler(with_mean=False), LinearRegression()).fit(X, y) rid_r = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=2)).fit(X, y) las_r = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha=0.1)).fit(X, y) Compare the values of the coefficients for each model # Plot the absolute value of each coefficient. It can be seen from the graph that for linear regression, the coefficients are almost never zero. We can also see that Lasso regression has coefficients = 0 for all but the features needed for prediction.\nSince there were two useful features (n_informative = 2), we can see that linear regression has unconditional coefficients even on features that are not useful, while Lasso-Ridge regression could not be narrowed down to two features. Still, it resulted in many unwanted features having coefficients of zero.\nfeat_index = np.array([i for i in range(X.shape[1])]) plt.figure(figsize=(12, 4)) plt.bar( feat_index - 0.2, np.abs(lin_r.steps[1][1].coef_), width=0.2, label=\u0026#34;LinearRegression\u0026#34;, ) plt.bar(feat_index, np.abs(rid_r.steps[1][1].coef_), width=0.2, label=\u0026#34;RidgeRegression\u0026#34;) plt.bar( feat_index + 0.2, np.abs(las_r.steps[1][1].coef_), width=0.2, label=\u0026#34;LassoRegression\u0026#34;, ) plt.xlabel(r\u0026#34;$\\beta$\u0026#34;) plt.ylabel(r\u0026#34;$|\\beta|$\u0026#34;) plt.grid() plt.legend() "},{"id":43,"href":"/en/basic/ensemble/stucking/","title":"Stacking","section":"Ensemble","content":" import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import StackingClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.tree import export_graphviz from subprocess import call Create data for experiment # # Create data with 20 features n_features = 20 X, y = make_classification( n_samples=2500, n_features=n_features, n_informative=10, n_classes=2, n_redundant=0, n_clusters_per_class=4, random_state=777, ) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=777 ) Plot the data with respect to multiple features # Confirm that it does not appear to be classifiable by simple rules.\nplt.figure(figsize=(10, 10)) plt.subplot(2, 2, 1) plt.scatter(X[:, 2], X[:, 7], c=y) plt.xlabel(\u0026#34;x2\u0026#34;) plt.ylabel(\u0026#34;x7\u0026#34;) plt.subplot(2, 2, 2) plt.scatter(X[:, 4], X[:, 9], c=y) plt.xlabel(\u0026#34;x4\u0026#34;) plt.ylabel(\u0026#34;x9\u0026#34;) plt.subplot(2, 2, 3) plt.scatter(X[:, 5], X[:, 1], c=y) plt.xlabel(\u0026#34;x5\u0026#34;) plt.ylabel(\u0026#34;x1\u0026#34;) plt.subplot(2, 2, 4) plt.scatter(X[:, 1], X[:, 3], c=y) plt.xlabel(\u0026#34;x1\u0026#34;) plt.ylabel(\u0026#34;x3\u0026#34;) plt.show() Stacking vs. Random Forest # Classification with Random Forest # model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=777) model.fit(X_train, y_train) y_pred = model.predict(X_test) rf_score = roc_auc_score(y_test, y_pred) print(f\u0026#34;ROC-AUC = {rf_score}\u0026#34;) ROC-AUC = 0.855797033310609\rWhen stacking with multiple trees # We can confirm that stacking with only DecisionTreeClassifier does not improve accuracy much.\nsklearn.ensemble.StackingClassifier\n# Models used in base learner estimators = [ (\u0026#34;dt1\u0026#34;, DecisionTreeClassifier(max_depth=3, random_state=777)), (\u0026#34;dt2\u0026#34;, DecisionTreeClassifier(max_depth=4, random_state=777)), (\u0026#34;dt3\u0026#34;, DecisionTreeClassifier(max_depth=5, random_state=777)), (\u0026#34;dt4\u0026#34;, DecisionTreeClassifier(max_depth=6, random_state=777)), ] # Number of models included in base learner n_estimators = len(estimators) # aggregation model final_estimator = DecisionTreeClassifier(max_depth=3, random_state=777) # train base-learner and aggregation model clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator) clf.fit(X_train, y_train) # evaluate y_pred = clf.predict(X_test) clf_score = roc_auc_score(y_test, y_pred) print(\u0026#34;ROC-AUC\u0026#34;) print(f\u0026#34;Decision Tree Stacking＝{clf_score}, Random Forest＝{rf_score}\u0026#34;) ROC-AUC\rDecision Tree Stacking＝0.7359716965608031, Random Forest＝0.855797033310609\rVisualize trees used for stacking # export_graphviz( clf.final_estimator_, out_file=\u0026#34;tree_final_estimator.dot\u0026#34;, class_names=[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;], feature_names=[e[0] for e in estimators], proportion=True, filled=True, ) call( [ \u0026#34;dot\u0026#34;, \u0026#34;-Tpng\u0026#34;, \u0026#34;tree_final_estimator.dot\u0026#34;, \u0026#34;-o\u0026#34;, f\u0026#34;tree_final_estimator.png\u0026#34;, \u0026#34;-Gdpi=200\u0026#34;, ] ) display(Image(filename=\u0026#34;tree_final_estimator.png\u0026#34;)) Look at the feature importance of the trees used in the stacking. # We see that although stacked, in the end only the fourth tree is used almost exclusively in the forecast.\nplt.figure(figsize=(6, 3)) plot_index = [i for i in range(n_estimators)] plt.bar(plot_index, clf.final_estimator_.feature_importances_) plt.xticks(plot_index, [e[0] for e in estimators]) plt.xlabel(\u0026#34;model name\u0026#34;) plt.ylabel(\u0026#34;feature-importance\u0026#34;) plt.show() Check the performance of each tree in the base learner # scores = [] for clf_estim in clf.estimators_: print(\u0026#34;====\u0026#34;) y_pred = clf_estim.predict(X_test) scr = roc_auc_score(y_test, y_pred) scores.append(scr) print(clf_estim) print(scr) n_estimators = len(estimators) plot_index = [i for i in range(n_estimators)] plt.figure(figsize=(8, 4)) plt.bar(plot_index, scores) plt.xticks(plot_index, [e[0] for e in estimators]) plt.xlabel(\u0026#34;model name\u0026#34;) plt.ylabel(\u0026#34;roc-auc\u0026#34;) plt.show() ====\rDecisionTreeClassifier(max_depth=3, random_state=777)\r0.7660117774277722\r====\rDecisionTreeClassifier(max_depth=4, random_state=777)\r0.7744128916993818\r====\rDecisionTreeClassifier(max_depth=5, random_state=777)\r0.8000158677919086\r====\rDecisionTreeClassifier(max_depth=6, random_state=777)\r0.8084639977432473\r・Stacked Generalization (Stacking) ・MLWave/Kaggle-Ensemble-Guide\n"},{"id":44,"href":"/en/basic/dimensionality_reduction/svd/","title":"SVD","section":"Dimensionality Reduction","content":"\rimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from scipy import linalg from PIL import Image Data for experiment # Let\u0026rsquo;s consider the image of the word \u0026ldquo;実験\u0026rdquo; as a series of data (grayscale, so [0, 0, 1, 1, 0 \u0026hellip;]) ) and decompose it into singular values.\nhttps://docs.scipy.org/doc/scipy/tutorial/linalg.html\nimg = Image.open(\u0026#34;./sample.png\u0026#34;).convert(\u0026#34;L\u0026#34;).resize((163, 372)).rotate(90, expand=True) img Perform singular value decomposition # ## Decompose matrix X into singular values X = np.asarray(img) U, Sigma, VT = linalg.svd(X, full_matrices=True) print(f\u0026#34;A: {X.shape}, U: {U.shape}, Σ:{Sigma.shape}, V^T:{VT.shape}\u0026#34;) A: (163, 372), U: (163, 163), Σ:(163,), V^T:(372, 372)\rApproximate image with low rank # # Increasing the rank restores the original characters more precisely for rank in [1, 2, 3, 4, 5, 10, 20, 50]: # Extract elements up to the rank-th element U_i = U[:, :rank] Sigma_i = np.matrix(linalg.diagsvd(Sigma[:rank], rank, rank)) VT_i = VT[:rank, :] # Restore image temp_image = np.asarray(U_i * Sigma_i * VT_i) Image.fromarray(np.uint8(temp_image)) plt.title(f\u0026#34;rank={rank}\u0026#34;) plt.imshow(temp_image, cmap=\u0026#34;gray\u0026#34;) plt.show() Contents of matrix V # total = np.zeros((163, 372)) for rank in [1, 2, 3, 4, 5]: # Extract elements up to the rank-th element U_i = U[:, :rank] Sigma_i = np.matrix(linalg.diagsvd(Sigma[:rank], rank, rank)) VT_i = VT[:rank, :] # Set all values except the rank-th singular value to 0, leaving only the rank-th element. if rank \u0026gt; 1: for ri in range(rank - 1): Sigma_i[ri, ri] = 0 # Restore image temp_image = np.asarray(U_i * Sigma_i * VT_i) Image.fromarray(np.uint8(temp_image)) # Add only the rank-th element total += temp_image plt.figure(figsize=(5, 5)) plt.suptitle(f\u0026#34;$u_{rank}$\u0026#34;) plt.subplot(211) plt.imshow(temp_image, cmap=\u0026#34;gray\u0026#34;) plt.subplot(212) plt.plot(VT[0]) plt.show() # Confirm that adding the first through the fifth elements together properly restores the original image plt.imshow(total) "},{"id":45,"href":"/en/visualize/category-groupby/treemap/","title":"Tree map","section":"Numeric Value Distribution","content":"A treemap is a diagram that can be used to visualize numerical data with hierarchical categories. A typical example is a heat map of the Nikkei 225 or the S\u0026amp;P 500. This notebook uses squarify.\nA treemap can also be created by using plotly.（ Treemap charts with Python - Plotly）\nimport numpy as np import matplotlib.pyplot as plt import japanize_matplotlib import squarify np.random.seed(0) # Fix random numbers labels = [\u0026#34;A\u0026#34; * i for i in range(1, 5)] sizes = [i * 10 for i in range(1, 5)] colors = [\u0026#34;#%02x%02x%02x\u0026#34; % (i * 50, 0, 0) for i in range(1, 5)] plt.figure(figsize=(5, 5)) squarify.plot(sizes, color=colors, label=labels) plt.axis(\u0026#34;off\u0026#34;) plt.show() Visualize my portfolio # Suppose I have data on the acquisition price and current price for each stock I own. From there, I would create a heatmap like finviz.\nSuppose we read the following data from csv.\n※The data shown here are fictitious.\nimport pandas as pd data = [ [\u0026#34;PBR\u0026#34;, 80.20, 130.00], [\u0026#34;GOOG\u0026#34;, 1188.0, 1588.0], [\u0026#34;FLNG\u0026#34;, 70.90, 230.00], [\u0026#34;ZIM\u0026#34;, 400.22, 630.10], [\u0026#34;GOGL\u0026#34;, 120.20, 90.90], [\u0026#34;3466\\nラサールロジ\u0026#34;, 156.20, 147.00], # 日本語表示のテスト用 ] df = pd.DataFrame(data) df.columns = [\u0026#34;銘柄名\u0026#34;, \u0026#34;取得価額\u0026#34;, \u0026#34;現在の価額\u0026#34;] df[\u0026#34;評価損益\u0026#34;] = df[\u0026#34;現在の価額\u0026#34;] - df[\u0026#34;取得価額\u0026#34;] df.head(6) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rSpecify the color of the treemap. # Green for profitable areas and red for loss areas.\ncolors = [] percents = [] for p_or_l, oac in zip(df[\u0026#34;評価損益\u0026#34;], df[\u0026#34;取得価額\u0026#34;]): percent = p_or_l / oac * 100 if p_or_l \u0026gt; 0: g = np.min([percent * 255 / 100 + 100, 255.0]) color = \u0026#34;#%02x%02x%02x\u0026#34; % (0, int(g), 0) colors.append(color) else: r = np.min([-percent * 255 / 100 + 100, 255]) color = \u0026#34;#%02x%02x%02x\u0026#34; % (int(r), 0, 0) colors.append(color) percents.append(percent) print(df[\u0026#34;銘柄名\u0026#34;].values) print(colors) print(percents) ['PBR' 'GOOG' 'FLNG' 'ZIM' 'GOGL' '3466\\nラサールロジ']\r['#00ff00', '#00b900', '#00ff00', '#00f600', '#a20000', '#730000']\r[62.094763092269325, 33.670033670033675, 224.4005641748942, 57.43840887511868, -24.376039933444257, -5.8898847631241935]\rDisplaying a treemap # Let\u0026rsquo;s display the profit/loss in colors and the percentage of profit/loss on the treemap. Japanese characters are not garbled because import japanize_matplotlib is used at the beginning.\ncurrent_prices = [cp for cp in df[\u0026#34;現在の価額\u0026#34;]] labels = [ f\u0026#34;{name}\\n{np.round(percent, 2)}％\u0026#34;.replace(\u0026#34;-\u0026#34;, \u0026#34;▼\u0026#34;) for name, percent in zip(df[\u0026#34;銘柄名\u0026#34;], percents) ] plt.figure(figsize=(10, 10)) plt.rcParams[\u0026#34;font.size\u0026#34;] = 18 squarify.plot(current_prices, color=colors, label=labels) plt.axis(\u0026#34;off\u0026#34;) plt.show() Add cache display to the treemap # Let\u0026rsquo;s also add a cache display to the treemap. The color should be gray.\nplt.figure(figsize=(10, 10)) plt.rcParams[\u0026#34;font.size\u0026#34;] = 18 squarify.plot( current_prices + [3500], color=colors + [\u0026#34;#ccc\u0026#34;], label=labels + [\u0026#34;キャッシュ\u0026#34;] ) plt.axis(\u0026#34;off\u0026#34;) plt.show() "},{"id":46,"href":"/en/finance/visualize/","title":"Visualize","section":"Economic Data","content":" Chapter 2 # Visualize # "},{"id":47,"href":"/en/basic/ensemble/adaboost_classification/","title":"Adaboost (classification)","section":"Ensemble","content":"import matplotlib.pyplot as plt import japanize_matplotlib import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier Create data for experiment # n_features = 20 X, y = make_classification( n_samples=2500, n_features=n_features, n_informative=10, n_classes=2, n_redundant=4, n_clusters_per_class=5, ) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42 ) Train adaboost # Here we use a decision tree as a weak learner.\nsklearn.ensemble.StackingClassifier\nab_clf = AdaBoostClassifier( n_estimators=10, learning_rate=1.0, random_state=117117, base_estimator=DecisionTreeClassifier(max_depth=2), ) ab_clf.fit(X_train, y_train) y_pred = ab_clf.predict(X_test) ab_clf_score = roc_auc_score(y_test, y_pred) ab_clf_score 0.7546477034876885\rInfluence of learning-rate # The smaller the learning-rate, the smaller the range of weight updates. Conversely, if it is too large, convergence may not occur.\nscores = [] learning_rate_list = np.linspace(0.01, 1, 100) for lr in learning_rate_list: ab_clf_i = AdaBoostClassifier( n_estimators=10, learning_rate=lr, random_state=117117, base_estimator=DecisionTreeClassifier(max_depth=2), ) ab_clf_i.fit(X_train, y_train) y_pred = ab_clf_i.predict(X_test) scores.append(roc_auc_score(y_test, y_pred)) plt.figure(figsize=(5, 5)) plt.plot(learning_rate_list, scores) plt.xlabel(\u0026#34;learning rate\u0026#34;) plt.ylabel(\u0026#34;ROC-AUC\u0026#34;) plt.grid() plt.show() Influence of n_estimators # N_estimators specifies the number of weak learners. Normally, there is no need to make this parameter larger or smaller. Fix n_estimators at some large number and then adjust the other parameters.\nscores = [] n_estimators_list = [int(ne) for ne in np.linspace(5, 70, 40)] for n_estimators in n_estimators_list: ab_clf_i = AdaBoostClassifier( n_estimators=int(n_estimators), learning_rate=0.6, random_state=117117, base_estimator=DecisionTreeClassifier(max_depth=2), ) ab_clf_i.fit(X_train, y_train) y_pred = ab_clf_i.predict(X_test) scores.append(roc_auc_score(y_test, y_pred)) plt.figure(figsize=(5, 5)) plt.plot(n_estimators_list, scores) plt.xlabel(\u0026#34;n_estimators\u0026#34;) plt.ylabel(\u0026#34;ROC-AUC\u0026#34;) plt.grid() plt.show() Influence of base-estimator # base_estimator specifies what to use as a weak learner. In other words, it is one of the most important parameters in Adaboost.\nscores = [] base_estimator_list = [ DecisionTreeClassifier(max_depth=md) for md in [2, 3, 4, 5, 6, 7, 8, 9, 10] ] for base_estimator in base_estimator_list: ab_clf_i = AdaBoostClassifier( n_estimators=10, learning_rate=0.5, random_state=117117, base_estimator=base_estimator, ) ab_clf_i.fit(X_train, y_train) y_pred = ab_clf_i.predict(X_test) scores.append(roc_auc_score(y_test, y_pred)) plt.figure(figsize=(5, 5)) plt_index = [i for i in range(len(base_estimator_list))] plt.bar(plt_index, scores) plt.xticks(plt_index, [str(bm) for bm in base_estimator_list], rotation=90) plt.xlabel(\u0026#34;base_estimator\u0026#34;) plt.ylabel(\u0026#34;ROC-AUC\u0026#34;) plt.show() Visualization of data weights in Adaboost # Visualize assigning weights to data that is difficult to classify.\n# NOTE: Model created to check the sample_weight passed to the model # This DummyClassifier does not change the parameters of the Adaboost class DummyClassifier: def __init__(self): self.model = DecisionTreeClassifier(max_depth=3) self.n_classes_ = 2 self.classes_ = [\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;] self.sample_weight = None ## sample_weight def fit(self, X, y, sample_weight=None): self.sample_weight = sample_weight self.model.fit(X, y, sample_weight=sample_weight) return self.model def predict(self, X, check_input=True): proba = self.model.predict(X) return proba def get_params(self, deep=False): return {} def set_params(self, deep=False): return {} n_samples = 500 X_2, y_2 = make_classification( n_samples=n_samples, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, random_state=117, n_clusters_per_class=2, ) plt.figure( figsize=( 7, 7, ) ) plt.title(f\u0026#34;Scatter plots of sample data\u0026#34;) plt.scatter(X_2[:, 0], X_2[:, 1], c=y_2) plt.show() Weight after booting has progressed # The more weighted data is represented by a larger circle.\nclf = AdaBoostClassifier( n_estimators=4, random_state=0, algorithm=\u0026#34;SAMME\u0026#34;, base_estimator=DummyClassifier() ) clf.fit(X_2, y_2) for i, estimators_i in enumerate(clf.estimators_): plt.figure( figsize=( 7, 7, ) ) plt.title(f\u0026#34;Visualization of the {i}-th weighted sample\u0026#34;) plt.scatter( X_2[:, 0], X_2[:, 1], marker=\u0026#34;o\u0026#34;, c=y_2, alpha=0.4, s=estimators_i.sample_weight * n_samples ** 1.65, ) plt.show() "},{"id":48,"href":"/en/eval/classification/","title":"Classification","section":"Metrics","content":" Chapter 3 # Classification # "},{"id":49,"href":"/en/basic/tree/parameter/","title":"Decision Tree Parameters","section":"Decision Tree","content":" max_depth specifies the maximum depth of the tree min_samples_split specifies the minimum number of data required to create a branch. min_samples_leaf specifies the minimum number of data required to create a leaf. max_leaf_nodes specifies the maximum number of leaves. ccp_alpha is a parameter for pruning the decision tree to account for tree complexity class_weight specifies the weighting of classes in classification. sklearn.tree.DecisionTreeRegressor parrt/dtreeviz import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeRegressor from sklearn.datasets import make_regression from mpl_toolkits.mplot3d import Axes3D from dtreeviz.trees import dtreeviz, rtreeviz_bivar_3D Applying a decision tree to simple dataset # # dataset X, y = make_regression(n_samples=100, n_features=2, random_state=11) # train decision tree dt = DecisionTreeRegressor(max_depth=3) dt.fit(X, y) # visualize fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;MPG\u0026#34;, elev=40, azim=120, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() Mempelajari pohon keputusan dengan berbagai parameter # Mari kita periksa bagaimana pohon keputusan dengan struktur yang sedikit kompleks berperilaku ketika parameter pohon keputusan diubah. Pertama, periksa pohon keputusan dengan nilai default untuk semua parameter kecuali max_depth=3.\nX, y = make_regression( n_samples=500, n_features=2, effective_rank=4, noise=0.1, random_state=1 ) plt.figure(figsize=(10, 10)) plt.scatter(X[:, 0], X[:, 1], c=y) plt.show() dt = DecisionTreeRegressor(max_depth=3, random_state=117117) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() max_depth = 10 # When the value of max_depth is large, a deeper and more complex tree is created. This can represent complex rules, but may be over-fitting if the number of data is small.\ndt = DecisionTreeRegressor(max_depth=10, random_state=117117) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() max-depth=5 # dt = DecisionTreeRegressor(max_depth=5, random_state=117117) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() min_samples_split=60 # Specifies the minimum number of data required to create a single split. Smaller numbers of min_samples_split allow for more detailed rules. If you increase the number, you can avoid over-fittinging.\ndt = DecisionTreeRegressor(max_depth=5, min_samples_split=60, random_state=117117) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() ccp_alpha=0.4 # This parameter penalizes the complexity of the tree. The higher the value of ccp_alpha, the simpler the tree will be.\ndt = DecisionTreeRegressor(max_depth=5, random_state=117117, ccp_alpha=0.4) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() max_leaf_nodes=5 # This parameter specifies the number of leaves that will eventually be created. The number of max_leaf_nodes matches the number of parcels.\ndt = DecisionTreeRegressor(max_depth=5, random_state=117117, max_leaf_nodes=5) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() when dataset contain outliers # Specify which criterion to apply when creating a branch. Let\u0026rsquo;s see how the tree changes when criterion=\u0026quot;squared_error\u0026quot; is specified with outliers. Since squared_error penalizes outliers more strongly than absolute_error, it is expected that a decision tree branch will be created if squared_error is specified.\n## Multiply some data values by 5 as outlier X, y = make_regression(n_samples=100, n_features=2, random_state=11) y[1:20] = y[1:20] * 5 dt = DecisionTreeRegressor(max_depth=5, random_state=117117, criterion=\u0026#34;absolute_error\u0026#34;) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() dt = DecisionTreeRegressor(max_depth=5, random_state=117117, criterion=\u0026#34;squared_error\u0026#34;) dt.fit(X, y) fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(111, projection=\u0026#34;3d\u0026#34;) t = rtreeviz_bivar_3D( dt, X, y, feature_names=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;], target_name=\u0026#34;y\u0026#34;, elev=40, azim=240, dist=8.0, show={\u0026#34;splits\u0026#34;, \u0026#34;title\u0026#34;}, ax=ax, ) plt.show() "},{"id":50,"href":"/en/visualize/category-groupby/pie-dounut/","title":"Doughnut chart","section":"Numeric Value Distribution","content":"A doughnut chart (doughnut graph) is a type of pie chart used to display ratios by category, with a blank space in the middle. The blank space has no special meaning, but it can be used to display overall statistics (e.g., \u0026ldquo;Total XXX yen\u0026rdquo;). Donut charts are created in python using matplotlib.pyplot.pie.\nimport matplotlib.pyplot as plt # Data percent = [40, 20, 20, 10, 10] explode = [0, 0, 0, 0, 0] labels = [\u0026#34;米国\u0026#34;, \u0026#34;エマージング\u0026#34;, \u0026#34;日本\u0026#34;, \u0026#34;欧州\u0026#34;, \u0026#34;その他\u0026#34;] percent.reverse() explode.reverse() labels.reverse() # Create pie chart plt.figure(figsize=(7, 7)) plt.pie(x=percent, labels=labels, explode=explode, autopct=\u0026#34;%1.0f%%\u0026#34;, startangle=90) # Add a blank circle in the middle background_color = \u0026#34;#fff\u0026#34; p = plt.gcf() p.gca().add_artist(plt.Circle((0, 0), 0.8, color=background_color)) plt.show() "},{"id":51,"href":"/en/timeseries/exponential_smoothing/003-haw-es/","title":"Holt-Winters Method","section":"Exponential smoothing","content":"import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from statsmodels.tsa.holtwinters import ExponentialSmoothing from statsmodels.tsa.holtwinters import Holt from statsmodels.tsa.holtwinters import SimpleExpSmoothing Sample Data # The data used is from Natural Gas Consumption (NATURALGAS). The unit of measurement is BCF (billion cubic feet).\ndata = { \u0026#34;value\u0026#34;: { \u0026#34;2013-01-01\u0026#34;: 2878.8, \u0026#34;2013-02-01\u0026#34;: 2567.2, \u0026#34;2013-03-01\u0026#34;: 2521.1, \u0026#34;2013-04-01\u0026#34;: 1967.5, \u0026#34;2013-05-01\u0026#34;: 1752.5, \u0026#34;2013-06-01\u0026#34;: 1742.9, \u0026#34;2013-07-01\u0026#34;: 1926.3, \u0026#34;2013-08-01\u0026#34;: 1927.4, \u0026#34;2013-09-01\u0026#34;: 1767.0, \u0026#34;2013-10-01\u0026#34;: 1866.8, \u0026#34;2013-11-01\u0026#34;: 2316.9, \u0026#34;2013-12-01\u0026#34;: 2920.8, \u0026#34;2014-01-01\u0026#34;: 3204.1, \u0026#34;2014-02-01\u0026#34;: 2741.2, \u0026#34;2014-03-01\u0026#34;: 2557.9, \u0026#34;2014-04-01\u0026#34;: 1961.7, \u0026#34;2014-05-01\u0026#34;: 1810.2, \u0026#34;2014-06-01\u0026#34;: 1745.4, \u0026#34;2014-07-01\u0026#34;: 1881.0, \u0026#34;2014-08-01\u0026#34;: 1933.1, \u0026#34;2014-09-01\u0026#34;: 1809.3, \u0026#34;2014-10-01\u0026#34;: 1912.8, \u0026#34;2014-11-01\u0026#34;: 2357.5, \u0026#34;2014-12-01\u0026#34;: 2679.2, \u0026#34;2015-01-01\u0026#34;: 3115.0, \u0026#34;2015-02-01\u0026#34;: 2925.2, \u0026#34;2015-03-01\u0026#34;: 2591.3, \u0026#34;2015-04-01\u0026#34;: 2007.9, \u0026#34;2015-05-01\u0026#34;: 1858.2, \u0026#34;2015-06-01\u0026#34;: 1899.9, \u0026#34;2015-07-01\u0026#34;: 2067.7, \u0026#34;2015-08-01\u0026#34;: 2052.7, \u0026#34;2015-09-01\u0026#34;: 1901.3, \u0026#34;2015-10-01\u0026#34;: 1987.3, \u0026#34;2015-11-01\u0026#34;: 2249.1, \u0026#34;2015-12-01\u0026#34;: 2588.2, \u0026#34;2016-01-01\u0026#34;: 3091.7, \u0026#34;2016-02-01\u0026#34;: 2652.3, \u0026#34;2016-03-01\u0026#34;: 2356.3, \u0026#34;2016-04-01\u0026#34;: 2083.9, \u0026#34;2016-05-01\u0026#34;: 1965.8, \u0026#34;2016-06-01\u0026#34;: 2000.7, \u0026#34;2016-07-01\u0026#34;: 2186.6, \u0026#34;2016-08-01\u0026#34;: 2208.4, \u0026#34;2016-09-01\u0026#34;: 1947.8, \u0026#34;2016-10-01\u0026#34;: 1925.2, \u0026#34;2016-11-01\u0026#34;: 2159.5, \u0026#34;2016-12-01\u0026#34;: 2866.3, \u0026#34;2017-01-01\u0026#34;: 2913.8, \u0026#34;2017-02-01\u0026#34;: 2340.2, \u0026#34;2017-03-01\u0026#34;: 2523.3, \u0026#34;2017-04-01\u0026#34;: 1932.0, \u0026#34;2017-05-01\u0026#34;: 1892.0, \u0026#34;2017-06-01\u0026#34;: 1910.4, \u0026#34;2017-07-01\u0026#34;: 2141.6, \u0026#34;2017-08-01\u0026#34;: 2093.8, \u0026#34;2017-09-01\u0026#34;: 1920.5, \u0026#34;2017-10-01\u0026#34;: 2031.5, \u0026#34;2017-11-01\u0026#34;: 2357.3, \u0026#34;2017-12-01\u0026#34;: 3086.0, \u0026#34;2018-01-01\u0026#34;: 3340.9, \u0026#34;2018-02-01\u0026#34;: 2710.7, \u0026#34;2018-03-01\u0026#34;: 2796.7, \u0026#34;2018-04-01\u0026#34;: 2350.5, \u0026#34;2018-05-01\u0026#34;: 2055.0, \u0026#34;2018-06-01\u0026#34;: 2063.1, \u0026#34;2018-07-01\u0026#34;: 2350.7, \u0026#34;2018-08-01\u0026#34;: 2313.8, \u0026#34;2018-09-01\u0026#34;: 2156.1, \u0026#34;2018-10-01\u0026#34;: 2285.9, \u0026#34;2018-11-01\u0026#34;: 2715.9, \u0026#34;2018-12-01\u0026#34;: 2999.5, \u0026#34;2019-01-01\u0026#34;: 3424.3, \u0026#34;2019-02-01\u0026#34;: 3019.1, \u0026#34;2019-03-01\u0026#34;: 2927.8, \u0026#34;2019-04-01\u0026#34;: 2212.4, \u0026#34;2019-05-01\u0026#34;: 2134.0, \u0026#34;2019-06-01\u0026#34;: 2119.3, \u0026#34;2019-07-01\u0026#34;: 2393.9, \u0026#34;2019-08-01\u0026#34;: 2433.9, \u0026#34;2019-09-01\u0026#34;: 2206.3, \u0026#34;2019-10-01\u0026#34;: 2306.5, \u0026#34;2019-11-01\u0026#34;: 2783.8, \u0026#34;2019-12-01\u0026#34;: 3170.7, \u0026#34;2020-01-01\u0026#34;: 3320.6, \u0026#34;2020-02-01\u0026#34;: 3058.5, \u0026#34;2020-03-01\u0026#34;: 2722.0, \u0026#34;2020-04-01\u0026#34;: 2256.9, \u0026#34;2020-05-01\u0026#34;: 2072.2, \u0026#34;2020-06-01\u0026#34;: 2127.9, \u0026#34;2020-07-01\u0026#34;: 2464.1, \u0026#34;2020-08-01\u0026#34;: 2399.5, \u0026#34;2020-09-01\u0026#34;: 2151.2, \u0026#34;2020-10-01\u0026#34;: 2315.9, \u0026#34;2020-11-01\u0026#34;: 2442.0, \u0026#34;2020-12-01\u0026#34;: 3182.8, \u0026#34;2021-01-01\u0026#34;: 3343.9, \u0026#34;2021-02-01\u0026#34;: 3099.2, \u0026#34;2021-03-01\u0026#34;: 2649.4, \u0026#34;2021-04-01\u0026#34;: 2265.1, \u0026#34;2021-05-01\u0026#34;: 2117.4, \u0026#34;2021-06-01\u0026#34;: 2238.4, \u0026#34;2021-07-01\u0026#34;: 2412.2, \u0026#34;2021-08-01\u0026#34;: 2433.8, \u0026#34;2021-09-01\u0026#34;: 2142.3, \u0026#34;2021-10-01\u0026#34;: 2262.6, \u0026#34;2021-11-01\u0026#34;: 2693.3, \u0026#34;2021-12-01\u0026#34;: 3007.3, \u0026#34;2022-01-01\u0026#34;: 3612.1, \u0026#34;2022-02-01\u0026#34;: 3064.2, \u0026#34;2022-03-01\u0026#34;: 2785.4, \u0026#34;2022-04-01\u0026#34;: 2379.3, \u0026#34;2022-05-01\u0026#34;: 2247.8, \u0026#34;2022-06-01\u0026#34;: 2326.9, \u0026#34;2022-07-01\u0026#34;: 2597.9, \u0026#34;2022-08-01\u0026#34;: 2566.1, \u0026#34;2022-09-01\u0026#34;: 2263.3, } } data = pd.DataFrame(data) data.rename(columns={\u0026#34;value\u0026#34;: \u0026#34;Natural Gas Consumption(BCF)\u0026#34;}, inplace=True) data.index = pd.to_datetime(data.index) data = data.asfreq(\u0026#34;MS\u0026#34;) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data) plt.grid() plt.show() Data Splitting # To evaluate the forecasting performance, the data is split, using the data from 2020 onwards for validation.\ndata_train = data[data.index \u0026lt; \u0026#34;2020-1-1\u0026#34;] data_test = data[data.index \u0026gt;= \u0026#34;2020-1-1\u0026#34;] plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.legend() plt.grid() Model Training and Forecasting # Specify the parameters while referring to statsmodels.tsa.holtwinters.ExponentialSmoothing.\nses = SimpleExpSmoothing(data_train) ses = ses.fit(smoothing_level=0.1) ses_pred = ses.forecast(33) holt = Holt(data_train) holt = holt.fit(smoothing_level=0.1, smoothing_trend=0.2) holt_pred = holt.forecast(33) hw = ExponentialSmoothing( data_train, trend=\u0026#34;additive\u0026#34;, seasonal=\u0026#34;add\u0026#34;, seasonal_periods=12 ) hw = hw.fit() hw_pred = hw.forecast(33) plt.figure(figsize=(12, 6)) plt.plot(data_train.index, data_train.values, label=\u0026#34;train\u0026#34;, linewidth=2) plt.plot(data_test.index, data_test.values, \u0026#34;-.\u0026#34;, label=\u0026#34;test\u0026#34;) plt.plot(ses_pred.index, ses_pred.values, \u0026#34;-.\u0026#34;, label=\u0026#34;prediction(ses)\u0026#34;) plt.plot(holt_pred.index, holt_pred.values, \u0026#34;-.\u0026#34;, label=\u0026#34;prediction(Holt)\u0026#34;) plt.plot(hw_pred.index, hw_pred.values, \u0026#34;-o\u0026#34;, label=\u0026#34;prediction(Holt-Winters)\u0026#34;) plt.legend() plt.grid() "},{"id":52,"href":"/en/basic/dimensionality_reduction/lda/","title":"LDA","section":"Dimensionality Reduction","content":"import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_blobs Experimental Data # sklearn.datasets.make_blobs\nX, y = make_blobs( n_samples=600, n_features=3, random_state=11711, cluster_std=4, centers=3 ) fig = plt.figure(figsize=(8, 8)) ax = fig.add_subplot(projection=\u0026#34;3d\u0026#34;) ax.scatter(X[:, 0], X[:, 1], X[:, 2], marker=\u0026#34;o\u0026#34;, c=y) ax.set_xlabel(\u0026#34;$x_1$\u0026#34;) ax.set_ylabel(\u0026#34;$x_2$\u0026#34;) ax.set_zlabel(\u0026#34;$x_3$\u0026#34;) Dimensionality Reduction to Two Dimensions Using LDA # sklearn.discriminant_analysis.LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA lda = LDA(n_components=2).fit(X, y) X_lda = lda.transform(X) fig = plt.figure(figsize=(8, 8)) plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y, alpha=0.5) Comparison of PCA and LDA # from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler pca = PCA(n_components=2) X_pca = pca.fit_transform(X) fig = plt.figure(figsize=(8, 8)) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, alpha=0.5) "},{"id":53,"href":"/en/basic/classification/","title":"Linear Classification","section":"Machine Learning","content":" Chapter 2 # Linear Classification # "},{"id":54,"href":"/en/prep/numerical/","title":"Numerical Data","section":"Preprocess","content":" Chapter 1 # numerical data # "},{"id":55,"href":"/en/basic/regression/robust_regression/","title":"Outliers and Robustness","section":"Linear Regression","content":" import japanize_matplotlib import matplotlib.pyplot as plt import numpy as np Visualisation of Huber loss # def huber_loss(y_pred: float, y: float, delta=1.0): \u0026#34;\u0026#34;\u0026#34;HuberLoss\u0026#34;\u0026#34;\u0026#34; huber_1 = 0.5 * (y - y_pred) ** 2 huber_2 = delta * (np.abs(y - y_pred) - 0.5 * delta) return np.where(np.abs(y - y_pred) \u0026lt;= delta, huber_1, huber_2) delta = 1.5 x_vals = np.arange(-2, 2, 0.01) y_vals = np.where( np.abs(x_vals) \u0026lt;= delta, 0.5 * np.square(x_vals), delta * (np.abs(x_vals) - 0.5 * delta), ) # plot graph fig = plt.figure(figsize=(8, 8)) plt.plot(x_vals, x_vals ** 2, \u0026#34;red\u0026#34;, label=r\u0026#34;$(y-\\hat{y})^2$\u0026#34;) ## Squared error plt.plot(x_vals, np.abs(x_vals), \u0026#34;orange\u0026#34;, label=r\u0026#34;$|y-\\hat{y}|$\u0026#34;) ## Absolute error plt.plot( x_vals, huber_loss(x_vals * 2, x_vals), \u0026#34;green\u0026#34;, label=r\u0026#34;huber-loss\u0026#34; ) # Huber-loss plt.axhline(y=0, color=\u0026#34;k\u0026#34;) plt.grid(True) plt.legend() plt.show() Comparison with the least-squares method # Prepare data for the experiment # To compare regression with Huber loss and normal linear regression, one outlier is intentionally included in the data.\nN = 30 x1 = np.array([i for i in range(N)]) x2 = np.array([i for i in range(N)]) X = np.array([x1, x2]).T epsilon = np.array([np.random.random() for i in range(N)]) y = 5 * x1 + 10 * x2 + epsilon * 10 y[5] = 500 plt.figure(figsize=(8, 8)) plt.plot(x1, y, \u0026#34;ko\u0026#34;, label=\u0026#34;data\u0026#34;) plt.legend() plt.show() Compare with least squares, ridge regression and huber regression # sklearn.linear_model.HuberRegressor\nPre-processing method: labeling outliers①\nfrom sklearn.datasets import make_regression from sklearn.linear_model import HuberRegressor, Ridge from sklearn.linear_model import LinearRegression plt.figure(figsize=(8, 8)) huber = HuberRegressor(alpha=0.0, epsilon=3) huber.fit(X, y) plt.plot(x1, huber.predict(X), \u0026#34;green\u0026#34;, label=\u0026#34;huber regression\u0026#34;) ridge = Ridge(alpha=0.0, random_state=0) ridge.fit(X, y) plt.plot(x1, ridge.predict(X), \u0026#34;orange\u0026#34;, label=\u0026#34;ridge regression\u0026#34;) lr = LinearRegression() lr.fit(X, y) plt.plot(x1, lr.predict(X), \u0026#34;r-\u0026#34;, label=\u0026#34;least square regression\u0026#34;) plt.plot(x1, y, \u0026#34;x\u0026#34;) plt.plot(x1, y, \u0026#34;ko\u0026#34;, label=\u0026#34;data\u0026#34;) plt.legend() plt.show() "},{"id":56,"href":"/en/prep/","title":"Preprocess","section":"Home","content":" Section 3 # Preprocess # Data preprocessing is the processing or deletion of data prior to its use to ensure or improve performance.\n"},{"id":57,"href":"/en/finance/visualize/003-radar-circle/","title":"Radar chart","section":"Visualize","content":"A radar chart is one method of comparing multiple items together. It is useful when checking whether multiple items are balanced high or low. It is easier to compare when all items are either \u0026ldquo;higher the better\u0026rdquo; or \u0026ldquo;lower the better\u0026rdquo;.\nA radar chart is a graph that expresses a variable with multiple items on a regular polygon without converting it into a composition ratio. The center of the chart is 0, and the larger the value of each item, the further outward it is represented. It is mainly used to compare the performance of entities that have these items as attributes.\nimport matplotlib.pyplot as plt import pandas as pd from math import pi df = pd.DataFrame( index=[\u0026#34;$AAA\u0026#34;, \u0026#34;$BBB\u0026#34;, \u0026#34;$CCC\u0026#34;], data={ \u0026#34;EPS\u0026#34;: [1, 2, 3], \u0026#34;Revenue\u0026#34;: [3, 3, 2], \u0026#34;Guidance\u0026#34;: [1, 2, 3], \u0026#34;D/E\u0026#34;: [3, 2, 1], \u0026#34;PER\u0026#34;: [1, 2, 3], \u0026#34;Dividend\u0026#34;: [2, 3, 3], }, ) Plotting a radar chart # matplotlib.projections set_theta_offset(offset) plt.figure(figsize=(7, 7)) ax = plt.subplot(111, polar=True) ax.set_theta_offset(pi / 2.0) ax.set_theta_direction(-1) # adjust the position of each label angles = [2 * n * pi / len(df.columns) for n in range(len((df.columns)))] plt.xticks(angles, df.columns, size=20) ax.set_rlabel_position(0) plt.yticks([1, 2, 3], [\u0026#34;★\u0026#34;, \u0026#34;★★\u0026#34;, \u0026#34;★★★\u0026#34;], color=\u0026#34;grey\u0026#34;, size=13) plt.ylim(0, 3.5) # Fill in the specified area for ticker_symbol in [\u0026#34;$AAA\u0026#34;, \u0026#34;$BBB\u0026#34;, \u0026#34;$CCC\u0026#34;]: values = df.loc[ticker_symbol].values.flatten().tolist() ax.plot( angles + [0], values + [values[0]], linewidth=1, linestyle=\u0026#34;solid\u0026#34;, c=\u0026#34;#000\u0026#34;, label=ticker_symbol, ) ax.fill(angles + [0], values + [values[0]], \u0026#34;#aaa\u0026#34;, alpha=0.2) plt.legend(bbox_to_anchor=(0.9, 1.1)) plt.show() "},{"id":58,"href":"/en/visualize/distribution/ridgelineplot/","title":"Ridgeline plot","section":"Category \u0026 Number","content":"Charts used to visualize the distribution of multiple groups and their differences. Since the chart of distribution is superimposed, it is easy to visualize slight differences in distribution and differences/changes in the position of the vertices for each group.\nridgeplot: beautiful ridgeline plots in Python\nimport numpy as np import seaborn as sns from ridgeplot import ridgeplot # List of columns to be visualized columns = [\u0026#34;sepal_length\u0026#34;, \u0026#34;sepal_width\u0026#34;, \u0026#34;petal_length\u0026#34;, \u0026#34;petal_width\u0026#34;] # Sample data df = sns.load_dataset(\u0026#34;iris\u0026#34;) df = df[columns] # ridgeline plot fig = ridgeplot( samples=df.values.T, labels=columns, colorscale=\u0026#34;viridis\u0026#34;, coloralpha=0.6 ) fig.update_layout(height=500, width=800) fig.show() !\n"},{"id":59,"href":"/en/timeseries/preprocess/003-seasonal-decompose/","title":"Trend \u0026 Periodicity","section":"Plotting and Preprocessing","content":" import japanize_matplotlib import matplotlib.pyplot as plt import numpy as np from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.tsa.seasonal import STL Sample data created # The data is a combination of several periodic values, and the trend changes in a segmented manner. Noise is also added by np.random.rand().\ndate_list = pd.date_range(\u0026#34;2021-01-01\u0026#34;, periods=365, freq=\u0026#34;D\u0026#34;) value_list = [ 10 + i % 14 + np.log(1 + i % 28 + np.random.rand()) + np.sqrt(1 + i % 7 + np.random.rand()) * 2 + (((i - 100) / 10)) * (i \u0026gt; 100) - ((i - 200) / 7) * (i \u0026gt; 200) + np.random.rand() for i, di in enumerate(date_list) ] df = pd.DataFrame( { \u0026#34;日付\u0026#34;: date_list, \u0026#34;観測値\u0026#34;: value_list, } ) df.head(10) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(10, 5)) sns.lineplot(x=df[\u0026#34;日付\u0026#34;], y=df[\u0026#34;観測値\u0026#34;]) plt.grid(axis=\u0026#34;x\u0026#34;) plt.show() Decompose into trend, periodicity, and residuals # Time series data using statsmodels.tsa.seasonal.STL\nTrend (.trend) Seasonal/periodic(.seasonal) Residual(.resid) (In the following code, dr means DecomposeResult.\nCleveland, Robert B., et al. \u0026ldquo;STL: A seasonal-trend decomposition.\u0026rdquo; J. Off. Stat 6.1 (1990): 3-73. (pdf)\nperiod = 28 stl = STL(df[\u0026#34;観測値\u0026#34;], period=period) dr = stl.fit() _, axes = plt.subplots(figsize=(12, 8), ncols=1, nrows=4, sharex=True) axes[0].set_title(\u0026#34;観測値\u0026#34;) axes[0].plot(dr.observed) axes[0].grid() axes[1].set_title(\u0026#34;トレンド\u0026#34;) axes[1].plot(dr.trend) axes[1].grid() axes[2].set_title(\u0026#34;季節性\u0026#34;) axes[2].plot(dr.seasonal) axes[2].grid() axes[3].set_title(\u0026#34;その他の要因・残差\u0026#34;) axes[3].plot(dr.resid) axes[3].grid() plt.tight_layout() plt.show() Check for resistance to outliers # def check_outlier(): period = 28 df_outlier = df[\u0026#34;観測値\u0026#34;].copy() for i in range(1, 6): df_outlier[i * 50] = df_outlier[i * 50] * 1.4 stl = STL(df_outlier, period=period, trend=31) dr_outlier = stl.fit() _, axes = plt.subplots(figsize=(12, 8), ncols=1, nrows=4, sharex=True) axes[0].set_title(\u0026#34;観測値\u0026#34;) axes[0].plot(dr_outlier.observed) axes[0].grid() axes[1].set_title(\u0026#34;トレンド\u0026#34;) axes[1].plot(dr_outlier.trend) axes[1].grid() axes[2].set_title(\u0026#34;季節性\u0026#34;) axes[2].plot(dr_outlier.seasonal) axes[2].grid() axes[3].set_title(\u0026#34;その他の要因・残差\u0026#34;) axes[3].plot(dr_outlier.resid) axes[3].grid() check_outlier() Check to see if the trend is extracted # If the trend is correctly extracted, the trend should not contain a cyclical component and the PACF (Partial Autocorrelation) should be close to zero.\nstatsmodels.tsa.stattools.acf statsmodels.tsa.stattools.pacf from statsmodels.graphics.tsaplots import plot_acf, plot_pacf _, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) plt.suptitle(\u0026#34;トレンドの自己相関・偏自己相関\u0026#34;) plot_acf(dr.trend.dropna(), ax=axes[0]) plot_pacf(dr.trend.dropna(), method=\u0026#34;ywm\u0026#34;, ax=axes[1]) plt.tight_layout() plt.show() _, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) plt.suptitle(\u0026#34;残差の自己相関・偏自己相関\u0026#34;) plot_acf(dr.resid.dropna(), ax=axes[0]) plot_pacf(dr.resid.dropna(), method=\u0026#34;ywm\u0026#34;, ax=axes[1]) plt.tight_layout() plt.show() Portmanteau test # from statsmodels.stats.diagnostic import acorr_ljungbox acorr_ljungbox(dr.resid.dropna(), lags=int(np.log(df.shape[0]))) .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":60,"href":"/en/timeseries/shape/004-ts-extract-features/","title":"tsfresh","section":"Shape \u0026 Similarity","content":"\rtsfresh — tsfresh 0.18.1.dev documentation\ntsfresh # Overview on extracted featuresを参考に、どんな特徴量が作成されるか確認してみます。\nimport os import numpy as np import pandas as pd import matplotlib.pyplot as plt from tsfresh import extract_features X = [] for id, it in enumerate(np.linspace(0.1, 100, 100)): for jt in range(10): X.append( [ id, jt, jt + np.sin(it), jt % 2 + np.cos(it), jt % 3 + np.tan(it), np.log(it + jt), ] ) X = pd.DataFrame(X) X.columns = [\u0026#34;id\u0026#34;, \u0026#34;time\u0026#34;, \u0026#34;fx1\u0026#34;, \u0026#34;fx2\u0026#34;, \u0026#34;fx3\u0026#34;, \u0026#34;fx4\u0026#34;] X.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rX[X[\u0026#34;id\u0026#34;] == 3].plot(subplots=True, sharex=True, figsize=(12, 10)) plt.show() Calculating Features # tsfresh.feature_extraction package\nYou can calculate all features at once using the extract_features function. Additionally, you can perform feature selection using functions available under tsfresh.feature_selection.\nextracted_features = extract_features(X, column_id=\u0026#34;id\u0026#34;, column_sort=\u0026#34;time\u0026#34;) extracted_features.head() Feature Extraction: 100%|█\r.dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r"},{"id":61,"href":"/en/timeseries/model/","title":"Univariate","section":"TimeSeries","content":" Chapter 3 # Univariate # "},{"id":62,"href":"/en/basic/clustering/x-means/","title":"X-means","section":"Clustering","content":" pyclustering.cluster.xmeans.xmeans Class Reference\nPelleg, Dan, and Andrew W. Moore. \u0026ldquo;X-means: Extending k-means with efficient estimation of the number of clusters.\u0026rdquo; Icml. Vol. 1. 2000.\nimport numpy as np import matplotlib.pyplot as plt from sklearn.cluster import KMeans from sklearn.datasets import make_blobs When k is pre-specified in k-means # def plot_by_kmeans(X, k=5): y_pred = KMeans(n_clusters=k, random_state=random_state, init=\u0026#34;random\u0026#34;).fit_predict( X ) plt.scatter(X[:, 0], X[:, 1], c=y_pred, marker=\u0026#34;x\u0026#34;) plt.title(f\u0026#34;k-means, n_clusters={k}\u0026#34;) # Create sample data n_samples = 1000 random_state = 117117 X, _ = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=1, centers=10 ) # Run k-means++. plot_by_kmeans(X) Run without specifying the number of clusters in x-mean # BIC(bayesian information criterion) # from pyclustering.cluster.xmeans import xmeans from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer BAYESIAN_INFORMATION_CRITERION = 0 MINIMUM_NOISELESS_DESCRIPTION_LENGTH = 1 def plot_by_xmeans( X, c_min=3, c_max=10, criterion=BAYESIAN_INFORMATION_CRITERION, tolerance=0.025 ): initial_centers = kmeans_plusplus_initializer(X, c_min).initialize() xmeans_instance = xmeans( X, initial_centers, c_max, criterion=criterion, tolerance=tolerance ) xmeans_instance.process() # Create data for plots clusters = xmeans_instance.get_clusters() n_samples = X.shape[0] c = [] for i, cluster_i in enumerate(clusters): X_ci = X[cluster_i] color_ci = [i for _ in cluster_i] plt.scatter(X_ci[:, 0], X_ci[:, 1], marker=\u0026#34;x\u0026#34;) plt.title(\u0026#34;x-means\u0026#34;) # Run x-means plot_by_xmeans(X, c_min=3, c_max=10, criterion=BAYESIAN_INFORMATION_CRITERION) MINIMUM_NOISELESS_DESCRIPTION_LENGTH # plot_by_xmeans(X, c_min=3, c_max=10, criterion=MINIMUM_NOISELESS_DESCRIPTION_LENGTH) Influence of tolerance parameter # X, _ = make_blobs( n_samples=2000, random_state=random_state, cluster_std=0.4, centers=10, ) plt.figure(figsize=(25, 5)) for i, ti in enumerate(np.linspace(0.0001, 1, 5)): ti = np.round(ti, 4) plt.subplot(1, 10, i + 1) plot_by_xmeans( X, c_min=3, c_max=10, criterion=BAYESIAN_INFORMATION_CRITERION, tolerance=ti ) plt.title(f\u0026#34;tol={ti}\u0026#34;) Compare k-means and x-means for various data # for i in range(5): X, _ = make_blobs( n_samples=n_samples, random_state=random_state, cluster_std=0.7, centers=5 + i * 5, ) plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plot_by_kmeans(X) plt.subplot(1, 2, 2) plot_by_xmeans(X, c_min=3, c_max=20) plt.show() "},{"id":63,"href":"/en/prep/numerical/yeojonson/","title":"YeoJonson transformation","section":"Numerical Data","content":" scipy.stats.yeojohnson\nUnlike the box-cox transformation, this method can be used even when negative values are included.\nI. Yeo and R.A. Johnson, “A New Family of Power Transformations to Improve Normality or Symmetry”, Biometrika 87.4 (2000):\nfrom scipy import stats import matplotlib.pyplot as plt x = stats.loggamma.rvs(1, size=1000) - 0.5 plt.hist(x) plt.axvline(x=0, color=\u0026#34;r\u0026#34;) # verify that there is data below 0 as well plt.show() import numpy as np from scipy.stats import yeojohnson plt.hist(yeojohnson(x)) plt.show() "},{"id":64,"href":"/en/basic/ensemble/adaboost_regression/","title":"Adaboost(Regression)","section":"Ensemble","content":"import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import AdaBoostRegressor # NOTE: This model is to check the model\u0026#39;s sample_weight class DummyRegressor: def __init__(self): self.model = DecisionTreeRegressor(max_depth=5) self.error_vector = None self.X_for_plot = None self.y_for_plot = None def fit(self, X, y): self.model.fit(X, y) y_pred = self.model.predict(X) # Weights are calculated based on regression error. # https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_weight_boosting.py#L1130 self.error_vector = np.abs(y_pred - y) self.X_for_plot = X.copy() self.y_for_plot = y.copy() return self.model def predict(self, X, check_input=True): return self.model.predict(X) def get_params(self, deep=False): return {} def set_params(self, deep=False): return {} Fit regression models to training data # # training dataset X = np.linspace(-10, 10, 500)[:, np.newaxis] y = (np.sin(X).ravel() + np.cos(4 * X).ravel()) * 10 + 10 + np.linspace(-2, 2, 500) ## train Adaboost regressor reg = AdaBoostRegressor( DummyRegressor(), n_estimators=100, random_state=100, loss=\u0026#34;linear\u0026#34;, learning_rate=0.8, ) reg.fit(X, y) y_pred = reg.predict(X) # Check the fitting to the training data. plt.figure(figsize=(10, 5)) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;訓練データ\u0026#34;) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;prediction\u0026#34;, linewidth=1) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.legend() plt.show() Visualize sample weights (for loss=\u0026lsquo;linear\u0026rsquo;) # Adaboost determines the weights based on the errors of the regression. Visualize the magnitude of the weights when specified as \u0026rsquo;linear\u0026rsquo;. Observe how data with added weights have a higher probability of being sampled during training.\nloss{‘linear’, ‘square’, ‘exponential’}, default=’linear’ The loss function to use when updating the weights after each boosting iteration.\nsklearn.ensemble.AdaBoostRegressor\ndef visualize_weight(reg, X, y, y_pred): \u0026#34;\u0026#34;\u0026#34;Function for plotting the value (number of times sampled) corresponding to the weights of the sample Parameters ---------- reg : sklearn.ensemble._weight_boosting boosting model X : numpy.ndarray training dataset y : numpy.ndarray target y_pred: prediction \u0026#34;\u0026#34;\u0026#34; assert reg.estimators_ is not None, \u0026#34;len(reg.estimators_) \u0026gt; 0\u0026#34; for i, estimators_i in enumerate(reg.estimators_): if i % 100 == 0: # Count how many times the data appears in the data used to create the i-th model weight_dict = {xi: 0 for xi in X.ravel()} for xi in estimators_i.X_for_plot.ravel(): weight_dict[xi] += 1 # Plot the number of occurrences as orange circles on the graph (the more the number, the larger the circle) weight_x_sorted = sorted(weight_dict.items(), key=lambda x: x[0]) weight_vec = np.array([s * 100 for xi, s in weight_x_sorted]) # plot graph plt.figure(figsize=(10, 5)) plt.title(f\u0026#34;Visualization of the weighted sample after creating the {i}-th model, loss={reg.loss}\u0026#34;) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;training data\u0026#34;) plt.scatter( estimators_i.X_for_plot, estimators_i.y_for_plot, marker=\u0026#34;o\u0026#34;, alpha=0.2, c=\u0026#34;orange\u0026#34;, s=weight_vec, ) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;prediction\u0026#34;, linewidth=2) plt.legend(loc=\u0026#34;upper right\u0026#34;) plt.show() ## Create a regression model with loss=\u0026#34;linear reg = AdaBoostRegressor( DummyRegressor(), n_estimators=101, random_state=100, loss=\u0026#34;linear\u0026#34;, learning_rate=1, ) reg.fit(X, y) y_pred = reg.predict(X) visualize_weight(reg, X, y, y_pred) ## Create regression model with loss=\u0026#34;square reg = AdaBoostRegressor( DummyRegressor(), n_estimators=101, random_state=100, loss=\u0026#34;square\u0026#34;, learning_rate=1, ) reg.fit(X, y) y_pred = reg.predict(X) visualize_weight(reg, X, y, y_pred) "},{"id":65,"href":"/en/prep/categorical/","title":"Categorical Data","section":"Preprocess","content":" Chapter 2 # categorical data # "},{"id":66,"href":"/en/basic/tree/","title":"Decision Tree","section":"Machine Learning","content":" Chapter 3 # Decision Tree # "},{"id":67,"href":"/en/basic/dimensionality_reduction/kernel-pca/","title":"Kernel-PCA","section":"Dimensionality Reduction","content":"import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.datasets import make_circles Datos para el experimento # sklearn.datasets.make_circles\nX, y = make_circles(n_samples=400, factor=0.3, noise=0.15) fig = plt.figure(figsize=(8, 8)) plt.scatter(X[:, 0], X[:, 1], c=y) KernelPCA # sklearn.decomposition.KernelPCA\nfrom sklearn.decomposition import KernelPCA kpca = KernelPCA(kernel=\u0026#34;rbf\u0026#34;, degree=2, gamma=3) X_kpca = kpca.fit_transform(X) fig = plt.figure(figsize=(8, 8)) plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y) \u0026lt;matplotlib.collections.PathCollection at 0x7fc8e28323a0\u0026gt;\r"},{"id":68,"href":"/en/eval/","title":"Metrics","section":"Home","content":" Section 4 # Metrics # "},{"id":69,"href":"/en/timeseries/model_mv/","title":"Multi-variate","section":"TimeSeries","content":" Chapter 4 # Multi-variate # "},{"id":70,"href":"/en/visualize/category-groupby/sankey-diagram/","title":"Sankey Diagram","section":"Numeric Value Distribution","content":"Sankey diagrams are charts that represent the flow rate between steps and can be utilized to visualize how the allocation of amounts and resources is changing. The thickness of the line indicates the amount of flow allocated. This page uses plotly to create a sankey diagram in python.\nPlease note that the following is a visualization of profit and loss for one period for \u0026ldquo;Genco Shipping \u0026amp; Trading Limited\u0026rdquo; and not the latest data.\nSankey Diagram in Python | plotly\nimport plotly.graph_objects as go fig = go.Figure( data=[ go.Sankey( node=dict( pad=300, thickness=20, line=dict(color=\u0026#34;white\u0026#34;, width=0.0), label=[ \u0026#34;Voyage revenues\u0026#34;, \u0026#34;Total revenues\u0026#34;, \u0026#34;Total operating expenses\u0026#34;, \u0026#34;Net income\u0026#34;, \u0026#34;Voyage expenses\u0026#34;, \u0026#34;Vessel operating expenses\u0026#34;, \u0026#34;Charter hire expenses\u0026#34;, \u0026#34;General and administrative expenses\u0026#34;, \u0026#34;Depreciation and amortization\u0026#34;, ], color=[ \u0026#34;#666666\u0026#34;, \u0026#34;#666666\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#2BA02D\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#CC0001\u0026#34;, \u0026#34;#CC0001\u0026#34;, ], ), link=dict( source=[0, 1, 1, 2, 2, 2, 2, 2], target=[1, 2, 3, 4, 5, 6, 7, 8], value=[121008, 84759, 36249, 36702, 18789, 8325, 5854, 13769], color=[ \u0026#34;#B3B3B3\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#9CCC9A\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#E18685\u0026#34;, \u0026#34;#E18685\u0026#34;, ], ), ) ] ) fig.update_layout( title_text=\u0026#34;Condensed Consolidated Statements of Operations\u0026#34;, font_size=18 ) fig.show() "},{"id":71,"href":"/en/visualize/distribution/violinplot/","title":"Violin plot","section":"Category \u0026 Number","content":"Violin plot is a box-and-whisker diagram with a density graph rotated 90 degrees on each side. Violin plot allows comparison of the distribution of values for several groups.\nViolin plot - Wikipedia\nimport matplotlib.pyplot as plt import seaborn as sns df = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.set(rc={\u0026#34;figure.figsize\u0026#34;: (12, 8)}) sns.violinplot(x=df[\u0026#34;species\u0026#34;], y=df[\u0026#34;sepal_length\u0026#34;]) "},{"id":72,"href":"/en/timeseries/model/005-ar-process/","title":"AR Process","section":"Univariate","content":" import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib Create data for AR process # Prepare functions to generate data\ndef create_ARdata(phis=[0.1], N=500, init=1, c=1, sigma=0.3): \u0026#34;\u0026#34;\u0026#34;Generating AR process data\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;==AR({len(phis)}), #data={N}==\u0026#34;) data = np.zeros(N) data[0] = init + np.random.normal(0, sigma) for t in range(2, N): res = c + np.random.normal(0, sigma) for j, phi_j in enumerate(phis): res += phi_j * data[t - j - 1] data[t] = res return data φ \u0026lt; 1 # plt.figure(figsize=(12, 6)) phis = [0.1] ar1_1 = create_ARdata(phis=phis) plt.plot(ar1_1) plt.show() ==AR(1), #data=500==\rφ = 1 # plt.figure(figsize=(12, 6)) phis = [1] ar1_2 = create_ARdata(phis=phis) plt.plot(ar1_2) plt.show() ==AR(1), #data=500==\rφ \u0026gt; 1 # plt.figure(figsize=(12, 6)) phis = [1.04] ar1_2 = create_ARdata(phis=phis) plt.plot(ar1_2) plt.show() ==AR(1), #data=500==\rAR(2) # plt.figure(figsize=(12, 6)) phis = [0.1, 0.3] ar2_1 = create_ARdata(phis=phis, N=100) plt.plot(ar2_1) plt.show() ==AR(2), #data=100==\rplt.figure(figsize=(12, 6)) phis = [0.1, -1.11] ar2_1 = create_ARdata(phis=phis) plt.plot(ar2_1) plt.show() ==AR(2), #data=500==\rEstimate the autoregressive (AR) model # from statsmodels.tsa.ar_model import AutoReg res = AutoReg(ar1_1, lags=1).fit() out = \u0026#34;AIC: {0:0.3f}, HQIC: {1:0.3f}, BIC: {2:0.3f}\u0026#34; print(out.format(res.aic, res.hqic, res.bic)) AIC: 231.486, HQIC: 236.445, BIC: 244.124\rprint(res.params) print(res.sigma2) res.summary() [1.03832755 0.07236388]\r0.09199676371696269\r"},{"id":73,"href":"/en/timeseries/preprocess/004-preprocess-log/","title":"Box-Cox transformation","section":"Plotting and Preprocessing","content":" import japanize_matplotlib import matplotlib.pyplot as plt import numpy as np from scipy import stats import numpy as np plt.figure(figsize=(12, 5)) data_wb = np.random.weibull(2.0, size=50000) plt.hist(data_wb, bins=30, rwidth=0.9) plt.show() plt.figure(figsize=(12, 5)) data_lg = stats.loggamma.rvs(2.0, size=50000) plt.hist(data_lg, bins=30, rwidth=0.9) plt.show() scipy.stats.boxcox — SciPy v1.8.0 Manual\nfrom scipy.stats import boxcox plt.figure(figsize=(12, 5)) plt.hist(boxcox(data_wb), bins=30, rwidth=0.9) plt.show() try: plt.figure(figsize=(12, 5)) plt.hist(boxcox(data_lg), bins=30, rwidth=0.9) plt.show() except ValueError as e: print(f\u0026#34;エラーの内容： ValueError {e.args}\u0026#34;) エラーの内容： ValueError ('Data must be positive.',)\r\u0026lt;Figure size 864x360 with 0 Axes\u0026gt;\rscipy.stats.yeojohnson — SciPy v1.8.0 Manual\nfrom scipy.stats import yeojohnson plt.figure(figsize=(12, 5)) plt.hist(yeojohnson(data_lg), bins=30, rwidth=0.9) plt.show() Fitting Ridge Regression # If we apply ridge regression without making the distribution of y closer to a normal distribution, we find that the distribution of the residuals is biased.\nfrom sklearn.linear_model import Ridge N = 1000 rng = np.random.RandomState(0) y = np.random.weibull(2.0, size=N) X = rng.randn(N, 5) X[:, 0] = np.sqrt(y) + np.random.rand(N) / 10 plt.figure(figsize=(12, 5)) plt.hist(y, bins=20, rwidth=0.9) plt.title(\u0026#34;yの分布\u0026#34;) plt.show() clf = Ridge(alpha=1.0) clf.fit(X, y) pred = clf.predict(X) plt.figure(figsize=(12, 6)) plt.subplot(121) plt.title(\u0026#34;正解と出力の分布\u0026#34;) plt.scatter(y, pred) plt.plot([0, 2], [0, 2], \u0026#34;r\u0026#34;) plt.xlabel(\u0026#34;正解\u0026#34;) plt.ylabel(\u0026#34;出力\u0026#34;) plt.xlim(0, 2) plt.ylim(0, 2) plt.grid() plt.subplot(122) plt.title(\u0026#34;残差の分布\u0026#34;) plt.hist(y - pred) plt.xlim(-0.5, 0.5) plt.show() clf = Ridge(alpha=1.0) clf.fit(X, yeojohnson(y)[0]) pred = clf.predict(X) plt.figure(figsize=(12, 6)) plt.subplot(121) plt.title(\u0026#34;正解と出力の分布\u0026#34;) plt.scatter(yeojohnson(y)[0], pred) plt.plot([0, 2], [0, 2], \u0026#34;r\u0026#34;) plt.xlabel(\u0026#34;正解\u0026#34;) plt.ylabel(\u0026#34;出力\u0026#34;) plt.xlim(0, 2) plt.ylim(0, 2) plt.grid() plt.subplot(122) plt.title(\u0026#34;残差の分布\u0026#34;) plt.hist(yeojohnson(y)[0] - pred) plt.xlim(-0.15, 0.15) plt.show() "},{"id":74,"href":"/en/basic/ensemble/","title":"Ensemble","section":"Machine Learning","content":" Chapter 4 # Ensemble # "},{"id":75,"href":"/en/basic/ensemble/gradient_boosting1/","title":"Gradient boosting","section":"Ensemble","content":"import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from sklearn.ensemble import GradientBoostingRegressor Fit a regression model to training data # Create experimental data, waveform data with trigonometric functions added together.\n# training dataset X = np.linspace(-10, 10, 500)[:, np.newaxis] noise = np.random.rand(X.shape[0]) * 10 # target y = ( (np.sin(X).ravel() + np.cos(4 * X).ravel()) * 10 + 10 + np.linspace(-10, 10, 500) + noise ) # train gradient boosting reg = GradientBoostingRegressor( n_estimators=50, learning_rate=0.5, ) reg.fit(X, y) y_pred = reg.predict(X) # Check the fitting to the training data plt.figure(figsize=(10, 5)) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;training data\u0026#34;) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;Predictions for the final created model\u0026#34;, linewidth=1) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.title(\u0026#34;Degree of fitting to training dataset\u0026#34;) plt.legend() plt.show() Effect of loss function on results # Check how fitting to the training data changes when loss is changed to [\u0026ldquo;squared_error\u0026rdquo;, \u0026ldquo;absolute_error\u0026rdquo;, \u0026ldquo;huber\u0026rdquo;, \u0026ldquo;quantile\u0026rdquo;].\u0026quot; It is expected that \u0026ldquo;absolute_error\u0026rdquo; and \u0026ldquo;huber\u0026rdquo; will not go on to predict outliers since the penalty for outliers is not as large as the squared error.\nsklearn/ensemble/_gb_losses.py sklearn.ensemble.GradientBoostingRegressor # training data X = np.linspace(-10, 10, 500)[:, np.newaxis] # prepare outliers noise = np.random.rand(X.shape[0]) * 10 for i, ni in enumerate(noise): if i % 80 == 0: noise[i] = 70 + np.random.randint(-10, 10) # target y = ( (np.sin(X).ravel() + np.cos(4 * X).ravel()) * 10 + 10 + np.linspace(-10, 10, 500) + noise ) for loss in [\u0026#34;squared_error\u0026#34;, \u0026#34;absolute_error\u0026#34;, \u0026#34;huber\u0026#34;, \u0026#34;quantile\u0026#34;]: # train gradient boosting reg = GradientBoostingRegressor( n_estimators=50, learning_rate=0.5, loss=loss, ) reg.fit(X, y) y_pred = reg.predict(X) # Check the fitting to the training data. plt.figure(figsize=(10, 5)) plt.scatter(X, y, c=\u0026#34;k\u0026#34;, marker=\u0026#34;x\u0026#34;, label=\u0026#34;training dataset\u0026#34;) plt.plot(X, y_pred, c=\u0026#34;r\u0026#34;, label=\u0026#34;Predictions for the final created model\u0026#34;, linewidth=1) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.title(f\u0026#34;Degree of fitting to training data, loss={loss}\u0026#34;, fontsize=18) plt.legend() plt.show() Effect of n_estimators on results # You can see how the degree of improvement comes to a head when you increase n_estimators to some extent.\nfrom sklearn.metrics import mean_squared_error as MSE # trainig dataset X = np.linspace(-10, 10, 500)[:, np.newaxis] noise = np.random.rand(X.shape[0]) * 10 # target y = ( (np.sin(X).ravel() + np.cos(4 * X).ravel()) * 10 + 10 + np.linspace(-10, 10, 500) + noise ) # Try to create a model with different n_estimators n_estimators_list = [(i + 1) * 5 for i in range(20)] mses = [] for n_estimators in n_estimators_list: reg = GradientBoostingRegressor( n_estimators=n_estimators, learning_rate=0.3, ) reg.fit(X, y) y_pred = reg.predict(X) mses.append(MSE(y, y_pred)) # Plotting mean_squared_error for different n_estimators plt.figure(figsize=(10, 5)) plt.plot(n_estimators_list, mses, \u0026#34;x\u0026#34;) plt.xlabel(\u0026#34;n_estimators\u0026#34;) plt.ylabel(\u0026#34;Mean Squared Error(training data)\u0026#34;) plt.grid() plt.show() Impact of learning_rate on results # If learning_rate is too small, the accuracy does not improve, and if learning_rate is too large, it does not converge.\n# Try to create a model with different n_estimators learning_rate_list = [np.round(0.1 * (i + 1), 1) for i in range(20)] mses = [] for learning_rate in learning_rate_list: reg = GradientBoostingRegressor( n_estimators=30, learning_rate=learning_rate, ) reg.fit(X, y) y_pred = reg.predict(X) mses.append(np.log(MSE(y, y_pred))) # Plotting mean_squared_error for different n_estimators plt.figure(figsize=(10, 5)) plt_index = [i for i in range(len(learning_rate_list))] plt.plot(plt_index, mses, \u0026#34;x\u0026#34;) plt.xticks(plt_index, learning_rate_list, rotation=90) plt.xlabel(\u0026#34;learning_rate\u0026#34;, fontsize=15) plt.ylabel(\u0026#34;log(Mean Squared Error) (training data)\u0026#34;, fontsize=15) plt.grid() plt.show() "},{"id":76,"href":"/en/timeseries/shape/","title":"Shape \u0026 Similarity","section":"TimeSeries","content":" Chapter 3 # Shape \u0026amp; Similarity # "},{"id":77,"href":"/en/prep/table/","title":"Table","section":"Preprocess","content":" Chapter 3 # Table # "},{"id":78,"href":"/en/timeseries/","title":"TimeSeries","section":"Home","content":" Section 5 # Time Series # A time series is a set of values obtained by continuously observing changes in a phenomenon over time.\n"},{"id":79,"href":"/en/timeseries/forecast/","title":"timeseries forecast","section":"TimeSeries","content":" Chapter 3 # "},{"id":80,"href":"/en/timeseries/preprocess/005-inflation-adjustment/","title":"Adjustment","section":"Plotting and Preprocessing","content":"Sometimes data affected by the value of money need to be adjusted before analysis. Here we will use the Consumer Price Index (CPI) to adjust time series data for inflation. Specifically, in this page, I\u0026rsquo;ll try to\u0026hellip;\nObtain CPI data from FRED using the python API Obtain median household income data for the U.S. Adjust the U.S. median household income data using the CPI Compare the median household income data (adjusted) in the U.S. with the data adjusted using the CPI. A Python library that quickly adjusts U.S. dollars for inflation using the Consumer Price Index (CPI).\nimport os import cpi import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from full_fred.fred import Fred from datetime import date cpi.update() # Make sure you do it Get data from FRED # Obtain the necessary data from FRED. The method of data acquisition is explained in a separate video, but it is necessary to issue an API key to access the data from python.\n# FRED_API_KEY = os.getenv(\u0026#39;FRED_API_KEY\u0026#39;) fred = Fred() print(f\u0026#34;FRED API key is set to an environment variable：{fred.env_api_key_found()}\u0026#34;) def get_fred_data(name, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;): df = fred.get_series_df(name)[[\u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;]].copy() df[\u0026#34;date\u0026#34;] = pd.to_datetime(df[\u0026#34;date\u0026#34;]) df[\u0026#34;value\u0026#34;] = pd.to_numeric(df[\u0026#34;value\u0026#34;], errors=\u0026#34;coerce\u0026#34;) df = df.set_index(\u0026#34;date\u0026#34;) if end == \u0026#34;\u0026#34;: df = df.loc[f\u0026#34;{start}\u0026#34;:] else: df = df.loc[f\u0026#34;{start}\u0026#34;:f\u0026#34;{end}\u0026#34;] return df FRED API key is set to an environment variable：True\rHousehold Income in the United States # Let\u0026rsquo;s check the data for Median Household Income in the United States (MEHOINUSA646N). This data is in units of\nUnits: Current Dollars, Not Seasonally Adjusted\nand this data is not adjusted for inflation or seasonally adjusted. We adjust this household data using the inflation rate.\ndata = get_fred_data(\u0026#34;MEHOINUSA646N\u0026#34;, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;) data.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rAdjust values using CPI # data[\u0026#34;adjusted\u0026#34;] = [ cpi.inflate(dollers.value, date.year) for date, dollers in data.iterrows() ] Comparison before and after adjustment # plt.figure(figsize=(12, 6)) sns.lineplot( data=data, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;Current Dollars, Not Seasonally Adjusted(MEHOINUSA646N)\u0026#34;, ) sns.lineplot(data=data, x=\u0026#34;date\u0026#34;, y=\u0026#34;adjusted\u0026#34;, label=\u0026#34;MEHOINUSA646N adjusted\u0026#34;) plt.legend() Real Median Household Income in the United States # Adjusted data are provided in Real Median Household Income in the United States (MEHOINUSA672N). Compare the values in MEHOINUSA672N with the inflation-adjusted data (values in the adjusted column in the data) from earlier. The unadjusted values, adjusted using the Consumer Price Index (CPI), are expected to match the adjusted values (MEHOINUSA672N) almost exactly.\ndata_real = get_fred_data(\u0026#34;MEHOINUSA672N\u0026#34;, start=\u0026#34;2013-01-01\u0026#34;, end=\u0026#34;\u0026#34;) data_real.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rplt.figure(figsize=(12, 6)) sns.lineplot(data=data_real, x=\u0026#34;date\u0026#34;, y=\u0026#34;value\u0026#34;, label=\u0026#34;MEHOINUSA672N\u0026#34;) sns.lineplot(data=data, x=\u0026#34;date\u0026#34;, y=\u0026#34;adjusted\u0026#34;, label=\u0026#34;MEHOINUSA646N adjusted\u0026#34;) for t, v in data_real.iterrows(): plt.text(t, v[0] - 500, f\u0026#34;{v[0]:.2f}\u0026#34;) for t, v in data.iterrows(): plt.text(t, v[1] + 500, f\u0026#34;{v[1]:.2f}\u0026#34;) plt.legend() "},{"id":81,"href":"/en/basic/clustering/","title":"Clustering","section":"Machine Learning","content":" Chapter 5 # Clustering # "},{"id":82,"href":"/en/timeseries/model/005-2-ma-process/","title":"MA Process","section":"Univariate","content":"import statsmodels.api as sm import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib Generate data for MA process # Prepare functions to generate data\ndef create_MAdata(thetas=[0.1], mu=1, N=400, init=1, c=1, sigma=0.3): \u0026#34;\u0026#34;\u0026#34;GGenerate MA Process data\u0026#34;\u0026#34;\u0026#34; epsilon = np.random.normal(loc=0, scale=sigma, size=N) data = np.zeros(N) data[0] = init for t in range(2, N): res = mu + epsilon[t] for j, theta_j in enumerate(thetas): res += theta_j * epsilon[t - j - 1] data[t] = res return data MA(1) # plt.figure(figsize=(12, 6)) thetas = [0.5] ma1_1 = create_MAdata(thetas=thetas) plt.plot(ma1_1) plt.show() MA(2) # plt.figure(figsize=(12, 6)) thetas = [0.5, 0.5] ma1_2 = create_MAdata(thetas=thetas) plt.plot(ma1_1) plt.show() MA(5) # plt.figure(figsize=(12, 6)) thetas = [0.5 for _ in range(10)] ma1_5 = create_MAdata(thetas=thetas) plt.plot(ma1_5) plt.show() "},{"id":83,"href":"/en/prep/special/","title":"Others","section":"Preprocess","content":" Chapter 4 # Others # "},{"id":84,"href":"/en/visualize/","title":"Visualization","section":"Home","content":" Section 6 # Visualization # "},{"id":85,"href":"/en/basic/dimensionality_reduction/","title":"Dimensionality Reduction","section":"Machine Learning","content":" Chapter 6 # Dimensionality Reduction # "},{"id":86,"href":"/en/finance/","title":"Economic Data","section":"Home","content":" Section 7 # Economic Data # Wikipedia Economic indicator\n"},{"id":87,"href":"/en/basic/feature_selection/","title":"Feature Selection","section":"Machine Learning","content":" Chapter 8 # "},{"id":88,"href":"/en/timeseries/hts/","title":"Hierarchical and Grouped Time Series","section":"TimeSeries","content":" Chapter 6 # Hierarchical and Grouped Time Series # "},{"id":89,"href":"/en/basic/timeseries/","title":"Time Series","section":"Machine Learning","content":" Chapter 8 # Time Series # "},{"id":90,"href":"/en/basic/anomaly/","title":"Anomaly detection","section":"Machine Learning","content":" Chapter 9 # Anomaly detection # "},{"id":91,"href":"/en/finance/nlp/","title":"NLP","section":"Economic Data","content":" Chapter 3 # NLP # "},{"id":92,"href":"/en/visualize/tips/","title":"Appendix","section":"Visualization","content":" Tips # "},{"id":93,"href":"/en/others/issues/","title":"Issues","section":"Others","content":"If you find any errors, mistakes, or problems, please contact us in the comments section of this page. If you would like to communicate with us by e-mail, please contact us at the e-mail address listed in our Privacy Policy.\n"},{"id":94,"href":"/en/others/privacy-policy/","title":"Privacy Policy","section":"Others","content":"The words \u0026ldquo;this Web site\u0026rdquo;, \u0026ldquo;this site\u0026rdquo;, etc. below refer to all of the following pages.\nVideo and text uploaded to https://www.youtube.com/@K_DM and https://www.youtube.com/@K_DL Contents in https://kdm.hatenablog.jp/ Contents in https://k-dm.work/ Contents in https://k-dm.work/en/ The content of this site is the personal opinion of the individual and is in no way related to any organization or group to which he/she belongs.\nContacts # k▲datamining■gmail▲com\n※Replace \u0026ldquo;■\u0026rdquo; with \u0026ldquo;@\u0026rdquo; and \u0026ldquo;▲\u0026rdquo; with ”.”.\nDisclaimer # While every effort is made to ensure the accuracy of the information contained in this Web site, we assume no responsibility for any actions taken by users based on the information contained in this Web site. This website shall not be liable for any loss or damage incurred by the user or any third party as a result of the user\u0026rsquo;s use of this website. The text, images, and all content of this website are protected by copyright, portrait rights, and other rights. Distribution of advertisements and use of logs # Google Adsense # This site uses Google Adsense, a third-party advertising service, which uses cookies to serve ads based on a user\u0026rsquo;s past visits to that website and other websites.\nThe purpose of this is to enable Google and its partners to display relevant advertisements to you based on your visits to that or other websites.\nAmazon Associates # This site is a participant in the Amazon Associates Program, an affiliate program designed to provide a means for sites to earn referral fees by promoting and linking to Amazon.com. Third parties may provide content and advertising, collect information directly from visitors, and set or recognize cookies on visitors\u0026rsquo; browsers.\nGoogle Analytics # This website uses Google Analytics, an access analysis tool by Google. This Google Analytics uses cookies to collect traffic data. This traffic data is collected anonymously and does not personally identify you. You can refuse the collection of this feature by disabling cookies, so please check your browser settings.\n"}]