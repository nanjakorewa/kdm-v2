<!DOCTYPE html>
<html lang="ja" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  


  arXivから論文を取得する
  
  #
  

以下の論文をarXivからダウンロードして、概要を翻訳します。
断りがない限り、コード中に出現する英文は以下の論文の「Abstract」の英文の一部です。
Vaswani, Ashish, et al. &ldquo;Attention is all you need.&rdquo; Advances in neural information processing systems 30 (2017).
import arxiv

search = arxiv.Search(id_list=[&#34;1706.03762&#34;])
paper = next(search.results())
print(f&#34;論文タイトル：{paper.title}&#34;)
論文タイトル：Attention Is All You Need

pdf_path = paper.download_pdf()
print(f&#34;pdf保存先：{pdf_path}&#34;)
pdf保存先：./1706.03762v5.Attention_Is_All_You_Need.pdf


  pdfからテキストを抽出する
  
  #
  

import fitz

abstract_text = &#34;&#34;
with fitz.open(pdf_path) as pages:
    first_page = pages[0]
    text = first_page.get_text().replace(&#34;\n&#34;, &#34;&#34;)
    print(f&#39;アブスト開始位置：{text.find(&#34;Abstract&#34;)}, イントロ開始位置：{text.find(&#34;Introduction&#34;)}&#39;)
    abstract_text = text[text.find(&#34;Abstract&#34;) &#43; 8 : text.find(&#34;Introduction&#34;) - 1]

print(f&#34;{abstract_text[:400]}...&#34;)
アブスト開始位置：394, イントロ開始位置：1528
The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperforming models also connect the encoder and decoder through an attentionmechanism. We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely. Experimen...


  deepl-pythonを使って英語を翻訳する
  
  #
  

import deepl
import os

translator = deepl.Translator(os.getenv(&#34;DEEPL_AUTH_KEY&#34;))
result = translator.translate_text(&#34;Good morning!&#34;, source_lang=&#34;EN&#34;, target_lang=&#34;JA&#34;)
print(result)
おはようございます。

result = translator.translate_text(abstract_text, source_lang=&#34;EN&#34;, target_lang=&#34;JA&#34;)
print(result)
優性配列変換モデルは、エンコーダとデコーダを含む複雑なリカレントニューラルネットワークまたは畳み込みニューラルネットワークをベースにしています。また、最も優れたモデルでは、エンコーダとデコーダをアテンションメカニズムで接続しています。本研究では、再帰や畳み込みを必要とせず、注目メカニズムのみに基づいた新しいシンプルなネットワークアーキテクチャ「Transformer」を提案する。2つの機械翻訳タスクを用いた実験では、これらのモデルが優れた品質を持ち、並列化が可能で学習時間が大幅に短縮されることが示された。WMT 2014の英独翻訳タスクにおいて、我々のモデルは28.4 BLEUを達成し、アセンブルを含む既存の最良の結果よりも2 BLEU以上向上した。WMT 2014の英仏翻訳タスクでは、8つのGPUを用いて3.5日間の学習を行った結果、41.8という最新のBLEUスコアを達成しましたが、これは文献に掲載されている最高のモデルの学習コストのごく一部です。また、大規模および限定的な学習データを用いた英語の構文解析に適用することで、Transformerが他のタスクにもよく適応することを示しました。
">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/prep/special/deepl/">
  <meta property="og:site_name" content="K_DM Book">
  <meta property="og:title" content="deeplで論文を翻訳">
  <meta property="og:description" content="arXivから論文を取得する # 以下の論文をarXivからダウンロードして、概要を翻訳します。 断りがない限り、コード中に出現する英文は以下の論文の「Abstract」の英文の一部です。
Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).
import arxiv search = arxiv.Search(id_list=[&#34;1706.03762&#34;]) paper = next(search.results()) print(f&#34;論文タイトル：{paper.title}&#34;) 論文タイトル：Attention Is All You Needpdf_path = paper.download_pdf() print(f&#34;pdf保存先：{pdf_path}&#34;) pdf保存先：./1706.03762v5.Attention_Is_All_You_Need.pdfpdfからテキストを抽出する # import fitz abstract_text = &#34;&#34; with fitz.open(pdf_path) as pages: first_page = pages[0] text = first_page.get_text().replace(&#34;\n&#34;, &#34;&#34;) print(f&#39;アブスト開始位置：{text.find(&#34;Abstract&#34;)}, イントロ開始位置：{text.find(&#34;Introduction&#34;)}&#39;) abstract_text = text[text.find(&#34;Abstract&#34;) &#43; 8 : text.find(&#34;Introduction&#34;) - 1] print(f&#34;{abstract_text[:400]}...&#34;) アブスト開始位置：394, イントロ開始位置：1528The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperforming models also connect the encoder and decoder through an attentionmechanism. We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely. Experimen...deepl-pythonを使って英語を翻訳する # import deepl import os translator = deepl.Translator(os.getenv(&#34;DEEPL_AUTH_KEY&#34;)) result = translator.translate_text(&#34;Good morning!&#34;, source_lang=&#34;EN&#34;, target_lang=&#34;JA&#34;) print(result) おはようございます。result = translator.translate_text(abstract_text, source_lang=&#34;EN&#34;, target_lang=&#34;JA&#34;) print(result) 優性配列変換モデルは、エンコーダとデコーダを含む複雑なリカレントニューラルネットワークまたは畳み込みニューラルネットワークをベースにしています。また、最も優れたモデルでは、エンコーダとデコーダをアテンションメカニズムで接続しています。本研究では、再帰や畳み込みを必要とせず、注目メカニズムのみに基づいた新しいシンプルなネットワークアーキテクチャ「Transformer」を提案する。2つの機械翻訳タスクを用いた実験では、これらのモデルが優れた品質を持ち、並列化が可能で学習時間が大幅に短縮されることが示された。WMT 2014の英独翻訳タスクにおいて、我々のモデルは28.4 BLEUを達成し、アセンブルを含む既存の最良の結果よりも2 BLEU以上向上した。WMT 2014の英仏翻訳タスクでは、8つのGPUを用いて3.5日間の学習を行った結果、41.8という最新のBLEUスコアを達成しましたが、これは文献に掲載されている最高のモデルの学習コストのごく一部です。また、大規模および限定的な学習データを用いた英語の構文解析に適用することで、Transformerが他のタスクにもよく適応することを示しました。">
  <meta property="og:locale" content="ja">
  <meta property="og:type" content="article">
    <meta property="article:section" content="prep">
<title>deeplで論文を翻訳 | K_DM Book</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/prep/special/deepl/">
<link rel="stylesheet" href="/book.min.5c80a20e1f11b0e9dc4cae70ce44083860ffa999a7bf8ab5dab38664fde8a418.css" integrity="sha256-XICiDh8RsOncTK5wzkQIOGD/qZmnv4q12rOGZP3opBg=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/ja.search.min.58d3a47dfc146864a58893dc5fc3dea0c3290cbd2695f142a0697316f5b64e98.js" integrity="sha256-WNOkffwUaGSliJPcX8PeoMMpDL0mlfFCoGlzFvW2Tpg=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr" class="book-kind-page book-type-prep">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>K_DM Book</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="検索" aria-label="検索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>



  



  
    
  
    
  
    
  



<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex">
      <a role="button" class="flex flex-auto">
        <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
        <span>日本語</span>
      </a>
    </label>

    <ul>
      
      <li>
        <a href="/en/">
          <span>English</span>
        </a>
      </li>
      
      <li>
        <a href="/es/">
          <span>Español</span>
        </a>
      </li>
      
      <li>
        <a href="/id/">
          <span>Indonesia</span>
        </a>
      </li>
      
    </ul>
  </li>
</ul>














  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/" class="">線形回帰</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/linear_regression/" class="">最小二乗法</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/ridge_and_lasso/" class="">リッジ回帰・ラッソ回帰</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/robust_regression/" class="">外れ値と頑健性</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/regressionanalysis/" class="">残差の分析</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/classification/" class="">線形分類</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/classification/logistic_regression/" class="">ロジスティック回帰</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/classification/linear_discriminant_analysis/" class="">判別分析</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/" class="">決定木</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/decision_tree_classifier/" class="">決定木(分類)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/decision_tree_regressor/" class="">決定木(回帰)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/parameter/" class="">決定木のパラメータ</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/rulefit/" class="">RuleFit</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/" class="">アンサンブル</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/randomforest/" class="">ランダムフォレスト</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/stucking/" class="">スタッキング</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/adaboost_classification/" class="">Adaboost(分類)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/adaboost_regression/" class="">Adaboost(回帰)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/gradient_boosting1/" class="">勾配ブースティング</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/gradient_boosting2/" class="">勾配ブースティングの可視化</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/clustering/" class="">クラスタリング</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/clustering/k-means1/" class="">k-means</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/clustering/k-means2/" class="">k-means&#43;&#43;</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/clustering/x-means/" class="">X-means</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/" class="">次元削減</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/pca/" class="">PCA</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/svd/" class="">SVD</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/lda/" class="">LDA</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/kernel-pca/" class="">Kernel-PCA</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/feature_selection/" class="">特徴選択</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/feature_selection/boruta/" class="">Boruta</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/timeseries/" class="">時系列</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/timeseries/prophet/" class="">Prophetを使ってみる</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/anomaly/" class="">異常検知</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/anomaly/adtk1/" class="">異常検知①</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/anomaly/adtk2/" class="">異常検知②</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>deeplで論文を翻訳</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#arxivから論文を取得する">arXivから論文を取得する</a></li>
    <li><a href="#pdfからテキストを抽出する">pdfからテキストを抽出する</a></li>
    <li><a href="#deepl-pythonを使って英語を翻訳する">deepl-pythonを使って英語を翻訳する</a></li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article">
<div class="youtube-embed">
  <iframe loading="lazy" class="youtube-video" src="https://www.youtube.com/embed/_5j8KMIb0qc" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<h2 id="arxivから論文を取得する">
  arXivから論文を取得する
  
  <a class="anchor" href="#arxiv%e3%81%8b%e3%82%89%e8%ab%96%e6%96%87%e3%82%92%e5%8f%96%e5%be%97%e3%81%99%e3%82%8b">#</a>
  
</h2>
<p>以下の論文をarXivからダウンロードして、概要を翻訳します。
断りがない限り、コード中に出現する英文は以下の論文の「Abstract」の英文の一部です。</p>
<p>Vaswani, Ashish, et al. &ldquo;<a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need.</a>&rdquo; Advances in neural information processing systems 30 (2017).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> arxiv
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>search <span style="color:#f92672">=</span> arxiv<span style="color:#f92672">.</span>Search(id_list<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;1706.03762&#34;</span>])
</span></span><span style="display:flex;"><span>paper <span style="color:#f92672">=</span> next(search<span style="color:#f92672">.</span>results())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;論文タイトル：</span><span style="color:#e6db74">{</span>paper<span style="color:#f92672">.</span>title<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>論文タイトル：Attention Is All You Need
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>pdf_path <span style="color:#f92672">=</span> paper<span style="color:#f92672">.</span>download_pdf()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;pdf保存先：</span><span style="color:#e6db74">{</span>pdf_path<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>pdf保存先：./1706.03762v5.Attention_Is_All_You_Need.pdf
</code></pre>
<h2 id="pdfからテキストを抽出する">
  pdfからテキストを抽出する
  
  <a class="anchor" href="#pdf%e3%81%8b%e3%82%89%e3%83%86%e3%82%ad%e3%82%b9%e3%83%88%e3%82%92%e6%8a%bd%e5%87%ba%e3%81%99%e3%82%8b">#</a>
  
</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> fitz
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>abstract_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> fitz<span style="color:#f92672">.</span>open(pdf_path) <span style="color:#66d9ef">as</span> pages:
</span></span><span style="display:flex;"><span>    first_page <span style="color:#f92672">=</span> pages[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> first_page<span style="color:#f92672">.</span>get_text()<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;アブスト開始位置：</span><span style="color:#e6db74">{</span>text<span style="color:#f92672">.</span>find(<span style="color:#e6db74">&#34;Abstract&#34;</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">, イントロ開始位置：</span><span style="color:#e6db74">{</span>text<span style="color:#f92672">.</span>find(<span style="color:#e6db74">&#34;Introduction&#34;</span>)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>    abstract_text <span style="color:#f92672">=</span> text[text<span style="color:#f92672">.</span>find(<span style="color:#e6db74">&#34;Abstract&#34;</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">8</span> : text<span style="color:#f92672">.</span>find(<span style="color:#e6db74">&#34;Introduction&#34;</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>abstract_text[:<span style="color:#ae81ff">400</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">...&#34;</span>)
</span></span></code></pre></div><pre><code>アブスト開始位置：394, イントロ開始位置：1528
The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperforming models also connect the encoder and decoder through an attentionmechanism. We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely. Experimen...
</code></pre>
<h2 id="deepl-pythonを使って英語を翻訳する">
  deepl-pythonを使って英語を翻訳する
  
  <a class="anchor" href="#deepl-python%e3%82%92%e4%bd%bf%e3%81%a3%e3%81%a6%e8%8b%b1%e8%aa%9e%e3%82%92%e7%bf%bb%e8%a8%b3%e3%81%99%e3%82%8b">#</a>
  
</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> deepl
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>translator <span style="color:#f92672">=</span> deepl<span style="color:#f92672">.</span>Translator(os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;DEEPL_AUTH_KEY&#34;</span>))
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> translator<span style="color:#f92672">.</span>translate_text(<span style="color:#e6db74">&#34;Good morning!&#34;</span>, source_lang<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;EN&#34;</span>, target_lang<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;JA&#34;</span>)
</span></span><span style="display:flex;"><span>print(result)
</span></span></code></pre></div><pre><code>おはようございます。
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>result <span style="color:#f92672">=</span> translator<span style="color:#f92672">.</span>translate_text(abstract_text, source_lang<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;EN&#34;</span>, target_lang<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;JA&#34;</span>)
</span></span><span style="display:flex;"><span>print(result)
</span></span></code></pre></div><pre><code>優性配列変換モデルは、エンコーダとデコーダを含む複雑なリカレントニューラルネットワークまたは畳み込みニューラルネットワークをベースにしています。また、最も優れたモデルでは、エンコーダとデコーダをアテンションメカニズムで接続しています。本研究では、再帰や畳み込みを必要とせず、注目メカニズムのみに基づいた新しいシンプルなネットワークアーキテクチャ「Transformer」を提案する。2つの機械翻訳タスクを用いた実験では、これらのモデルが優れた品質を持ち、並列化が可能で学習時間が大幅に短縮されることが示された。WMT 2014の英独翻訳タスクにおいて、我々のモデルは28.4 BLEUを達成し、アセンブルを含む既存の最良の結果よりも2 BLEU以上向上した。WMT 2014の英仏翻訳タスクでは、8つのGPUを用いて3.5日間の学習を行った結果、41.8という最新のBLEUスコアを達成しましたが、これは文献に掲載されている最高のモデルの学習コストのごく一部です。また、大規模および限定的な学習データを用いた英語の構文解析に適用することで、Transformerが他のタスクにもよく適応することを示しました。
</code></pre>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>





  
  
  




  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 
      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    

<aside class="book-toc">
  <div class="book-toc-content">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#arxivから論文を取得する">arXivから論文を取得する</a></li>
    <li><a href="#pdfからテキストを抽出する">pdfからテキストを抽出する</a></li>
    <li><a href="#deepl-pythonを使って英語を翻訳する">deepl-pythonを使って英語を翻訳する</a></li>
  </ul>
</nav>



  </div>
</aside>

 
  </main>

  
</body>
</html>
















