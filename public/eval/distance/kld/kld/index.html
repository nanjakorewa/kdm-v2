<!DOCTYPE html>
<html lang="ja" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="
  


  KLダイバージェンス
  
  #
  

Kullback-Leiblerダイバージェンスは二つの確率分布の違いを数値で表現したもの。交差エントロピーから情報エントロピーを引くことで求めることができる。
import numpy as np
import matplotlib.pyplot as plt
import japanize_matplotlib
from scipy.special import kl_div

  2つの分布
  
  #
  

plt.figure(figsize=(12, 4))
a = np.array([0.1, 0.2, 0.3, 0.2, 0.1, 0.1])
b = np.array([0.05, 0.1, 0.2, 0.3, 0.3, 0.05])

plt.bar(np.arange(a.shape[0]) - 0.1, a, width=0.5)
plt.bar(np.arange(b.shape[0]) &#43; 0.1, b, width=0.5)


  KLD
  
  #
  

def kld(dist1, dist2):
    &#34;&#34;&#34;KLダイバージェンス&#34;&#34;&#34;
    assert dist1.shape == dist2.shape, &#34;確率分布1と確率分布2は同じ長さである必要があります&#34;
    assert all(dist1) != 0 and all(dist2) != 0, &#34;確率分布に0を含んではいけません&#34;
    dist1 = dist1 / np.sum(dist1)
    dist2 = dist2 / np.sum(dist2)
    return sum([p1 * np.log2(p1 / p2) for p1, p2 in zip(dist1, dist2)])


print(f&#34;同じ分布ならKLDは0のはず→{kld(a, a)}&#34;)
print(f&#34;違う分布ならKLDは0より大きいはず→{kld(a, b)}&#34;)
print(f&#34;違う分布ならKLDは0より大きいがkld(a, b)とは違う値になるかも→{kld(b, a)}&#34;)
同じ分布ならKLDは0のはず→0.0
違う分布ならKLDは0より大きいはず→0.3
違う分布ならKLDは0より大きいがkld(a, b)とは違う値になるかも→0.3339850002884623


  Jensen–Shannon Divergenceの導入
  
  #
  

plt.hist(np.random.normal(1, 1, 1000), alpha=0.85, color=&#34;blue&#34;)
plt.hist(np.random.normal(4, 1, 1000), alpha=0.85, color=&#34;red&#34;)
plt.hist(np.random.normal(2.5, 1, 1000), alpha=0.85, color=&#34;green&#34;)
plt.show()
">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:url" content="http://localhost:1313/eval/distance/kld/kld/">
  <meta property="og:site_name" content="K_DM Book">
  <meta property="og:title" content="Kullback-Leiblerダイバージェンス">
  <meta property="og:description" content="KLダイバージェンス # Kullback-Leiblerダイバージェンスは二つの確率分布の違いを数値で表現したもの。交差エントロピーから情報エントロピーを引くことで求めることができる。
import numpy as np import matplotlib.pyplot as plt import japanize_matplotlib from scipy.special import kl_div 2つの分布 # plt.figure(figsize=(12, 4)) a = np.array([0.1, 0.2, 0.3, 0.2, 0.1, 0.1]) b = np.array([0.05, 0.1, 0.2, 0.3, 0.3, 0.05]) plt.bar(np.arange(a.shape[0]) - 0.1, a, width=0.5) plt.bar(np.arange(b.shape[0]) &#43; 0.1, b, width=0.5) KLD # def kld(dist1, dist2): &#34;&#34;&#34;KLダイバージェンス&#34;&#34;&#34; assert dist1.shape == dist2.shape, &#34;確率分布1と確率分布2は同じ長さである必要があります&#34; assert all(dist1) != 0 and all(dist2) != 0, &#34;確率分布に0を含んではいけません&#34; dist1 = dist1 / np.sum(dist1) dist2 = dist2 / np.sum(dist2) return sum([p1 * np.log2(p1 / p2) for p1, p2 in zip(dist1, dist2)]) print(f&#34;同じ分布ならKLDは0のはず→{kld(a, a)}&#34;) print(f&#34;違う分布ならKLDは0より大きいはず→{kld(a, b)}&#34;) print(f&#34;違う分布ならKLDは0より大きいがkld(a, b)とは違う値になるかも→{kld(b, a)}&#34;) 同じ分布ならKLDは0のはず→0.0違う分布ならKLDは0より大きいはず→0.3違う分布ならKLDは0より大きいがkld(a, b)とは違う値になるかも→0.3339850002884623Jensen–Shannon Divergenceの導入 # plt.hist(np.random.normal(1, 1, 1000), alpha=0.85, color=&#34;blue&#34;) plt.hist(np.random.normal(4, 1, 1000), alpha=0.85, color=&#34;red&#34;) plt.hist(np.random.normal(2.5, 1, 1000), alpha=0.85, color=&#34;green&#34;) plt.show()">
  <meta property="og:locale" content="ja">
  <meta property="og:type" content="article">
    <meta property="article:section" content="eval">
<title>Kullback-Leiblerダイバージェンス | K_DM Book</title>
<link rel="icon" href="/favicon.png" >
<link rel="manifest" href="/manifest.json">
<link rel="canonical" href="http://localhost:1313/eval/distance/kld/kld/">
  <link rel="alternate" hreflang="id" href="http://localhost:1313/id/eval/distance/kld/kld/" title="Kullback-Leibler Divergence">
<link rel="stylesheet" href="/book.min.5c80a20e1f11b0e9dc4cae70ce44083860ffa999a7bf8ab5dab38664fde8a418.css" integrity="sha256-XICiDh8RsOncTK5wzkQIOGD/qZmnv4q12rOGZP3opBg=" crossorigin="anonymous">
  <script defer src="/fuse.min.js"></script>
  <script defer src="/ja.search.min.58d3a47dfc146864a58893dc5fc3dea0c3290cbd2695f142a0697316f5b64e98.js" integrity="sha256-WNOkffwUaGSliJPcX8PeoMMpDL0mlfFCoGlzFvW2Tpg=" crossorigin="anonymous"></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr" class="book-kind-page book-type-eval">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    
<aside class="book-menu">
  <div class="book-menu-content">
    
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>K_DM Book</span>
  </a>
</h2>


<div class="book-search hidden">
  <input type="text" id="book-search-input" placeholder="検索" aria-label="検索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>
<script>document.querySelector(".book-search").classList.remove("hidden")</script>



  



  
    
  
    
  
    
  


  


<ul class="book-languages">
  <li>
    <input type="checkbox" id="languages" class="toggle" />
    <label for="languages" class="flex">
      <a role="button" class="flex flex-auto">
        <img src="/svg/translate.svg" class="book-icon" alt="Languages" />
        <span>日本語</span>
      </a>
    </label>

    <ul>
      
      <li>
        <a href="/en/">
          <span>English</span>
        </a>
      </li>
      
      <li>
        <a href="/es/">
          <span>Español</span>
        </a>
      </li>
      
      <li>
        <a href="/id/eval/distance/kld/kld/">
          <span>Indonesia</span>
        </a>
      </li>
      
    </ul>
  </li>
</ul>














  
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/" class="">線形回帰</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/linear_regression/" class="">最小二乗法</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/ridge_and_lasso/" class="">リッジ回帰・ラッソ回帰</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/robust_regression/" class="">外れ値と頑健性</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/regression/regressionanalysis/" class="">残差の分析</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/classification/" class="">線形分類</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/classification/logistic_regression/" class="">ロジスティック回帰</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/classification/linear_discriminant_analysis/" class="">判別分析</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/" class="">決定木</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/decision_tree_classifier/" class="">決定木(分類)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/decision_tree_regressor/" class="">決定木(回帰)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/parameter/" class="">決定木のパラメータ</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/tree/rulefit/" class="">RuleFit</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/" class="">アンサンブル</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/randomforest/" class="">ランダムフォレスト</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/stucking/" class="">スタッキング</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/adaboost_classification/" class="">Adaboost(分類)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/adaboost_regression/" class="">Adaboost(回帰)</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/gradient_boosting1/" class="">勾配ブースティング</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/ensemble/gradient_boosting2/" class="">勾配ブースティングの可視化</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/clustering/" class="">クラスタリング</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/clustering/k-means1/" class="">k-means</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/clustering/k-means2/" class="">k-means&#43;&#43;</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/clustering/x-means/" class="">X-means</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/" class="">次元削減</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/pca/" class="">PCA</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/svd/" class="">SVD</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/lda/" class="">LDA</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/dimensionality_reduction/kernel-pca/" class="">Kernel-PCA</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/feature_selection/" class="">特徴選択</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/feature_selection/boruta/" class="">Boruta</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/timeseries/" class="">時系列</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/timeseries/prophet/" class="">Prophetを使ってみる</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/anomaly/" class="">異常検知</a>
  

          
  <ul>
    
      
        <li>
          
  
  

  
    <a href="/basic/anomaly/adtk1/" class="">異常検知①</a>
  

        </li>
      
    
      
        <li>
          
  
  

  
    <a href="/basic/anomaly/adtk2/" class="">異常検知②</a>
  

        </li>
      
    
  </ul>

        </li>
      
    
  </ul>














</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>



  </div>
</aside>
 

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <h3>Kullback-Leiblerダイバージェンス</h3>

  <label for="toc-control">
    
    <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#klダイバージェンス">KLダイバージェンス</a>
      <ul>
        <li><a href="#2つの分布">2つの分布</a></li>
        <li><a href="#kld">KLD</a></li>
        <li><a href="#jensenshannon-divergenceの導入">Jensen–Shannon Divergenceの導入</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
  <article class="markdown book-article">
<div class="youtube-embed">
  <iframe loading="lazy" class="youtube-video" src="https://www.youtube.com/embed/mXhriCYo7dM" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
<h2 id="klダイバージェンス">
  KLダイバージェンス
  
  <a class="anchor" href="#kl%e3%83%80%e3%82%a4%e3%83%90%e3%83%bc%e3%82%b8%e3%82%a7%e3%83%b3%e3%82%b9">#</a>
  
</h2>
<p>Kullback-Leiblerダイバージェンスは二つの確率分布の違いを数値で表現したもの。交差エントロピーから情報エントロピーを引くことで求めることができる。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> japanize_matplotlib
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.special <span style="color:#f92672">import</span> kl_div
</span></span></code></pre></div><h3 id="2つの分布">
  2つの分布
  
  <a class="anchor" href="#2%e3%81%a4%e3%81%ae%e5%88%86%e5%b8%83">#</a>
  
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.1</span>])
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.05</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(np<span style="color:#f92672">.</span>arange(a<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.1</span>, a, width<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(np<span style="color:#f92672">.</span>arange(b<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.1</span>, b, width<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span></code></pre></div><p><img src="/images/eval/distance/kld_files/kld_3_1.png" alt="png" /></p>
<h3 id="kld">
  KLD
  
  <a class="anchor" href="#kld">#</a>
  
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kld</span>(dist1, dist2):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;KLダイバージェンス&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> dist1<span style="color:#f92672">.</span>shape <span style="color:#f92672">==</span> dist2<span style="color:#f92672">.</span>shape, <span style="color:#e6db74">&#34;確率分布1と確率分布2は同じ長さである必要があります&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> all(dist1) <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">and</span> all(dist2) <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;確率分布に0を含んではいけません&#34;</span>
</span></span><span style="display:flex;"><span>    dist1 <span style="color:#f92672">=</span> dist1 <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sum(dist1)
</span></span><span style="display:flex;"><span>    dist2 <span style="color:#f92672">=</span> dist2 <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sum(dist2)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> sum([p1 <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log2(p1 <span style="color:#f92672">/</span> p2) <span style="color:#66d9ef">for</span> p1, p2 <span style="color:#f92672">in</span> zip(dist1, dist2)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;同じ分布ならKLDは0のはず→</span><span style="color:#e6db74">{</span>kld(a, a)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;違う分布ならKLDは0より大きいはず→</span><span style="color:#e6db74">{</span>kld(a, b)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;違う分布ならKLDは0より大きいがkld(a, b)とは違う値になるかも→</span><span style="color:#e6db74">{</span>kld(b, a)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre><code>同じ分布ならKLDは0のはず→0.0
違う分布ならKLDは0より大きいはず→0.3
違う分布ならKLDは0より大きいがkld(a, b)とは違う値になるかも→0.3339850002884623
</code></pre>
<h3 id="jensenshannon-divergenceの導入">
  Jensen–Shannon Divergenceの導入
  
  <a class="anchor" href="#jensenshannon-divergence%e3%81%ae%e5%b0%8e%e5%85%a5">#</a>
  
</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1000</span>), alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.85</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;blue&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1000</span>), alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.85</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;red&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">2.5</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1000</span>), alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.85</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;green&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="/images/eval/distance/kld_files/kld_7_0.png" alt="png" /></p>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>





  
  
  




  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 
      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    

<aside class="book-toc">
  <div class="book-toc-content">
    
  
<nav id="TableOfContents">
  <ul>
    <li><a href="#klダイバージェンス">KLダイバージェンス</a>
      <ul>
        <li><a href="#2つの分布">2つの分布</a></li>
        <li><a href="#kld">KLD</a></li>
        <li><a href="#jensenshannon-divergenceの導入">Jensen–Shannon Divergenceの導入</a></li>
      </ul>
    </li>
  </ul>
</nav>



  </div>
</aside>

 
  </main>

  
</body>
</html>
















