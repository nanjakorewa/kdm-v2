---
title: "AIC と BIC"
pre: "4.1.4 "
weight: 4
title_suffix: "情報量基準でモデルを比較する"
---

{{< lead >}}
AIC（Akaike Information Criterion）と BIC（Bayesian Information Criterion）は、尤度にモデルの複雑さへのペナルティを加えた情報量基準です。フィッティングが良いだけでなく、過学習を避けたモデル選択に役立ちます。
{{< /lead >}}

---

## 1. 定義

対数尤度を \\(\ell\\)、パラメータ数を \\(k\\)、サンプル数を \\(n\\) とすると、

$$
\mathrm{AIC} = -2\ell + 2k, \qquad
\mathrm{BIC} = -2\ell + k \log n
$$

- **AIC**：汎化誤差に近い量を最小化する指標。パラメータ数へのペナルティが一定。
- **BIC**：`log n` が掛かるため、サンプル数が増えるほど複雑さへの罰則が強くなり、よりシンプルなモデルを好む傾向。

値が小さいモデルほど望ましいとされます。

---

## 2. Python で計算

scikit-learn には AIC/BIC を直接計算する仕組みが少ないため、statsmodels などを利用します。

```python
import statsmodels.api as sm
from sklearn.datasets import load_boston

X, y = load_boston(return_X_y=True)
X = sm.add_constant(X)  # 切片項

model = sm.OLS(y, X).fit()
print("AIC:", model.aic)
print("BIC:", model.bic)
```

`model.aic` と `model.bic` が自動計算されます。一般化線形モデル（GLM）でも同様に取得可能です。

---

## 3. 直感と使い分け

- **AIC**：予測性能を重視。汎化誤差を近似する性質があり、サンプル数が多い場合は複雑なモデルを許容しやすい。
- **BIC**：真のモデルがパラメータ集合の中に存在するというベイズ的仮定の下で導出。パラメータ節約を重視し、サンプル数が多いほどシンプルなモデルが選ばれやすい。
- **比較は必ず同じデータで**：異なるデータセットや尤度の定義が違うモデル間で比較することはできません。

---

## 4. 実務での活用

- **特徴量選択**：候補モデルの AIC/BIC を比較し、無駄な特徴量を省く。
- **時系列モデル**：ARIMA/SARIMAX などで (p,d,q) を選ぶ際に AIC/BIC を最小化する組み合わせを探索するのが一般的。
- **一般化線形モデル**：リンク関数や分布族が異なるモデルを比較し、最も適合しつつシンプルなモデルを選ぶ。
- **報告書での根拠**：R² や RMSE だけでなく AIC/BIC も提示すると、モデルの複雑さを考慮した比較であると説明しやすい。

---

## 5. 注意点

- **尤度の仮定**：モデルが仮定する確率分布が大きく外れていると、AIC/BIC は適切に機能しない。
- **巨大データでの差**：サンプル数が非常に多い場合、BIC が複雑さを過度に抑えすぎることがある。目的に応じて AIC/BIC のどちらを採用するかを判断する。
- **比較対象の条件**：定数項や誤差分布が異なるモデル同士を直接比較するのは避ける。

---

## まとめ

- AIC と BIC は尤度にペナルティを加えてモデルの良さと複雑さをバランスさせる指標。
- AIC は予測重視、BIC はモデルの簡潔さ重視と覚えておくと使い分けしやすい。
- 同じデータ・同じ尤度の枠組みで比較し、RMSE や Adjusted R² など他指標と併用するとより納得感のあるモデル選択ができる。

---
