---
title: "Jensen-Shannon ダイバージェンス"
pre: "4.4.2 "
weight: 5
title_suffix: "KL を対称化した分布間距離"
---

{{< lead >}}
Jensen-Shannon ダイバージェンス（JSD）は、2 つの確率分布の類似度を測る指標で、KL ダイバージェンスを対称化し有限値にしたものです。生成モデルの評価や言語モデルの比較で広く使われます。
{{< /lead >}}

---

## 1. 定義

確率分布 \\(P\\) と \\(Q\\)、その平均 \\(M = \frac{1}{2}(P + Q)\\) を用いると、

$$
\mathrm{JSD}(P \parallel Q) = \frac{1}{2} \mathrm{KL}(P \parallel M) + \frac{1}{2} \mathrm{KL}(Q \parallel M)
$$

- 常に 0 以上 1 以下（底 2 の対数を使う場合）。
- KL ダイバージェンスと異なり、\\(P\\) と \\(Q\\) を入れ替えても同じ値。
- 分布が完全に一致すると 0、完全に異なると 1 に近づきます。

---

## 2. Python で計算

```python
import numpy as np
from scipy.spatial.distance import jensenshannon

p = np.array([0.4, 0.4, 0.2])
q = np.array([0.1, 0.3, 0.6])

js = jensenshannon(p, q, base=2)
print(f"Jensen-Shannon divergence = {js:.4f}")
```

`scipy.spatial.distance.jensenshannon` は平方根を取った値（Jensen-Shannon 距離）を返します。距離を二乗すればダイバージェンスそのものになります。

---

## 3. 直感と特徴

- **対称性**：`JSD(P‖Q) = JSD(Q‖P)` のため、どちらの分布を基準にしても同じ。
- **有限値**：KL はサポートが一致しないと無限大になるが、JSD は補間した平均分布と比較するため有限。
- **メトリック性**：平方根を取った Jensen-Shannon 距離は三角不等式を満たし、クラスタリングなど距離ベース手法で扱いやすい。

---

## 4. 実務での活用

- **生成モデルの評価**：生成した分布と実データ分布の差を測る際に堅牢。GAN の学習進度の指標としても用いられる。
- **言語モデル・トピックモデル**：単語分布や Topic 分布の類似度を比較。
- **異常検知**：時系列で得られる分布を比較し、異常期間の分布シフトを検出。
- **ハイパーパラメータ探索**：複数の候補モデルの出力分布を比較して、最も実データに近いモデルを選ぶ。

---

## 5. 注意点

- **離散化が必要**：連続分布の場合は適切なビン分割やカーネル推定を行ってから計算する。
- **ゼロ確率の扱い**：`jensenshannon` は内部でクリップを行うが、ゼロの多い分布ではスムージングを事前に施すと安定。
- **解釈**：値が 0.1 → 0.05 に減った場合、距離は半分だが必ずしもモデル精度が倍になるわけではない。相対比較に向いた指標と割り切る。

---

## まとめ

- Jensen-Shannon ダイバージェンスは KL を対称化し有限値にした分布間距離で、生成モデルや言語モデルの比較で有用。
- SciPy の `jensenshannon` を使えば簡単に距離を計算でき、平方根を二乗することでダイバージェンスに戻せる。
- 前処理やゼロ確率の扱いに気を配りつつ、KL や JS を組み合わせて分布シフトを評価しよう。

---
