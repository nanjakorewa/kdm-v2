---
title: "F1・Fβスコア"
pre: "4.3.8 "
weight: 8
title_suffix: "適合率と再現率の調和平均"
---

{{< lead >}}
F1 スコアは適合率（Precision）と再現率（Recall）の調和平均で、両者のバランスを一つの数値で捉えられます。Fβ スコアでは β を変えることで、再現率をどれだけ重視するかを調整できます。
{{< /lead >}}

---

## 1. 定義

適合率を \\(P\\)、再現率を \\(R\\) とすると、

$$
F_1 = 2 \cdot \frac{P \cdot R}{P + R}
$$

より一般的な Fβ スコアは次の式です。

$$
F_\beta = (1 + \beta^2) \cdot \frac{P \cdot R}{\beta^2 P + R}
$$

- β > 1：再現率を重視（偽陰性を嫌う）
- β < 1：適合率を重視（偽陽性を嫌う）

---

## 2. Python で計算

```python
from sklearn.metrics import f1_score, fbeta_score, classification_report

print("F1:", f1_score(y_test, y_pred))
print("F0.5:", fbeta_score(y_test, y_pred, beta=0.5))
print("F2:", fbeta_score(y_test, y_pred, beta=2.0))

print(classification_report(y_test, y_pred, digits=3))
```

`classification_report` ではクラスごとの F1 が表示されます。マルチクラスでは `average` 引数で `micro`、`macro`、`weighted` などを選択でき、全体の F1 を計算できます。

---

## 3. F1 と Fβ の使い分け

- **F1 スコア**：適合率と再現率を同等に評価したいとき。一般的なデフォルト。
- **F0.5 スコア**：偽陽性コストが高い場合（例：業務フローで手作業チェックが高コスト）。
- **F2 スコア**：偽陰性を避けたい場合（例：医療スクリーニングや不正検知）。

閾値を調整しながら目的の Fβ を最大化すると、業務要件に合った判定ラインを引けます。

---

## 4. マクロ平均とマイクロ平均

- **マクロ F1（macro）**：クラスごとの F1 を単純平均。少数派クラスを重視したいとき。
- **マイクロ F1（micro）**：全サンプルの TP/FP/FN を合算して計算。大規模データで安定。
- **加重平均（weighted）**：クラスごとの F1 をサンプル数で重み付け。クラス比を反映した全体スコア。

多ラベル問題では `average="samples"` を使うとサンプル単位で平均化できます。

---

## 5. 実務でのポイント

- Accuracy と併せて F1 を提示すると、クラス不均衡時でも実態を説明しやすい。
- モデル比較では ROC-AUC とF1 を見比べ、判別能力と実際のしきい値性能をバランス良く評価する。
- しきい値探索では `Precision-Recall` 曲線と並べて F1 の変化を可視化すると意思決定者に伝わりやすい。

---

## まとめ

- F1 スコアは適合率と再現率の調和平均で、両者のバランスを一つの数値にまとめる。
- Fβ スコアで β を調整すると、偽陽性・偽陰性のコストに応じた評価が可能。
- `average` の指定やしきい値調整を活用し、業務要件を満たす最適なモデルを選択しよう。

---
