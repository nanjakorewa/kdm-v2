---
title: "コーエンのカッパ係数（Cohen's Kappa）"
pre: "4.3.11 "
weight: 11
title_suffix: "偶然一致を差し引いた評価指標"
---

{{< lead >}}
コーエンのカッパ係数（Cohen's Kappa）は、偶然一致で説明できる部分を差し引いて分類性能を評価する指標です。アノテータ間一致度の評価だけでなく、クラス比が偏った分類モデルの品質検証にも利用できます。
{{< /lead >}}

---

## 1. 定義

カッパ係数は実際の一致率を \\(p_o\\)、ランダムに一致する確率を \\(p_e\\) とすると、

$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$

- 1 に近いほど完全一致。
- 0 は偶然一致と同程度。
- 負の値は偶然一致より悪いことを示す。

---

## 2. Python で計算

```python
from sklearn.metrics import cohen_kappa_score, confusion_matrix

print("Cohen's Kappa:", cohen_kappa_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

多クラス分類にも適用でき、`weights="quadratic"` を指定すると順序クラス向けの重み付きカッパ係数を計算できます。

---

## 3. 解釈の目安

Landis & Koch (1977) の基準では以下のように解釈されます。

| κ | 解釈 |
| --- | --- |
| < 0 | 非常に低い一致 |
| 0.0–0.2 | わずかな一致 |
| 0.2–0.4 | ある程度の一致 |
| 0.4–0.6 | 中程度の一致 |
| 0.6–0.8 | かなり良い一致 |
| 0.8–1.0 | ほぼ完全な一致 |

ただし領域によって解釈は異なるため、業務に応じて基準を明確にしておきましょう。

---

## 4. モデル評価での利点

- **クラス不均衡に強い**：偶然一致を除外するため、多数派を予測するだけのモデルでは κ が低くなる。
- **アノテーション品質チェック**：モデル vs. 人間のラベル、またはアノテータ同士の一致度を測れる。
- **重み付きカッパ**：順序クラス（例：5段階評価）で、どれだけ離れたラベルを誤ったかに応じて重みづけが可能。

---

## 5. 実務のヒント

- Accuracy が高く κ が低い場合、偶然一致に頼っている可能性がある。混同行列を確認して改善策を検討する。
- 監査/査定の業務では、規制当局から κ による報告を求められることがあるため、計算方法を把握しておくと安心。
- 学習データのアノテーション品質のチェックに使い、疑わしいラベルやアノテータを発見する。

---

## まとめ

- コーエンのカッパ係数は偶然一致を差し引いた一致度指標で、クラス不均衡でも実力を測りやすい。
- `cohen_kappa_score` で気軽に計算でき、重み付き設定で順序データにも対応できる。
- Accuracy や F1 と併用し、モデルやアノテーションの品質を多角的に評価しよう。

---
