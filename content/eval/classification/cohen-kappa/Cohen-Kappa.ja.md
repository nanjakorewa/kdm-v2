---
title: "コーエンのカッパ係数（Cohen's Kappa）"
linkTitle: "Cohen's Kappa"
seo_title: "コーエンのカッパ係数｜偶然一致を差し引いた評価指標"
title_suffix: "偶然一致を差し引いた評価指標"
pre: "4.3.11 "
weight: 11
---

{{< lead >}}
コーエンのカッパ係数は、偶然に一致した分を差し引いて分類性能やアノテーションの一致度を測る指標です。クラス比が大きく偏ったデータでも実力を見抜けるので、二者間のラベリング品質評価に広く使われます。
{{< /lead >}}

---

## 1. 定義
実測の一致率を \\(p_o\\)、偶然の一致率を \\(p_e\\) とすると、カッパ係数は次で与えられます。

$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$

- \\(\kappa = 1\\)：完全一致  
- \\(\kappa = 0\\)：偶然一致と同程度  
- \\(\kappa < 0\\)：偶然一致より悪い

---

## 2. Python 3.13 での計算
```bash
python --version  # 例: Python 3.13.0
pip install scikit-learn
```

```python
from sklearn.metrics import cohen_kappa_score, confusion_matrix

print("Cohen's Kappa:", cohen_kappa_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

多クラス分類にも対応しており、`weights="quadratic"` を指定すると順序クラス向けの重み付きカッパ係数を計算できます。

---

## 3. 解釈の目安
Landis & Koch (1977) では以下のように解釈されています。領域によって許容値は異なるため、業務での基準を明文化しておきましょう。

| κ       | 解釈             |
| ------- | ---------------- |
| < 0     | ほぼ一致なし     |
| 0.0–0.2 | わずかな一致     |
| 0.2–0.4 | ある程度の一致   |
| 0.4–0.6 | 中程度の一致     |
| 0.6–0.8 | 十分良い一致     |
| 0.8–1.0 | ほぼ完全な一致   |

---

## 4. モデル評価での利点
- **偶然一致に強い**：多数派クラスばかり予測するモデルでは κ が低くなり、Accuracy の過大評価を防げる。
- **アノテーション品質チェック**：モデル vs. 人手ラベル、またはアノテータ同士の一致度を客観的に比較できる。
- **重み付きカッパ**：5 段階評価など順序がある場合、外れ方に応じた重み付けで厳密に評価できる。

---

## 5. 実務でのヒント
- Accuracy が高く κ が低い場合は、偶然一致に頼っている可能性が高い。混同行列を併せて確認し改善策を検討する。
- 監査や査定業務では、規制当局から κ による報告を求められることもあるため、計算手順をドキュメント化しておく。
- 学習データのラベル監査に κ を用いると、一貫性の低いアノテータやラベルセットを早期に特定できる。

---

## まとめ
- コーエンのカッパ係数は偶然一致を差し引いて一致度を測るため、クラス不均衡でもモデルの実力を判断しやすい。
- scikit-learn の `cohen_kappa_score` で簡単に算出でき、重み付き設定で順序データにも対応できる。
- Accuracy や F1 などと併用し、モデル性能とアノテーション品質を多角的に評価しよう。
