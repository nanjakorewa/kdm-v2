---
title: 線形分類
weight: 3
chapter: true
not_use_colab: true
not_use_twitter: true
pre: "<b>2.2 </b>"
---

{{% summary %}}
- 線形分類は入力と出力の関係を超平面で区切るシンプルな枠組みで、解釈性と計算効率に優れる。
- パーセプトロン、ロジスティック回帰、SVM など原理が異なる複数の手法を比較しながら理解できる。
- 正則化・カーネル・距離学習などの応用に繋がる基礎理論を身につけられる。
{{% /summary %}}

# 線形分類

## 直感
線形分類器は特徴量空間に「超平面」を置き、その両側でクラスを識別します。最も基本的なパーセプトロンから、確率的なロジスティック回帰、マージン最大化を行う SVM まで、直線をどう引くかという観点が異なるだけで、多くの共通点があります。

## 具体的な数式
一般的な線形分類器は \\(\hat{y} = \operatorname{sign}(\mathbf{w}^\top \mathbf{x} + b)\\) の形式を持ちます。学習手法によって \\(\mathbf{w}\\) の決め方が変わり、確率を扱う場合はシグモイドやソフトマックスに通して確率値を得ます。カーネル法を使うと、内積をカーネルに置き換えたまま同じアイデアを非線形空間に拡張できます。

## Pythonを用いた実験や説明
本章では以下のページで、Python と scikit-learn を用いた実験例を紹介します。

- パーセプトロン：最も単純な線形分類器の更新則と決定境界
- ロジスティック回帰：確率的な解釈を持つ二値分類
- ソフトマックス回帰：多クラス分類への拡張
- 線形判別分析（LDA）：クラス間距離を最大化する次元削減
- サポートベクターマシン（SVM）：マージン最大化とカーネルトリック
- ナイーブベイズ：条件付き独立仮定による高速分類
- k 近傍法：距離に基づく怠惰学習

コードはそのまま実行できるので、手元で動かしながら決定境界や確率の挙動を確認してみてください。

## 参考文献
{{% references %}}
<li>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <i>The Elements of Statistical Learning</i>. Springer.</li>
<li>Bishop, C. M. (2006). <i>Pattern Recognition and Machine Learning</i>. Springer.</li>
{{% /references %}}
