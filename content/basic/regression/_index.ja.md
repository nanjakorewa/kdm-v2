---
title: 線形回帰
weight: 1
chapter: true
not_use_colab: true
not_use_twitter: true
pre: "<b>2.1 </b>"
---

# 線形回帰

<div class="pagetop-box">
  <p><b>線形回帰</b>は、入力（説明変数）と出力（目的変数）の関係を一次式で近似し、未知の値を予測したり変数間の関係を解釈するための最も基本的な機械学習モデルです。</p>
</div>

---

## なぜ線形回帰を学ぶのか？

- 数式が明快で、モデルの挙動を追跡しやすい  \n- 計算コストが低く、データサイエンスの最初の一歩に適している  \n- 係数を通じて「各特徴量がどれだけ効いているか」を説明できる  \n- リッジ/ラッソ、一般化線形モデル、ディープラーニングなど多くの発展的手法の基礎になる

---

## 線形回帰でできること

- **予測**: 売上や需要などの将来値を推定する  \n- **関係性の可視化**: 「X が 1 増えると Y がどれだけ変化するか」を係数で説明  \n- **特徴量の重要度評価**: 係数の大きさや符号からビジネス上の示唆を得る

---

## 学ぶ順番の流れ

本章の日本語コンテンツでは、次のようにステップを広げながら線形回帰を深掘りします。

1. **最小二乗法（Linear Regression）**  \n   単回帰・重回帰の基本と解析解、`scikit-learn` による実装。
2. **正則化付き回帰（Ridge & Lasso）**  \n   過学習を抑えるための \(L_2/L_1\) ペナルティと係数の違いを解説。
3. **ロバスト回帰（Huber など）**  \n   外れ値を含む現実データに対して安定な推定を行う方法。
4. **多項式回帰**  \n   特徴量を多項式に拡張して、直線では捉えられない曲線パターンを学習。
5. **Elastic Net 回帰**  \n   L1/L2 をブレンドして相関の強い特徴量をバランス良く扱う。
6. **ベイズ線形回帰**  \n   予測の平均だけでなく不確実性まで推定し、意思決定に活かす。
7. **分位点回帰（Quantile Regression）**  \n   条件付き中央値や上位分位を直接モデリングし、予測区間を推定。
8. **主成分回帰（PCR）**  \n   PCA で次元圧縮した後に回帰し、多重共線性を緩和。
9. **PLS 回帰**  \n   説明変数と目的変数の共分散を最大化する潜在因子で回帰。

必要に応じて、上記の順番で読み進めたり、興味のあるアルゴリズムから直接参照してください。

---

## まとめ

- 線形回帰はシンプルながら応用範囲が広く、ビジネス/研究の両面で活躍するスタンダードモデル。  \n- 正則化・特徴量拡張・ベイズ推論を組み合わせることで、実務のさまざまな要件に対応できる。  \n- 各ページの Python コードを実際に動かし、自分のデータに当てはめることで理解が定着します。

---
