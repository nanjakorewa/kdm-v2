---
title: 線形回帰
weight: 1
chapter: true
not_use_colab: true
not_use_twitter: true
pre: "<b>2.1 </b>"
---

# 線形回帰

<div class="pagetop-box">
  <p><b>線形回帰</b>は、入力（説明変数）と出力（目的変数）の関係を一次式で近似し、未知の値を予測したり変数間の関係を解釈するための最も基本的な機械学習モデルです。</p>
</div>

---

## なぜ線形回帰を学ぶのか？

- 数式が明快で、モデルの挙動を追跡しやすい  
- 計算コストが低く、データサイエンスの最初の一歩に適している  
- 係数を通じて「各特徴量がどれだけ効いているか」を説明できる  
- リッジ/ラッソ、一般化線形モデル、ディープラーニングなど多くの発展的手法の基礎になる

---

## 線形回帰でできること

- **予測**: 売上や需要などの将来値を推定する  
- **関係性の解釈**: 「入力が 1 増えると出力がどれだけ変化するか」を係数で説明  
- **特徴量の重要度評価**: 係数の大きさや符号からビジネス上の示唆を得る

---

## 学ぶ順番の流れ

本章の日本語コンテンツでは、次のステップで線形回帰を深掘りします。

1. **最小二乗法（Linear Regression）**  
   単回帰・重回帰の基本と解析解、`scikit-learn` による実装。
2. **正則化付き回帰（Ridge & Lasso）**  
   過学習を抑える \(L_2/L_1\) ペナルティと係数の違いを解説。
3. **ロバスト回帰（Huber など）**  
   外れ値を含む実データでも安定に学習する方法。
4. **多項式回帰**  
   特徴量を多項式に拡張して曲線的パターンを学習。
5. **Elastic Net 回帰**  
   L1/L2 をブレンドして相関の強い特徴量をバランス良く扱う。
6. **ベイズ線形回帰**  
   予測の平均だけでなく不確実性まで推論。
7. **分位点回帰（Quantile Regression）**  
   条件付き中央値や上位分位を直接モデリングし、予測区間を推定。
8. **主成分回帰（PCR）**  
   PCA で次元圧縮してから回帰し、多重共線性を緩和。
9. **PLS 回帰**  
   説明変数と目的変数の共分散を最大化する潜在因子で回帰。
10. **加重最小二乗法（WLS）**  
    サンプルごとの信頼度や分散の違いを重みで補正。
11. **Orthogonal Matching Pursuit（OMP）**  
    貪欲に特徴量を選んで疎な係数ベクトルを得る。

必要に応じて順番に沿って学ぶもよし、興味のあるアルゴリズムから直接読んでも構いません。

---

## まとめ

- 線形回帰はシンプルながら応用範囲が広く、ビジネスと研究の両面で活躍するスタンダードモデル  
- 正則化・特徴量拡張・ベイズ推論・重み付けなどを組み合わせることで、実務の要求に合わせた柔軟なモデリングが可能  
- 各ページの Python コードを実際に動かし、自分のデータに適用することで理解が定着します

---
