---
title: "サポートベクター回帰（SVR）"
pre: "2.1.9 "
weight: 9
title_suffix: "ε不感帯でロバストに予測する"
---

{{< lead >}}
サポートベクター回帰（Support Vector Regression, SVR）は、サポートベクターマシンを回帰問題に拡張した手法です。ε不感帯とカーネルを組み合わせ、外れ値に強く柔軟な非線形関係を学習できます。
{{< /lead >}}

---

## 1. 基本アイデア

- **ε 不感帯**：誤差が ε 以内であれば損失 0 とみなす。外れ値の影響を局所的に抑制。
- **マージン最大化**：サポートベクター（境界上の点）のみがモデルに影響する。
- **カーネル**：リッジ回帰では難しい非線形を、特徴空間に写像して表現できる。

目的関数は次のように書けます。

$$
\min_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{n} (\xi_i + \xi_i^*)
$$

制約条件は \\(|y_i - (\mathbf{w}^\top \phi(x_i) + b)| \le \epsilon + \xi_i\\) などで表されます。

---

## 2. Python 実装

```python
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

svr = make_pipeline(
    StandardScaler(),
    SVR(kernel="rbf", C=10.0, epsilon=0.1, gamma="scale"),
)

svr.fit(X_train, y_train)
pred = svr.predict(X_test)
```

特徴量のスケーリングが重要なため、`StandardScaler` などとパイプライン化すると便利です。

---

## 3. 主要ハイパーパラメータ

| パラメータ | 役割 | 調整の直感 |
| --- | --- | --- |
| `kernel` | カーネル種別（`linear`, `rbf`, `poly` 等） | `rbf` が汎用。線形関係なら `linear` |
| `C` | 誤差に対するペナルティ | 大きくすると過学習しやすいが柔軟。小さいと滑らか |
| `epsilon` | ε 不感帯の幅 | 大きいと誤差を許容し、サポートベクターが減る |
| `gamma` | RBF カーネルの帯域 | 大きいと局所的（過学習）、小さいと滑らか |

グリッドサーチやランダムサーチで `C`、`epsilon`、`gamma` を同時に最適化します。

---

## 4. 実務での活用

- **需要予測**：外れ値が混じるデータでも ε 不感帯で安定した予測。
- **金融時系列**：ノイズが多い価格データの短期予測に適用。
- **センサー値補完**：非線形な関係を捉えつつ、極端値の影響を抑える。

---

## 5. 注意点

- 大規模データでは学習コストとメモリがネックになる。サンプル数が多い場合は線形 SVR やランダム特徴写像を検討。
- パラメータのスケーリングに敏感なため、標準化や PCA と組み合わせる。
- ε と C の組み合わせによってサポートベクター数が大きく変わるので、モデルの軽量性も考慮してチューニングする。

---

## まとめ

- SVR は ε 不感帯とカーネルにより、外れ値に強い非線形回帰が可能。
- `SVR` クラスをパイプラインで使うとスケーリングも含めて効率的に学習できる。
- `C`, `epsilon`, `gamma` をバランスよく調整し、他の回帰モデルと比較しながら最適な設定を見つけよう。

---
