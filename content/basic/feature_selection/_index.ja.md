---
title: "特徴選択 | 機械学習の基礎解説"
linkTitle: "特徴選択"
seo_title: "特徴選択 | 機械学習の基礎解説"
weight: 7
chapter: true
not_use_colab: true
not_use_twitter: true
pre: "<b>2.7 </b>"
---

### Chapter 7

# 特徴選択

<div class="pagetop-box">
  <p><b>特徴選択（Feature Selection）</b>とは、多数の特徴量の中から「本当に有用なものだけ」を残す工程です。  
  機械学習モデルの精度向上・計算効率化・解釈性の改善のために欠かせないステップです。</p>
</div>

---

## なぜ特徴選択を学ぶのか？

- **過学習の防止**  
  不要な特徴を含むと、モデルがノイズに適合してしまう。  

- **計算効率の向上**  
  特徴が減れば学習と推論が速くなる。  

- **解釈性の改善**  
  モデルが「何を根拠に予測しているか」が分かりやすくなる。  

---

## 特徴選択の主なアプローチ

### 1. フィルタ法（Filter methods）
統計的な基準で特徴を選ぶ。モデルを使わないため軽量。  
例：相関係数、χ²検定、相互情報量

### 2. ラッパー法（Wrapper methods）
モデルを実際に学習させ、精度に基づいて特徴を選ぶ。  
例：逐次前進選択（SFS）、逐次後退選択（SBS）

{{% notice warning %}}
SFSやSBSは「何度もモデルを学習 → 精度を見て人が選ぶ」流れを含むため、  
**人のバイアスや過学習が生じやすい**点に注意が必要です。
{{% /notice %}}

### 3. 組み込み法（Embedded methods）
学習過程で得られる「特徴の重要度」を利用。  
例：Lasso（L1正則化）、決定木の feature_importances_、Boruta

---

## この章で学ぶこと
- **特徴選択の基本**（全体像と3つのアプローチ）  
- **Boruta**（木モデルに基づく安定した特徴選択）  
- （発展）他の手法との比較や実務での活用法  

---

## まとめ
- 特徴選択は **精度・効率・解釈性** の観点で重要。  
- アプローチは「フィルタ・ラッパー・組み込み」の3つに大別できる。  
- 実務では **組み込み法（Lassoや木系モデル、Boruta）** がよく使われる。  

---
