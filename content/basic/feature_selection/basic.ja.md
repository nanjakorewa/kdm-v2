---
title: "特徴選択の基本"
pre: "2.7.1 "
weight: 1
title_suffix: "なぜ必要か・どのような手法があるか"
---

{{< katex />}}

<div class="pagetop-box">
  <p><b>特徴選択（Feature Selection）</b>とは、データに含まれる多数の特徴量の中から「有用なものだけを残す」処理です。  
  不要な特徴を削除することで、<b>過学習の防止・計算効率化・解釈性の向上</b>につながります。</p>
</div>

---

## 1. なぜ特徴選択が必要か？
- **過学習の防止**：ノイズの多い特徴はモデルの精度を下げる原因になる。  
- **計算効率の向上**：特徴が少なければ学習も推論も速い。  
- **解釈性の改善**：重要な特徴が明確になり、モデルが何を見ているか理解しやすい。  

---

## 2. 特徴選択の3つのアプローチ

### 2.1 フィルタ法（Filter methods）
- 特徴ごとに統計的な指標を計算し、有用性を判定。  
- モデルを使わずに軽量に実行できる。  

例：
- 相関係数（高い相関のある特徴を削除）
- カイ二乗検定（χ² test）
- 相互情報量（Mutual Information）

```python
from sklearn.feature_selection import chi2, SelectKBest
X_new = SelectKBest(score_func=chi2, k=5).fit_transform(X, y)
```

---

### 2.2 ラッパー法（Wrapper methods）
- 実際にモデルを学習させ、精度に基づいて特徴を選ぶ方法。  
- 代表例：
  - 逐次前進選択（SFS: Sequential Forward Selection）
  - 逐次後退選択（SBS: Sequential Backward Selection）

⚠️ 注意  
- SFSやSBSは「何度もモデルを学習 → 精度を見ながら人が選ぶ」プロセスを含むため、**バイアスや過学習が生じやすい**点に注意が必要です。  
- そのため大規模データでは計算コストも大きく、実務では慎重に使う必要があります。  

---

### 2.3 組み込み法（Embedded methods）
- モデル学習の過程で得られる「重要度」を利用する。  
- 一度の学習で特徴選択が可能なため、効率が良い。  

例：
- L1正則化（Lasso回帰）  
  不要な係数をゼロにする性質を利用。  
- 決定木系モデル（ランダムフォレストやXGBoost）  
  `feature_importances_` で重要度を出力可能。  

```python
from sklearn.linear_model import Lasso
model = Lasso(alpha=0.1).fit(X, y)
selected = model.coef_ != 0
```

---

## 3. 実務での注意点
- データ分割（train/test）前に特徴選択を行うとデータリークが起きるので注意。  
- 高次元データ（例：遺伝子データ、テキストデータ）では必須の前処理。  
- 「精度向上」だけでなく「解釈性の向上」という観点も重要。  

---

## まとめ
- 特徴選択は **精度・効率・解釈性** を改善する重要ステップ。  
- 手法は大きく **フィルタ法・ラッパー法・組み込み法** に分類される。  
- SFSやSBSは分かりやすいが、**人の判断によるバイアスや過学習のリスク**があるので注意。  
- 実務では「組み込み法（Lassoや木モデル）」がよく使われる。  

---
