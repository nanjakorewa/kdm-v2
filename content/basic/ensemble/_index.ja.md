---
title: アンサンブル
weight: 5
chapter: true
not_use_colab: true
not_use_twitter: true
pre: "<b>2.4 </b>"
---

# アンサンブル

<div class="pagetop-box">
  <p><b>アンサンブル学習</b>とは、複数のモデル（弱学習器）を組み合わせて、単体のモデルよりも<b>精度</b>や<b>安定性</b>を高める手法の総称です。</p>
  <p>「1人の専門家よりも、多数の意見をまとめたほうが良い判断ができる」──その直感を機械学習に取り入れたのがアンサンブル学習です。</p>
</div>

---

## なぜアンサンブルを学ぶのか？

- **単一モデルの弱点を補える**（決定木は過学習しやすいが、ランダムフォレストは安定）  
- **予測精度を底上げできる**（ブースティングは難しいサンプルを重点的に学ぶ）  
- **幅広い応用で活躍**（画像・テキスト・表形式データまで対応可能）  
- Kaggle などのデータコンペで「上位の常連手法」  

---

## アンサンブルでできること

- **住宅価格の予測**：ランダムフォレストや勾配ブースティングで高精度な回帰  
- **顧客の離脱予測**：AdaBoost で難しいケースを重点学習  
- **不正検知・リスク評価**：複数モデルの予測を統合して堅牢性を確保  

---

## この章で学ぶ内容

1. **RandomForest（バギング系）**  
   多数の決定木をブートストラップサンプルで学習し、多数決や平均で予測  
2. **AdaBoost（ブースティング系）**  
   誤分類（誤差）の大きなサンプルに重みを置き、弱学習器を逐次強化  
3. **Gradient Boosting（ブースティング系）**  
   損失関数の勾配（残差）を逐次近似し、段階的に予測を改善  
4. **Stacking（スタッキング）**  
   複数種類のモデルを組み合わせ、メタ学習器で最終予測を行う  

---

## まとめ

- アンサンブル学習は「多数の弱い学習器を組み合わせて強い学習器を作る」考え方  
- Bagging（並列で木を育てる）、Boosting（誤差を修正しながら足す）、Stacking（異種モデルを統合）の3系統が代表  
- 実務でも学術でも「とりあえず使うと強い」手法であり、機械学習の中核をなす  

---
